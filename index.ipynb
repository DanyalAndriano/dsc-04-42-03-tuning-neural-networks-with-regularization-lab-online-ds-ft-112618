{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n",
    "\n",
    "#Alternative custom script:\n",
    "# random.seed(123)\n",
    "# test_index = random.sample(range(1,10000), 1500)\n",
    "# test = one_hot_results[test_index]\n",
    "# train = np.delete(one_hot_results, test_index, 0)\n",
    "# label_test = product_onehot[test_index]\n",
    "# label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.9465 - acc: 0.1732 - val_loss: 1.9339 - val_acc: 0.1800\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9160 - acc: 0.1947 - val_loss: 1.9129 - val_acc: 0.1970\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8947 - acc: 0.2249 - val_loss: 1.8919 - val_acc: 0.2130\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8717 - acc: 0.2541 - val_loss: 1.8685 - val_acc: 0.2540\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8459 - acc: 0.2819 - val_loss: 1.8412 - val_acc: 0.2770\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8160 - acc: 0.3163 - val_loss: 1.8101 - val_acc: 0.3120\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7827 - acc: 0.3468 - val_loss: 1.7760 - val_acc: 0.3420\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7458 - acc: 0.3712 - val_loss: 1.7391 - val_acc: 0.3660\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7055 - acc: 0.3928 - val_loss: 1.6991 - val_acc: 0.3800\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6619 - acc: 0.4121 - val_loss: 1.6553 - val_acc: 0.4060\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6149 - acc: 0.4316 - val_loss: 1.6073 - val_acc: 0.4370\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5633 - acc: 0.4603 - val_loss: 1.5555 - val_acc: 0.4650\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5089 - acc: 0.4911 - val_loss: 1.5012 - val_acc: 0.5040\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4537 - acc: 0.5256 - val_loss: 1.4476 - val_acc: 0.5250\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3988 - acc: 0.5491 - val_loss: 1.3948 - val_acc: 0.5500\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3454 - acc: 0.5761 - val_loss: 1.3444 - val_acc: 0.5720\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.2950 - acc: 0.601 - 0s 32us/step - loss: 1.2940 - acc: 0.6016 - val_loss: 1.2969 - val_acc: 0.5840\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2451 - acc: 0.6175 - val_loss: 1.2523 - val_acc: 0.6010\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1991 - acc: 0.6347 - val_loss: 1.2088 - val_acc: 0.6240\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1560 - acc: 0.6489 - val_loss: 1.1689 - val_acc: 0.6400\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1156 - acc: 0.6620 - val_loss: 1.1323 - val_acc: 0.6440\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0777 - acc: 0.6713 - val_loss: 1.0988 - val_acc: 0.6600\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0422 - acc: 0.6831 - val_loss: 1.0661 - val_acc: 0.6640\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0092 - acc: 0.6929 - val_loss: 1.0361 - val_acc: 0.6700\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9782 - acc: 0.7032 - val_loss: 1.0086 - val_acc: 0.6800\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9493 - acc: 0.7085 - val_loss: 0.9819 - val_acc: 0.6800\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9223 - acc: 0.7147 - val_loss: 0.9585 - val_acc: 0.6950\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8968 - acc: 0.7217 - val_loss: 0.9365 - val_acc: 0.6900\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8736 - acc: 0.7275 - val_loss: 0.9171 - val_acc: 0.7000\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8514 - acc: 0.7327 - val_loss: 0.8977 - val_acc: 0.7060\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8308 - acc: 0.7388 - val_loss: 0.8785 - val_acc: 0.7030\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8114 - acc: 0.7404 - val_loss: 0.8634 - val_acc: 0.7120\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7934 - acc: 0.7447 - val_loss: 0.8472 - val_acc: 0.7210\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7766 - acc: 0.7467 - val_loss: 0.8326 - val_acc: 0.7180\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7606 - acc: 0.7521 - val_loss: 0.8215 - val_acc: 0.7230\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7459 - acc: 0.7551 - val_loss: 0.8083 - val_acc: 0.7220\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7317 - acc: 0.7589 - val_loss: 0.7966 - val_acc: 0.7330\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7179 - acc: 0.7639 - val_loss: 0.7885 - val_acc: 0.7330\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7062 - acc: 0.7680 - val_loss: 0.7762 - val_acc: 0.7310\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6940 - acc: 0.7681 - val_loss: 0.7666 - val_acc: 0.7360\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6825 - acc: 0.7725 - val_loss: 0.7599 - val_acc: 0.7380\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6721 - acc: 0.7769 - val_loss: 0.7509 - val_acc: 0.7340\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6621 - acc: 0.7773 - val_loss: 0.7435 - val_acc: 0.7410\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6525 - acc: 0.7815 - val_loss: 0.7363 - val_acc: 0.7450\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6431 - acc: 0.7849 - val_loss: 0.7297 - val_acc: 0.7460\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6343 - acc: 0.7868 - val_loss: 0.7236 - val_acc: 0.7470\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6255 - acc: 0.7899 - val_loss: 0.7195 - val_acc: 0.7460\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6180 - acc: 0.7921 - val_loss: 0.7133 - val_acc: 0.7470\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6099 - acc: 0.7951 - val_loss: 0.7070 - val_acc: 0.7490\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.6027 - acc: 0.794 - 0s 31us/step - loss: 0.6025 - acc: 0.7943 - val_loss: 0.7025 - val_acc: 0.7510\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5955 - acc: 0.7945 - val_loss: 0.6990 - val_acc: 0.7500\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5882 - acc: 0.7988 - val_loss: 0.6936 - val_acc: 0.7510\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.5817 - acc: 0.8017 - val_loss: 0.6909 - val_acc: 0.7490\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5751 - acc: 0.8049 - val_loss: 0.6858 - val_acc: 0.7550\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5690 - acc: 0.8063 - val_loss: 0.6822 - val_acc: 0.7540\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5630 - acc: 0.8036 - val_loss: 0.6811 - val_acc: 0.7490\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5570 - acc: 0.8107 - val_loss: 0.6776 - val_acc: 0.7510\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5511 - acc: 0.8133 - val_loss: 0.6725 - val_acc: 0.7570\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5456 - acc: 0.8115 - val_loss: 0.6772 - val_acc: 0.7470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5404 - acc: 0.8160 - val_loss: 0.6674 - val_acc: 0.7570\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5345 - acc: 0.8176 - val_loss: 0.6673 - val_acc: 0.7530\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5299 - acc: 0.8204 - val_loss: 0.6648 - val_acc: 0.7520\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5246 - acc: 0.8189 - val_loss: 0.6602 - val_acc: 0.7590\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5199 - acc: 0.8221 - val_loss: 0.6568 - val_acc: 0.7600\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5152 - acc: 0.8224 - val_loss: 0.6564 - val_acc: 0.7540\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5103 - acc: 0.8253 - val_loss: 0.6537 - val_acc: 0.7600\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5056 - acc: 0.8271 - val_loss: 0.6519 - val_acc: 0.7570\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5007 - acc: 0.8265 - val_loss: 0.6510 - val_acc: 0.7630\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4964 - acc: 0.8303 - val_loss: 0.6532 - val_acc: 0.7500\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4922 - acc: 0.8315 - val_loss: 0.6490 - val_acc: 0.7590\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4868 - acc: 0.8337 - val_loss: 0.6460 - val_acc: 0.7590\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4837 - acc: 0.8355 - val_loss: 0.6453 - val_acc: 0.7560\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.4795 - acc: 0.838 - 0s 31us/step - loss: 0.4795 - acc: 0.8379 - val_loss: 0.6434 - val_acc: 0.7570\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4755 - acc: 0.8384 - val_loss: 0.6437 - val_acc: 0.7570\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4711 - acc: 0.8421 - val_loss: 0.6417 - val_acc: 0.7610\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4673 - acc: 0.8400 - val_loss: 0.6386 - val_acc: 0.7540\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4634 - acc: 0.8431 - val_loss: 0.6371 - val_acc: 0.7560\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4591 - acc: 0.8451 - val_loss: 0.6365 - val_acc: 0.7540\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4556 - acc: 0.8465 - val_loss: 0.6384 - val_acc: 0.7560\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4519 - acc: 0.8495 - val_loss: 0.6334 - val_acc: 0.7570\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4481 - acc: 0.8488 - val_loss: 0.6327 - val_acc: 0.7560\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4448 - acc: 0.8528 - val_loss: 0.6313 - val_acc: 0.7650\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4412 - acc: 0.8527 - val_loss: 0.6344 - val_acc: 0.7590\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4378 - acc: 0.8532 - val_loss: 0.6329 - val_acc: 0.7600\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4342 - acc: 0.8557 - val_loss: 0.6297 - val_acc: 0.7570\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4306 - acc: 0.8559 - val_loss: 0.6310 - val_acc: 0.7580\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4274 - acc: 0.8557 - val_loss: 0.6279 - val_acc: 0.7580\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4240 - acc: 0.8601 - val_loss: 0.6335 - val_acc: 0.7580\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4206 - acc: 0.8607 - val_loss: 0.6315 - val_acc: 0.7680\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4177 - acc: 0.8608 - val_loss: 0.6292 - val_acc: 0.7640\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4143 - acc: 0.8625 - val_loss: 0.6264 - val_acc: 0.7610\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4110 - acc: 0.8628 - val_loss: 0.6269 - val_acc: 0.7620\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4082 - acc: 0.8648 - val_loss: 0.6245 - val_acc: 0.7590\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4052 - acc: 0.8641 - val_loss: 0.6272 - val_acc: 0.7640\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4021 - acc: 0.8661 - val_loss: 0.6256 - val_acc: 0.7650\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3990 - acc: 0.8680 - val_loss: 0.6266 - val_acc: 0.7690\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3960 - acc: 0.8699 - val_loss: 0.6265 - val_acc: 0.7680\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3927 - acc: 0.8712 - val_loss: 0.6256 - val_acc: 0.7630\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3899 - acc: 0.8709 - val_loss: 0.6238 - val_acc: 0.7670\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3871 - acc: 0.8728 - val_loss: 0.6243 - val_acc: 0.7620\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3841 - acc: 0.8741 - val_loss: 0.6261 - val_acc: 0.7690\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3816 - acc: 0.8736 - val_loss: 0.6219 - val_acc: 0.7610\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3787 - acc: 0.8769 - val_loss: 0.6222 - val_acc: 0.7630\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3758 - acc: 0.8769 - val_loss: 0.6218 - val_acc: 0.7640\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3727 - acc: 0.8793 - val_loss: 0.6219 - val_acc: 0.7620\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3698 - acc: 0.8805 - val_loss: 0.6220 - val_acc: 0.7620\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.3673 - acc: 0.8820 - val_loss: 0.6232 - val_acc: 0.7590\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3649 - acc: 0.8831 - val_loss: 0.6212 - val_acc: 0.7660\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3622 - acc: 0.8831 - val_loss: 0.6221 - val_acc: 0.7620\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3595 - acc: 0.8859 - val_loss: 0.6248 - val_acc: 0.7650\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3572 - acc: 0.8861 - val_loss: 0.6259 - val_acc: 0.7660\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.3544 - acc: 0.8868 - val_loss: 0.6217 - val_acc: 0.7640\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3521 - acc: 0.8873 - val_loss: 0.6239 - val_acc: 0.7630\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3490 - acc: 0.8876 - val_loss: 0.6218 - val_acc: 0.7610\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3466 - acc: 0.8907 - val_loss: 0.6244 - val_acc: 0.7700\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3441 - acc: 0.8919 - val_loss: 0.6239 - val_acc: 0.7690\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3417 - acc: 0.8921 - val_loss: 0.6276 - val_acc: 0.7670\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3396 - acc: 0.8933 - val_loss: 0.6223 - val_acc: 0.7710\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3370 - acc: 0.8935 - val_loss: 0.6240 - val_acc: 0.7700\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3346 - acc: 0.8951 - val_loss: 0.6238 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3325421634713809, 0.8945333333333333]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5999678586324056, 0.7633333338101705]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd4FdXWwOHfSkioIYEUCARI6IQQWkQUpNgoVqyg2JWLvV7BdhX1XhUVEXtBbAjXKxZUxE+RIkoLSC8SeqghkNAhZX1/zBCDpAE5mZT1Ps95PGdmz5w1Z3BW9t4ze4uqYowxxgD4eR2AMcaY0sOSgjHGmByWFIwxxuSwpGCMMSaHJQVjjDE5LCkYY4zJYUnBlBgR8ReRfSLSsDjLlnYi8qmIPOW+7yEiy4pS9iS+x2e/mYgki0iP4t6vKX0sKZh8uReYo69sETmY6/O1J7o/Vc1S1RqqurE4y54METlNRBaIyF4RWSki5/rie/5OVaepauvi2JeIzBSRG3Pt26e/makYLCmYfLkXmBqqWgPYCFyUa9nYv5cXkUolH+VJexOYCNQE+gKbvQ3HmNLBkoI5aSLyrIj8V0TGicheYKCInCEis0UkTUS2isgoEQlwy1cSERWRaPfzp+76H9y/2GeJSMyJlnXX9xGRP0UkXUReE5Hfcv8VnYdMYIM61qrqikKOdbWI9M71OVBEdolIvIj4icgXIrLNPe5pItIqn/2cKyLrc33uKCIL3WMaB1TOtS5URCaJSIqI7BaRb0WkvrvuBeAM4G235jYyj98sxP3dUkRkvYg8IiLirrtVRKaLyCtuzGtF5PyCfoNccVVxz8VWEdksIiNEJNBdF+HGnOb+PjNybfeoiGwRkT1u7axHUb7PlCxLCuZU9QM+A4KB/+JcbO8FwoAuQG/gHwVsfw3wBFAbpzbyzImWFZEI4HPgn+73rgM6FRL3XOBlEWlbSLmjxgEDcn3uA2xR1cXu5++AZkBdYCnwSWE7FJHKwDfABzjH9A1waa4ifsB7QEOgEZABvAqgqkOAWcBgt+Z2Xx5f8SZQDWgMnA3cAlyfa/2ZwBIgFHgFGF1YzK5/AQlAPNAe5zw/4q77J7AWCMf5LZ5wj7U1zr+DDqpaE+f3s2auUsiSgjlVM1X1W1XNVtWDqjpPVeeoaqaqrgXeBboXsP0XqpqoqhnAWKDdSZS9EFioqt+4614Bdua3ExEZiHMhGwh8LyLx7vI+IjInn80+Ay4VkSru52vcZbjH/qGq7lXVQ8BTQEcRqV7AseDGoMBrqpqhquOBP46uVNUUVf3K/V33AP+h4N8y9zEGAFcBQ9241uL8LtflKrZGVT9Q1SzgIyBKRMKKsPtrgafc+HYAT+fabwZQD2ioqkdUdbq7PBOoArQWkUqqus6NyZQylhTMqdqU+4OItBSR792mlD04F4yCLjTbcr0/ANQ4ibL1csehziiPyQXs515glKpOAu4E/s9NDGcCP+e1gaquBNYAF4hIDZxE9Bnk3PUz3G2C2QMkuZsVdoGtByTrsaNSbjj6RkSqi8j7IrLR3e8vRdjnURGAf+79ue/r5/r8998TCv79j4osYL/Pu5+niMgaEfkngKquAh7E+feww21yrFvEYzElyJKCOVV/H2b3HZzmk6ZuM8G/APFxDFuBqKMf3Hbz+vkXpxLOX66o6jfAEJxkMBAYWcB2R5uQ+uHUTNa7y6/H6aw+G6cZrenRUE4kblfu20kfBmKATu5vefbfyhY0xPEOIAun2Sn3voujQ31rfvtV1T2qer+qRuM0hQ0Rke7uuk9VtQvOMfkDzxVDLKaYWVIwxS0ISAf2u52tBfUnFJfvgA4icpE4d0Ddi9OmnZ//AU+JSBsR8QNWAkeAqjhNHPkZh9MWPgi3luAKAg4DqTht+P8uYtwzAT8RucvtJL4S6PC3/R4AdotIKE6CzW07Tn/BcdxmtC+A/4hIDbdT/n7g0yLGVpBxwL9EJExEwnH6DT4FcM9BEzcxp+MkpiwRaSUiPd1+lIPuK6sYYjHFzJKCKW4PAjcAe3FqDf/19Req6nbgamAEzoW5CU7b/OF8NnkB+BjnltRdOLWDW3Eudt+LSM18vicZSAQ643RsHzUG2OK+lgG/FzHuwzi1jtuA3cBlwNe5iozAqXmkuvv84W+7GAkMcO/0GZHHV9yBk+zWAdNx+g0+LkpshRgGLMLppF4MzOGvv/pb4DRz7QN+A15V1Zk4d1UNx+nr2QbUAh4vhlhMMRObZMeUNyLij3OBvkJVf/U6HmPKEqspmHJBRHqLSLDbPPEETp/BXI/DMqbMsaRgyouuOPfH78R5NuJSt3nGGHMCrPnIGGNMDqspGGOMyVGWBjADICwsTKOjo70OwxhjypT58+fvVNWCbtUGfJgURKQBzu1vdYFs4F1VffVvZQRnLJe+OPdj36iqCwrab3R0NImJib4J2hhjyikR2VB4Kd/WFDKBB1V1gYgEAfNF5CdVXZ6rTB+cQcSaAacDb7n/NcYY4wGf9Smo6tajf/Wr6l5gBccPPXAJ8LE7fPFsIEREIn0VkzHGmIKVSEezO757e5wnH3Orz7EDqiWTx5g1IjJIRBJFJDElJcVXYRpjTIXn845md0TJCcB97vC/x6zOY5Pj7pFV1XdxhmAmISHB7qE1pgRlZGSQnJzMoUOHvA7FFEGVKlWIiooiICDgpLb3aVJwx3SfAIxV1S/zKJIMNMj1OQpneAJjTCmRnJxMUFAQ0dHRuBO3mVJKVUlNTSU5OZmYmJjCN8iDz5qP3DuLRgMrVDWvwbrAGZDsenF0BtJVdauvYjLGnLhDhw4RGhpqCaEMEBFCQ0NPqVbny5pCF5zZmJaIyEJ32aO448Wr6tvAJJzbUZNwbkm9yYfxGGNOkiWEsuNUz5XPkoI7XG6B0bkzTt3pqxhyS96TzIu/vchL579EgP/JtbUZY0x5V2GGuZi3eR6jpn3CszOe9ToUY8wJSE1NpV27drRr1466detSv379nM9Hjhwp0j5uuukmVq1aVWCZN954g7FjxxZHyHTt2pWFCxcWXrAUKnPDXJy0lf0IeP18nk3tw4XNL+S0+qd5HZExpghCQ0NzLrBPPfUUNWrU4KGHHjqmjKqiqvj55f137pgxYwr9njvvLJFGi1KvwtQUOneGRvWroJ9O4ooRIziYcdDrkIwxpyApKYm4uDgGDx5Mhw4d2Lp1K4MGDSIhIYHWrVvz9NNP55Q9+pd7ZmYmISEhDB06lLZt23LGGWewY8cOAB5//HFGjhyZU37o0KF06tSJFi1a8PvvzmR6+/fv5/LLL6dt27YMGDCAhISEQmsEn376KW3atCEuLo5HH30UgMzMTK677rqc5aNGjQLglVdeITY2lrZt2zJw4MBi/82KosLUFCIjYfo0fzp1ETa+/g4DIkfx1YMPWweaMSfgvsn3sXBb8TaLtKvbjpG9R57UtsuXL2fMmDG8/fbbADz//PPUrl2bzMxMevbsyRVXXEFsbOwx26Snp9O9e3eef/55HnjgAT744AOGDh163L5Vlblz5zJx4kSefvppJk+ezGuvvUbdunWZMGECixYtokOHDsdtl1tycjKPP/44iYmJBAcHc+655/Ldd98RHh7Ozp07WbJkCQBpaWkADB8+nA0bNhAYGJizrKRVmJoCQL16MGdmdULCjvDNY7dz/WtvYvNJGFN2NWnShNNO+6speNy4cXTo0IEOHTqwYsUKli9fftw2VatWpU+fPgB07NiR9evX57nvyy677LgyM2fOpH///gC0bduW1q1bFxjfnDlzOPvsswkLCyMgIIBrrrmGGTNm0LRpU1atWsW9997Ljz/+SHBwMACtW7dm4MCBjB079qQfPjtVFaamcFT9+rB4dihxp6fw6cMDqVzpPd6/Y5DXYRlTJpzsX/S+Ur169Zz3q1ev5tVXX2Xu3LmEhIQwcODAPO/XDwwMzHnv7+9PZmZmnvuuXLnycWVO9I/I/MqHhoayePFifvjhB0aNGsWECRN49913+fHHH5k+fTrffPMNzz77LEuXLsXf3/+EvvNUVaiawlENGgiLZ4cRVOswox+4ivs//NjrkIwxp2jPnj0EBQVRs2ZNtm7dyo8//ljs39G1a1c+//xzAJYsWZJnTSS3zp07M3XqVFJTU8nMzGT8+PF0796dlJQUVJUrr7ySYcOGsWDBArKyskhOTubss8/mxRdfJCUlhQMHDhT7MRSmwtUUjmrU0I/Fs8NonZDKyLvOJyriSx7se5nXYRljTlKHDh2IjY0lLi6Oxo0b06VLl2L/jrvvvpvrr7+e+Ph4OnToQFxcXE7TT16ioqJ4+umn6dGjB6rKRRddxAUXXMCCBQu45ZZbUFVEhBdeeIHMzEyuueYa9u7dS3Z2NkOGDCEoKKjYj6EwZW6O5oSEBC3OSXYWLTlCwhmHyKyyjc++38CA088rtn0bUx6sWLGCVq1aeR1GqZCZmUlmZiZVqlRh9erVnH/++axevZpKlUrX39d5nTMRma+qCYVtW7qOxANt2wQy6dtMevVqyMCrUmn52zLaRxXceWSMqZj27dvHOeecQ2ZmJqrKO++8U+oSwqkqX0dzks7rWY0339vF7TeeQY+BH7Duh0hqV63tdVjGmFImJCSE+fPnex2GT1XIjua8DL6hNlfetJU902+m58NvkJmd9x0JxhhTnllSyOXTtyOJidvB4nfv5b6xb3gdjjHGlDhLCrkEBsK07yMIDPTjjcc68duG2V6HZIwxJcqSwt80bAivj/KDTWdwyX1T2Xdkn9chGWNMifHlzGsfiMgOEVmaz/pgEflWRBaJyDIRKTUT7Nx6YzW6nreT1G/v55bRw70Ox5gKrUePHsc9iDZy5EjuuOOOArerUaMGAFu2bOGKK67Id9+F3eI+cuTIYx4i69u3b7GMS/TUU0/x0ksvnfJ+ipsvawofAr0LWH8nsFxV2wI9gJdFJLCA8iVGBP73cRhVqmXx+b/7MmXNNK9DMqbCGjBgAOPHjz9m2fjx4xkwYECRtq9Xrx5ffPHFSX//35PCpEmTCAkJOen9lXY+SwqqOgPYVVARIMidy7mGW7bU3PJTty68PqoSbO7MNY9O43DmYa9DMqZCuuKKK/juu+84fNj5f3D9+vVs2bKFrl275jw30KFDB9q0acM333xz3Pbr168nLi4OgIMHD9K/f3/i4+O5+uqrOXjwryH0b7/99pxht5988kkARo0axZYtW+jZsyc9e/YEIDo6mp07dwIwYsQI4uLiiIuLyxl2e/369bRq1YrbbruN1q1bc/755x/zPXlZuHAhnTt3Jj4+nn79+rF79+6c74+NjSU+Pj5nIL7p06fnTDLUvn179u7de9K/bZ6OTk7hixcQDSzNZ10QMBXYCuwDLihgP4OARCCxYcOGWlKys1XbnbFTqbxbH5rwYol9rzGlyfLly3Pe33uvavfuxfu6997CY+jbt69+/fXXqqr63HPP6UMPPaSqqhkZGZqenq6qqikpKdqkSRPNzs5WVdXq1aurquq6deu0devWqqr68ssv60033aSqqosWLVJ/f3+dN2+eqqqmpqaqqmpmZqZ2795dFy1apKqqjRo10pSUlJxYjn5OTEzUuLg43bdvn+7du1djY2N1wYIFum7dOvX399c//vhDVVWvvPJK/eSTT447pieffFJffNG5rrRp00anTZumqqpPPPGE3uv+KJGRkXro0CFVVd29e7eqql544YU6c+ZMVVXdu3evZmRkHLfv3OfsKCBRi3Dd9rKjuRewEKgHtANeF5GaeRVU1XdVNUFVE8LDw0ssQBEY90Eoflk1GPF0XVbuXFli322M+UvuJqTcTUeqyqOPPkp8fDznnnsumzdvZvv27fnuZ8aMGTmT18THxxMfH5+z7vPPP6dDhw60b9+eZcuWFTrY3cyZM+nXrx/Vq1enRo0aXHbZZfz6668AxMTE0K5dO6Dg4bnBmd8hLS2N7t27A3DDDTcwY8aMnBivvfZaPv3005wnp7t06cIDDzzAqFGjSEtLK/Ynqr18ovkm4Hk3gyWJyDqgJTDXw5iO07Il3H3fIV59aSDXv/Iwc/9tHc+m4hrp0cjZl156KQ888AALFizg4MGDOZPbjB07lpSUFObPn09AQADR0dF5DpedW14Ta61bt46XXnqJefPmUatWLW688cZC96MFjBt3dNhtcIbeLqz5KD/ff/89M2bMYOLEiTzzzDMsW7aMoUOHcsEFFzBp0iQ6d+7Mzz//TMuWLU9q/3nxsqawETgHQETqAC2AtR7Gk6//DKtBrbrpzBt9Dd+vmux1OMZUODVq1KBHjx7cfPPNx3Qwp6enExERQUBAAFOnTmXDhg0F7qdbt26MHTsWgKVLl7J48WLAGXa7evXqBAcHs337dn744YecbYKCgvJst+/WrRtff/01Bw4cYP/+/Xz11VecddZZJ3xswcHB1KpVK6eW8cknn9C9e3eys7PZtGkTPXv2ZPjw4aSlpbFv3z7WrFlDmzZtGDJkCAkJCaxcWbwtGD6rKYjIOJy7isJEJBl4EggAUNW3gWeAD0VkCSDAEFXd6at4TkW1avDaiOoMvKYdtz3zBBs/PpdKfjZslDElacCAAVx22WXH3Il07bXXctFFF5GQkEC7du0K/Yv59ttv56abbiI+Pp527drRqVMnwJlFrX379rRu3fq4YbcHDRpEnz59iIyMZOrUqTnLO3TowI033pizj1tvvZX27dsX2FSUn48++ojBgwdz4MABGjduzJgxY8jKymLgwIGkp6ejqtx///2EhITwxBNPMHXqVPz9/YmNjc2ZRa64VPihs4tKFWI7prJy9RFGfPs99/e4tcRjMMYLNnR22XMqQ2fbE81FJAJj3qwN+yJ57Jk97Dm8x+uQjDGm2FlSOAGdOwu9+6VycMbtDJs42utwjDGm2FlSOEFvjQhFtBKvvRTMroMFPZtnTPlR1pqZK7JTPVeWFE5QdDRcfcMeMhKv47Ev3vc6HGN8rkqVKqSmplpiKANUldTUVKpUqXLS+7CO5pOwdSs0iD4MsRNI/vVs6tao62k8xvhSRkYGycnJhd63b0qHKlWqEBUVRUBAwDHLbY5mH4qMhJsG7ef9N/rz8NgX+fgfQ7wOyRifCQgIICYmxuswTAmx5qOT9PxTtQmocpjPXm9Kyv4Ur8MxxphiYUnhJIWGOrWFrKX9eOJ/n3odjjHGFAtLCqfgmUfD8AvI5IPXw0g/lO51OMYYc8osKZyCiAi46ro0Mv7oz7+/+8TrcIwx5pRZUjhFLzwZgYjw2sjKHMg4UPgGxhhTillSOEUNG0Kfy1I5NGcgr//yudfhGGPMKbGkUAxeGhYBmVV5YVQa2ZrtdTjGGHPSLCkUg1athLZdtrJrxtV8vXSS1+EYY8xJs6RQTP7zWIQzgurri7wOxRhjTpolhWLSp7c/EY1SWfnd+czfssDrcIwx5qT4LCmIyAciskNElhZQpoeILBSRZSIy3VexlAQRGPJgVdhyGo9++J3X4RhjzEnxZU3hQ6B3fitFJAR4E7hYVVsDV/owlhLxj5urUbn6AX7+LJadB0rlzKLGGFMgnyUFVZ0BFDThwDXAl6q60S2/w1exlJTq1eHqgfvJXnEJr035n9fhGGPMCfOyT6E5UEtEponIfBG5Pr+CIjJIRBJFJDElpXQPPvfo/eGQHcCb7x2w8eeNMWWOl0mhEtARuADoBTwhIs3zKqiq76pqgqomhIeHl2SMJ6xFC2iZsI2dv/bjl7XTvA7HGGNOiJdJIRmYrKr7VXUnMANo62E8xeaRe2tDWmOe+XCW16EYY8wJ8TIpfAOcJSKVRKQacDqwwsN4is3VVwZSteY+ZnzZih37y3xXiTGmAvHlLanjgFlACxFJFpFbRGSwiAwGUNUVwGRgMTAXeF9V8719tSypXBkGXHcYXXERr0+Z4HU4xhhTZDZHs4+sXg3Nm0P4xSPY/vX9iIjXIRljKrCiztFsTzT7SLNm0LzDVlJmXsisTbO9DscYY4rEkoIPPXBHCOxqznNjf/U6FGOMKRJLCj40sH9VAqoeZPIX9dh/ZL/X4RhjTKEsKfhQ9erQ+9J0Mpf04+O5X3sdjjHGFMqSgo89ek8dyKjOK6OTvQ7FGGMKZUnBx04/XYiITmH1z91I2pXkdTjGGFMgSwo+JgKDbwuE5DN48Wublc0YU7pZUigBd90WjPhn8NnHVcnKzvI6HGOMyZclhRIQHg6dzt7GvnmXMnnlVK/DMcaYfFlSKCGP3VcHDoTz3OhlXodijDH5sqRQQvr2CqRG+C5mTYwl7VCa1+EYY0yeLCmUEH9/6D/wINlJ5/DmT997HY4xxuTJkkIJeuyeeiDw5rsHvQ7FGGPyZEmhBEVHCy06bWDzjF4s2bbc63CMMeY4lhRK2D/vrgV7GvDkezYrmzGm9PHlJDsfiMgOESlw4hwROU1EskTkCl/FUppcf1UIlYN3M2l8fTKyMrwOxxhjjuHLmsKHQO+CCoiIP/AC8KMP4yhVAgLggqt2cnjFuYz9zZ5ZMMaULj5LCqo6A9hVSLG7gQlAhZrI+NkHo0ErMfzNFK9DMcaYY3jWpyAi9YF+wNtFKDtIRBJFJDElpexfSFu1CKBB2yRW/HgmW/ds9zocY4zJ4WVH80hgiKoWOhiQqr6rqgmqmhAeHl4CofnefXdWgbQYnnhvptehGGNMDi+TQgIwXkTWA1cAb4rIpR7GU6LuuiGKgJqpjP+wNqrqdTjGGAN4mBRUNUZVo1U1GvgCuENVK8z0ZIGB0PuqZPYv7c4Xvyd6HY4xxgC+vSV1HDALaCEiySJyi4gMFpHBvvrOsubFoc1AlKdHWL+CMaZ0qOSrHavqgBMoe6Ov4ijNWjSpRvTpC1k6+XR2pKcTERzsdUjGmArOnmj22IP3VIUD4QwZNc/rUIwxxpKC126/qjmVI9bz39F1yc62DmdjjLcsKXjM31/od+MmDm6I44NvbZA8Y4y3LCmUAiMebgdVdvPv4fu9DsUYU8FZUigFIkODaNt3HutndWTxqnSvwzHGVGCWFEqJ/wytByj3P7Pa61CMMRWYJYVSou9pcdTuOI1pXzZnzx7rcDbGeMOSQilyxz2HyD5YkyEvJHkdijGmgrKkUIo82v8cKjWeyYdv1+LIEa+jMcZURJYUSpGqAVW5/LbVHNoVxqvv7fQ6HGNMBWRJoZQZPvhcqPsHzz+fTXa219EYYyoaSwqlTMOQBnS6ahq7kiP4fMJhr8MxxlQwlhRKof/c3QFqreGRYenYVAvGmJJkSaEUOrtJN6L6fMb6ZRH89JNlBWNMybGkUAqJCMPuawxByTz0xC6vwzHGVCCWFEqpgR2uJPjs91gyN5TffvM6GmNMReHLmdc+EJEdIrI0n/XXishi9/W7iLT1VSxlUaB/IP+8qxZUS2HIv2w8JGNMyfBlTeFDoHcB69cB3VU1HngGeNeHsZRJd3W9icAub/HbL8Ek2jTOxpgSUKSkICJNRKSy+76HiNwjIiEFbaOqM4B8G8RV9XdV3e1+nA1EFTHmCiO4SjC3DT4EVVN5aOhBr8MxxlQARa0pTACyRKQpMBqIAT4rxjhuAX7Ib6WIDBKRRBFJTElJKcavLf2GnHM7fme9yPQpVZkxw+tojDHlXVGTQraqZgL9gJGqej8QWRwBiEhPnKQwJL8yqvquqiaoakJ4eHhxfG2Z0SC4AdffuheCtvDPoUfsuQVjjE8VNSlkiMgA4AbgO3dZwKl+uYjEA+8Dl6hq6qnur7x6/JwHkG7/Zu6sQH780etojDHlWVGTwk3AGcC/VXWdiMQAn57KF4tIQ+BL4DpV/fNU9lXeNandhP7X70dqreOfQzLJyvI6ImNMeVWkpKCqy1X1HlUdJyK1gCBVfb6gbURkHDALaCEiySJyi4gMFpHBbpF/AaHAmyKyUETs/poCPN7zYfTsR1m6uBIffeR1NMaY8kq0CI3UIjINuBioBCwEUoDpqvqAT6PLQ0JCgiZW0Pszr/j8Sr7650OEHTqNpCQ/goK8jsgYU1aIyHxVTSisXFGbj4JVdQ9wGTBGVTsC555KgObEDevxFNnn38OOHX4895zX0RhjyqOiJoVKIhIJXMVfHc2mhLWOaM3Avs3xb/sZI0Yo69Z5HZExprwpalJ4GvgRWKOq80SkMbDad2GZ/DzZ/Un0nEfIliPcdRd2i6oxplgVtaP5f6oar6q3u5/Xqurlvg3N5KVp7abc3P18sns+zqRJMGGC1xEZY8qTog5zESUiX7kD3G0XkQkiYsNSeOSJ7k/gd/ob1G68nnvugXQbL88YU0yK2nw0BpgI1APqA9+6y4wHGgY35N4z7mTXOVexbZvy2GNeR2SMKS+KmhTCVXWMqma6rw+BijXeRCnz6FmPUqtJEg3O/YY334RZs7yOyBhTHhQ1KewUkYEi4u++BgI2LIWHalWtxRPdnmBjwnWE1jnEbbfBkSNeR2WMKeuKmhRuxrkddRuwFbgCZ+gL46E7TruDmDrhVO/3EMuWwfDhXkdkjCnrinr30UZVvVhVw1U1QlUvxXmQzXiocqXKDD9vOBvqvEGHc5N45hlYudLrqIwxZdmpzLxW4kNcmONd3upyekb3ZM3pF1CtejbXXw8ZGV5HZYwpq04lKUixRWFOmogwqs8o9gWuIeG20cybB//5j9dRGWPKqlNJCvYsbSkRFxHHnafdyZRq/6DPZbt45hmYO9frqIwxZVGBSUFE9orInjxee3GeWTClxLCewwivHs62blcQGakMHAh793odlTGmrCkwKahqkKrWzOMVpKqVSipIU7iQKiG82vtV/kibyqWPfMmaNTB4sI2NZIw5MafSfFQgEfnAHRZjaT7rRURGiUiSiCwWkQ6+iqWiuLr11fRp2ocxaTfwwKNpfPYZvPOO11EZY8oSnyUF4EOgdwHr+wDN3Ncg4C0fxlIhiAhvXvAmirKi5fX06qXcey/Mn+91ZMaYssJnSUFVZwC7CihyCfCxOmYDIe6cDeYURIdE82zPZ/k+6VsueeQLIiLgiitgV0FnwhhjXL6sKRSmPrAp1+dkd9lxRGSQiCSKSGJKSkqJBFeW3XP6PZzZ4EwenT2INz9MYfNmuPa2DGRfAAAeA0lEQVRayM72OjJjTGnnZVLI6zmHPLtFVfVdVU1Q1YTwcBuHrzD+fv58eMmHHM48zDvbb+LVV5XJk2HYMK8jM8aUdl4mhWSgQa7PUcAWj2Ipd5qFNuP5c5/n+9XfE3j6GG68EZ5+GsaP9zoyY0xp5mVSmAhc796F1BlIV9WtHsZT7tzV6S56RPfg3sn38OCzSXTrBjfcANOmeR2ZMaa08uUtqeOAWUALEUkWkVtEZLCIDHaLTALWAknAe8AdvoqlovITPz7p9wmB/oHcNGkAn084QtOmcOmlsGSJ19EZY0oj0TL2dFNCQoImJiZ6HUaZ8uWKL7n888t5+MyHubPFC5xxhvNQ24wZ0LSp19EZY0qCiMxX1YTCynnZfGRKyGWtLmNQh0EM/304Sw9P4qefnAl5zjkHNm0qfHtjTMVhSaGCeKX3K7St05aBXw6kWr31/N//QVqakxi2WPe+McZlSaGCqBZQjQlXTSBbs7ni8yuIjT/EDz/A1q3QpQusWeN1hMaY0sCSQgXSpHYTPu73MfO3zueuSXdxxhnKL784o6l27QqLF3sdoTHGa5YUKpiLW1zM42c9zug/RvP63Nc57TSnw9nfH7p3h99/9zpCY4yXLClUQMN6DuPiFhdz/4/3M2XtFGJjYeZMCAuD886DH3/0OkJjjFcsKVRAR59faBHWgiv/dyV/pv5JdLSTGJo3h4sugv/+1+sojTFesKRQQdWsXJOJ/Sfi7+dPr097sW3fNurUgalToXNnGDAA3rLBzI2pcCwpVGBNajdh0jWT2LF/B33G9mHP4T2EhDjNRxdeCHfcAU8+aaOrGlORWFKo4E6rfxoTrprA0h1L6ffffhzKPETVqjBhgjNO0tNPQ69ezq2rxpjyz5KCoXfT3nxw8Qf8su4X+n/Rn4ysDAICYMwYePdd+O03iI+3DmhjKgJLCgaA69pex+t9XuebVd9w88SbydZsROC22yAxESIjoU8fp+ZgzUnGlF+VvA7AlB53drqT9MPpPPbLYwT6BfLexe/hJ37ExsLs2fCPfzh9DLNnw4cfQkSE1xEbY4qbJQVzjEe6PsLhzMM8PeNpsjSL0RePxt/Pn2rV4OOP4cwz4f77oU0bGD3a6ZA2xpQf1nxkjiEiDOs5jGE9hvHRoo+44esbyMzOdNfB7bf/1Zx00UVwzTWwdq3HQRtjio0lBZOnf3X/F/85+z+MXTKWyz+/nEOZh3LWxcXBnDnw+OPw9dfQogXcdRfs3u1hwMaYYuHTpCAivUVklYgkicjQPNY3FJGpIvKHiCwWkb6+jMecmEfOeoTX+7zOxFUT6Tu2L3sO78lZV7kyPPOMM7rqrbfC229Dq1bOk9BlbN4mY0wuvpyO0x94A+gDxAIDRCT2b8UeBz5X1fZAf+BNX8VjTs6dne7k036fMmPDDLqN6UbynuRj1kdGOk8+z5sHUVHQvz/07g1Ll3oUsDHmlPiyptAJSFLVtap6BBgPXPK3MgrUdN8HAzbdSyl0bfy1TLp2Emt3r+X090/nj61/HFemfXvnrqSRI2HuXGjbFgYNgpUrPQjYGHPSfJkU6gO5J3tMdpfl9hQwUESSgUnA3XntSEQGiUiiiCSmpKT4IlZTiPObnM9vN/+Gv/jTdUxXvlj+xXFlKlWCe++FpCSnj2HMGKdJqUsXGD/empWMKQt8mRQkj2V/vywMAD5U1SigL/CJiBwXk6q+q6oJqpoQHh7ug1BNUbSp04Y5t84hvk48V/7vSp745Qmy9fgn2UJD4dVXnfmfhw+H1FRngL2zzoI/jq9kGGNKEV8mhWSgQa7PURzfPHQL8DmAqs4CqgBhPozJnKLIoEim3TCNW9rfwrO/PstF4y5i18FdeZatWxf++U9Yvhzefx/+/BM6doSrr4b580s4cGNMkfgyKcwDmolIjIgE4nQkT/xbmY3AOQAi0gonKVj7UClXuVJl3rvoPd7s+yY/r/2ZDu90YN7mefmW9/ODW25xksKQITB5MiQkQLdu8PrrsHlzCQZvjCmQz5KCqmYCdwE/Aitw7jJaJiJPi8jFbrEHgdtEZBEwDrhR1VqeywIR4fbTbmfmTTNRlC4fdOHF317MsznpqJAQeO65v5qVdu6Eu+927lrq1QsmTbJxlYzxmpS1a3BCQoImJiZ6HYbJJfVAKrd9extfrfyKntE9+ejSj2gQ3KDwDYEVK+Dzz+Gdd5zhuZs0gYEDnT6IFi18HLgxFYiIzFfVhMLK2RPN5pSFVgtlwlUTGH3xaOZunkubt9rw8aKPKcofHK1aOYPsrV8P48ZBo0bOSKwtWzr9Dy+95NQsjDElw5KCKRYiws3tb2bR4EW0qdOGG76+gX7/7ce2fduKtH1goPPg25QpkJwMI0aAv7/TUd2wIZx+Ojz/vFOzKGOVW2PKFGs+MsUuKzuLV+e8yqNTHqV6YHVe7f0q17a5FpG87lIu2OrVTvPSV1/9dcdSTAz07evMJd2hg9PM5O9fzAdhTDlT1OYjSwrGZ1buXMnN39zMrORZ9G7amxHnj6BVeKuT3t/GjU5n9Pffwy+/wIEDzvJatZwJgC680Blio1atYjoAY8oRSwqmVMjKzuK1ua/x5LQn2X9kP4MTBvNUj6cIq3Zqj6NkZsKqVU7tYepUJ1GkpDg1hm7dnCTRubPTL1GtWjEdjDFlmCUFU6qk7E9h2PRhvJ34NjUr1+SZns/wj4R/UMmveOZ5yspyxlz69lvndXRAPn9/aN4cWrd25pnu0cPpnwgMLJavNabMsKRgSqVlO5Zxz+R7+GXdL8RFxPHSeS/Rq2mvYv+eHTucOR/mzoUlS5ynqpOSnE7qatWcOSFiYqBZM2c2uS5doGbNwvdrTFllScGUWqrKlyu+5OGfH2bt7rWc1/g8njvnOTrW6+jT7929G6ZPd/ojVqxwZozbsMGpZfj5QWyskyxat3bGbwoKcobqiI11hgg/iX5yY0oNSwqm1DuceZi3Et/imRnPsOvgLi5odgH/6v4vOtXvVGIx7N/v1CimT4cFC5xaxYYNx5cLCXH6KLp1cxLEvHlO+Y4d4cEHnVqHMaWZJQVTZuw5vIfX577Oy7NeZtfBXZzX+Dwe7/Y43Rp18ySegwchPR327nWemVi2DBYvht9+c5qhAGrUcPoo5s1zhubo1ctJHH5+zt1PjRpB48bOGE8NivZwtzE+ZUnBlDl7D+/l7cS3eWnWS+zYv4MzG5zJkC5DuLD5hfgdP6K6J1JSnNfRZyM2b3YmFvruO+eOqKwsZ/2+fX9tU7++8zxF69bOdjVqQECA04cRHe0kjUrF099uTL4sKZgy60DGAUYvGM3Ls15mQ/oGWoa15P7O93Nd/HVUDajqdXiFUoW0NOfBuzlzYNYsp6bx55+QkXF8eX9/CA+HOnUgLAyqV3c6w9PSnJrKvn3Qpo1T62jTxrmbqnFjqFLF+jlM0VlSMGVeZnYmXyz/ghd/f5EFWxcQXi2c2xNu5x8J/6BeUD2vwzthGRlOf8WhQ877XbucMZ/WrYNt25w7plJTnX6O/fud5qioKOfiv2iRM7Vp7v9d/fygalWnxlG3rpNYRJwaS+XKTpKpW9dJIC1bOv8NDXXW5abqNJVVrerUYEz5ZEnBlBuqyvQN03l51st8/+f3+Pv5c1mry7jrtLvo2rDrSQ2fURbt3evUNv7800kmBw44/R9paX8lFRGnKerQIdi+3XllZh67n+rVndfRZzVSUuDwYad2csYZzqtWLScZBQVB7dpOk9fGjc53797tLKtd2+lgb9kS6tVz4ktPd5ZHRDixrFvnjGe1b99fSSoqynlVLf2VvnLFkoIpl9bsWsNbiW8x+o/RpB1KI75OPLcn3M5Vra+idtXaXodX6mRlObWTVaucRLJrl1MbOXAAjhxxagnh4c5FfONGmDHDaerK77Lg7+8kivT0ggcmrFnTeSUn518mIsKpvURHO7Wew4edRBUV5fTDpKc7MaemOkmqdm3nO/ftc+IXcbYLDHQSWo0aTvNbRIST9DIynH2mpjoJE6B9e6d/p2ZNJ1lmZzu1o8BAZ3+qx76OJt19+5x9R0Y6yezgQScJbtrk3Nq8dSvs2eOUa9DAuSutSRPne7dscfYfGenU/rZscZLlvn1/1dpWrXLufMvKcu5wO+ss59h27nS+x8/PeTVr5twifTJKRVIQkd7Aq4A/8L6qPp9HmauAp3Dmb16kqtcUtE9LCgacfodxS8bx2tzXWLR9EYH+gVzQ7AJubHcjfZv1LbYnpSuijIy/aiF79zo1gz17nIt148bOBS4721m+Zo3TrLVtGwQHOxfbnTudGkVqqvNg4DnnOBfq7dudcsnJzsV0wwZn+6O3AFeu7FzEN21yEpaIUwMJDXUuzLt2ORfGGjX+qmVkZTll9+93LrJZWXkf09GO/L/XmopbYKATz4kSgaZNnd91zZr8yw0Z4owWfDI8Twoi4g/8CZyHM1/zPGCAqi7PVaYZzhzNZ6vqbhGJUNUdBe3XkoLJTVVZuG0hnyz+hM+WfMb2/dupW6Mu18dfz7Xx1xJfJ97rEM0Jys52EkrNmsf3fxRE1ald7NjhJLXAQOcVGur8hX7kiDP8ycKFTvNapUrOxTgj469ak8ixr6pVnW2rV3eS3datzr6PNsFFRTlNaFFRTrz+/k5NIDHRSXZ16zqJLSPD2XbXLqcWFBPjlD982ElkMTHO/sBJmrNmOfGFhzvlVJ3fJSLC2f5klIakcAbwlKr2cj8/AqCqz+UqMxz4U1XfL+p+LSmY/GRkZfBD0g+M/mM0k1ZPIjM7k7iIOPq37s+Vra+keWhzr0M0xjOlYea1+kDuObOS3WW5NQeai8hvIjLbbW46jogMEpFEEUlMSUnxUbimrAvwD+DiFhfzTf9v2PLAFt7o+wbBlYN5fOrjtHi9Be3fac/w34azMX2j16EaU2r5sqZwJdBLVW91P18HdFLVu3OV+Q7IAK4CooBfgThVTctvv1ZTMCdqU/omJqyYwPil45mzeQ4AnaM6c2mLS7mk5SW0DGvpcYTG+F5pqCkkA7kf8I8CtuRR5htVzVDVdcAqoJkPYzIVUIPgBtzX+T5m3zqbpLuTeLbns2RkZTB0ylBavdGKVm+04rEpjzF381yyNdvrcI3xlC9rCpVwOprPATbjdDRfo6rLcpXpjdP5fIOIhAF/AO1UNTW//VpNwRSXTemb+GbVN3y18iumr59OlmYRUT2Cvs360rdpX85rch4hVUK8DtOYYuF5R7MbRF9gJM4tqR+o6r9F5GkgUVUnivPU0ctAbyAL+Leqji9on5YUjC+kHkhlctJkvl/9PZOTJrP70G78xZ8zGpxB36Z96dOsD23rtK0wD8qZ8qdUJAVfsKRgfC0zO5O5m+cyafUkfkj6gQVbFwAQWSOS3k1706tJL85pfM4pTylqTEmypGBMMdm6dyuTkybzQ9IP/LT2J9IOpSEI7SPb0zO6Jz2ie3BWw7MIrhLsdajG5MuSgjE+kJmdybzN85iybgo/r/2ZWcmzOJJ1BH/xp3NUZ/o07UPPmJ60r9u+TIzoaioOSwrGlICDGQeZs3kOP6/9mclJk5m/dT4Alfwq0a5uO7o36k7P6J50bdjVahLGU5YUjPHAjv07mLVpFrOTZ/N78u/MTp7NkawjCEJcRBxdGnShS8MunNngTGJCYqzj2pQYSwrGlAIHMw4yK3kWv274ld+Tf2fWplnsPbIXgDrV69ClYRe6NHCSRPu67alc6QQG+zHmBFhSMKYUysrOYnnKcn7b9Bu/b/qdmRtnsi5tHQCV/SuTUC+Brg27clbDs+gc1ZnQaqEeR2zKC0sKxpQRW/ZuYdamWcxKnsVvm34jcUsimdnOGM8xITGcVv80OkZ2JKFeAqfVO42gykEeR2zKIksKxpRRBzIOMHfzXOZunkvilkTmbZnH+rT1gNOB3al+J3o06kFcRBzNQ5vTMqwl1QOrexu0KfUsKRhTjqQeSCVxSyIzNsxgyropzNsyL2ecJj/xo2VYSzpGdqRDZAfa121P+8j21Kxc0+OoTWliScGYcuxQ5iHW7FrDqtRVLN6+mPlb55O4JZFt+7bllGkR2oJO9TvRvm574uvE07ZuW3sKuwKzpGBMBbRt3zb+2PoH87fOZ96WeczbPI+t+7bmrK8fVJ+2ddvSMbIjnep3IqFeAnWq17FbYysASwrGGMB5dmLJ9iUs3LaQRdsX8ce2P1iesjyn+Sm0aiix4bHERcQRXyee+DrxxEXEWfNTOWNJwRiTr/1H9vPHtj+Yv2U+y1OWsyxlGUt3LCX9cHpOmeiQaOIi4mgV1orY8Fhah7cmNjzWOrXLqKImhUolEYwxpnSpHlidrg270rVh15xlqsqmPZtYtG0RS3YsYdH2RSxPWc7/rfk/jmQdySkXExJD64jWxIbF0jqidU7isLGeygdLCsYYAESEhsENaRjckItaXJSzPDM7k7W717JsxzKWpbivHcv4MelHMrIznG0RmtZuSps6bYgNi6V5aHNahLWgZVhLa4YqY6z5yBhzUjKyMkjalZTT9LRkxxKWbF/C2t1rydKsnHJRNaOOaX5qVrsZTWo3oV5QPfzElzMCm9xKRZ+CO93mqzgzr72vqs/nU+4K4H/Aaapa4BXfkoIxpduRrCOs272OlTtXsmLnCpalLGN5ynJWpKzgYObBnHLVAqrRIrQFseGxtAprRavwVrQMa0lMSIw1RfmA50lBRPxx5mg+D0jGmaN5gKou/1u5IOB7IBC4y5KCMeVTtmazIW0DSbuSSNqVxJ+pf7J8p5MsNu3ZdEzZekH1aFKrCU1rN6Vp7aY0D21O89DmNK3dlGoB1Tw6grKtNHQ0dwKSVHWtG9B44BJg+d/KPQMMBx7yYSzGGI/5iR8xtWKIqRXDeU3OO2bd3sN7WZW6ij9T/2Tt7rWs2b2GNbvWMDlp8jHPWQA0qNmA5qHNaVa72TF9F42CG+Hv51+Sh1Qu+TIp1Adyp/9k4PTcBUSkPdBAVb8TkXyTgogMAgYBNGzY0AehGmO8FFQ5iIR6CSTUO/4P2b2H95K0K4lVqatYnbqaP3f9yZ+pfzJ+2XjSDqXllKvsX5mYWjE0qdWExrUa57ya1W5G41qNbVjyIvJlUsjrEcmctioR8QNeAW4sbEeq+i7wLjjNR8UUnzGmDAiqHET7SGc8p9xUldSDqazauYqVO1eycudK1qatZe3utfy68Vf2HN6TU9ZP/IiqGUWj4EY0DG6YkyyONk+FVQuzp7pdvkwKyUCDXJ+jgC25PgcBccA092TUBSaKyMWF9SsYY4yIEFYtjLCGYXRp2OWYdUcTxtH+i9Wpq1mzew2b9mxi5saZjFs6LueJboCgwKCcvovcySKmVgx1a9StUHdJ+bKjuRJOR/M5wGacjuZrVHVZPuWnAQ9ZR7MxxtcOZx5mfdp6Vu9azZpda1izew2rd61mdepq1qWtOyZhBPoH0qBmAxoEN6BBzQY0Cm6U0zQVUyuG+kH1y0RfhucdzaqaKSJ3AT/i3JL6gaouE5GngURVneir7zbGmIJUrlSZFmEtaBHW4rh1R7KOsCFtA6t3rWZ92no2pG1gQ/oGNu3ZxLT109i8d/MxSaOSXyWiakbRoGYDompGERMSk5M0GgQ7y6pUqlKSh3dK7OE1Y4w5ARlZGWxM38ja3WtZn7aedWnr2JC+geQ9yWxK38SmPZtyZs47KrJGJC3CWtCsdjMaBjckqmYUDYMb0ii4EQ2CGxDoH+jzuD2vKRhjTHkU4B9Ak9pNaFK7SZ7rM7Mz2ZS+ibW717J572Y2pW8iabfzXMbXK78m5UDKMeUFITIoMqemUS+oHvWC6uUkjUYhjYisEVliTVSWFIwxphhV8quU8zxGXg5lHsqpVaxPW8/6tPVs2uPUMJalLOPntT8fM1rt0X3WD6rP3Z3u5sEzH/Rt/D7duzHGmGNUqVQl5+6m/Ow/sp+N6RvZkL7BSRrpm9i4ZyORQZE+j8+SgjHGlDLVA6vTKtwZD6qkVZybb40xxhTKkoIxxpgclhSMMcbksKRgjDEmhyUFY4wxOSwpGGOMyWFJwRhjTA5LCsYYY3KUuQHxRCQF2HCCm4UBO30QjhfsWEonO5bSqzwdz6kcSyNVDS+sUJlLCidDRBKLMjpgWWDHUjrZsZRe5el4SuJYrPnIGGNMDksKxhhjclSUpPCu1wEUIzuW0smOpfQqT8fj82OpEH0Kxhhjiqai1BSMMcYUgSUFY4wxOcp1UhCR3iKySkSSRGSo1/GcCBFpICJTRWSFiCwTkXvd5bVF5CcRWe3+t5bXsRaViPiLyB8i8p37OUZE5rjH8l8R8f3s5cVEREJE5AsRWemeozPK6rkRkfvdf2NLRWSciFQpK+dGRD4QkR0isjTXsjzPgzhGudeDxSLSwbvIj5fPsbzo/htbLCJfiUhIrnWPuMeySkR6FVcc5TYpiIg/8AbQB4gFBohIrLdRnZBM4EFVbQV0Bu504x8KTFHVZsAU93NZcS+wItfnF4BX3GPZDdziSVQn51Vgsqq2BNriHFeZOzciUh+4B0hQ1TjAH+hP2Tk3HwK9/7Ysv/PQB2jmvgYBb5VQjEX1Iccfy09AnKrGA38CjwC414L+QGt3mzfda94pK7dJAegEJKnqWlU9AowHLvE4piJT1a2qusB9vxfnolMf5xg+cot9BFzqTYQnRkSigAuA993PApwNfOEWKUvHUhPoBowGUNUjqppGGT03ONPyVhWRSkA1YCtl5Nyo6gxg198W53ceLgE+VsdsIEREfD/pcRHldSyq+n+qmul+nA1Eue8vAcar6mFVXQck4VzzTll5Tgr1gU25Pie7y8ocEYkG2gNzgDqquhWcxAFEeBfZCRkJPAxku59DgbRc/+DL0vlpDKQAY9zmsPdFpDpl8Nyo6mbgJWAjTjJIB+ZTds8N5H8eyvo14WbgB/e9z46lPCcFyWNZmbv/VkRqABOA+1R1j9fxnAwRuRDYoarzcy/Oo2hZOT+VgA7AW6raHthPGWgqyovb3n4JEAPUA6rjNLP8XVk5NwUps//mROQxnCblsUcX5VGsWI6lPCeFZKBBrs9RwBaPYjkpIhKAkxDGquqX7uLtR6u87n93eBXfCegCXCwi63Ga8c7GqTmEuE0WULbOTzKQrKpz3M9f4CSJsnhuzgXWqWqKqmYAXwJnUnbPDeR/HsrkNUFEbgAuBK7Vvx4s89mxlOekMA9o5t5FEYjTKTPR45iKzG1zHw2sUNURuVZNBG5w398AfFPSsZ0oVX1EVaNUNRrnPPyiqtcCU4Er3GJl4lgAVHUbsElEWriLzgGWUwbPDU6zUWcRqeb+mzt6LGXy3LjyOw8Tgevdu5A6A+lHm5lKKxHpDQwBLlbVA7lWTQT6i0hlEYnB6TyfWyxfqqrl9gX0xemxXwM85nU8Jxh7V5zq4GJgofvqi9MWPwVY7f63ttexnuBx9QC+c983dv8hJwH/Ayp7Hd8JHEc7INE9P18DtcrquQGGASuBpcAnQOWycm6AcTh9IRk4fz3fkt95wGlyecO9HizBuePK82Mo5FiScPoOjl4D3s5V/jH3WFYBfYorDhvmwhhjTI7y3HxkjDHmBFlSMMYYk8OSgjHGmByWFIwxxuSwpGCMMSaHJQVjXCKSJSILc72K7SllEYnOPfqlMaVVpcKLGFNhHFTVdl4HYYyXrKZgTCFEZL2IvCAic91XU3d5IxGZ4o51P0VEGrrL67hj3y9yX2e6u/IXkffcuQv+T0SquuXvEZHl7n7Ge3SYxgCWFIzJrerfmo+uzrVuj6p2Al7HGbcJ9/3H6ox1PxYY5S4fBUxX1bY4YyItc5c3A95Q1dZAGnC5u3wo0N7dz2BfHZwxRWFPNBvjEpF9qlojj+XrgbNVda07SOE2VQ0VkZ1ApKpmuMu3qmqYiKQAUap6ONc+ooGf1Jn4BREZAgSo6rMiMhnYhzNcxtequs/Hh2pMvqymYEzRaD7v8yuTl8O53mfxV5/eBThj8nQE5ucandSYEmdJwZiiuTrXf2e573/HGfUV4Fpgpvt+CnA75MxLXTO/nYqIH9BAVafiTEIUAhxXWzGmpNhfJMb8paqILMz1ebKqHr0ttbKIzMH5Q2qAu+we4AMR+SfOTGw3ucvvBd4VkVtwagS344x+mRd/4FMRCcYZxfMVdab2NMYT1qdgTCHcPoUEVd3pdSzG+Jo1HxljjMlhNQVjjDE5rKZgjDEmhyUFY4wxOSwpGGOMyWFJwRhjTA5LCsYYY3L8P/asoSud1FWNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.9556 - acc: 0.1680 - val_loss: 1.9272 - val_acc: 0.1780\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9015 - acc: 0.2056 - val_loss: 1.8852 - val_acc: 0.2240\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8601 - acc: 0.2517 - val_loss: 1.8442 - val_acc: 0.2710\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8174 - acc: 0.2865 - val_loss: 1.8016 - val_acc: 0.3040\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7720 - acc: 0.3121 - val_loss: 1.7564 - val_acc: 0.3280\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7247 - acc: 0.3359 - val_loss: 1.7101 - val_acc: 0.3500\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6770 - acc: 0.3561 - val_loss: 1.6651 - val_acc: 0.3620\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.6302 - acc: 0.3700 - val_loss: 1.6203 - val_acc: 0.3760\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5845 - acc: 0.3804 - val_loss: 1.5762 - val_acc: 0.3950\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5414 - acc: 0.3967 - val_loss: 1.5351 - val_acc: 0.4150\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5002 - acc: 0.4124 - val_loss: 1.4957 - val_acc: 0.4340\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.4608 - acc: 0.436 - 0s 31us/step - loss: 1.4607 - acc: 0.4369 - val_loss: 1.4594 - val_acc: 0.4550\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.4234 - acc: 0.4585 - val_loss: 1.4229 - val_acc: 0.4760\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3875 - acc: 0.4823 - val_loss: 1.3882 - val_acc: 0.5070\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3527 - acc: 0.5093 - val_loss: 1.3561 - val_acc: 0.5200\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3192 - acc: 0.5353 - val_loss: 1.3226 - val_acc: 0.5490\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2863 - acc: 0.5561 - val_loss: 1.2910 - val_acc: 0.5700\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2542 - acc: 0.5805 - val_loss: 1.2600 - val_acc: 0.5840\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2226 - acc: 0.5965 - val_loss: 1.2307 - val_acc: 0.6020\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1916 - acc: 0.6195 - val_loss: 1.1994 - val_acc: 0.6150\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1607 - acc: 0.6319 - val_loss: 1.1699 - val_acc: 0.6320\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1300 - acc: 0.6468 - val_loss: 1.1404 - val_acc: 0.6350\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.0999 - acc: 0.6596 - val_loss: 1.1113 - val_acc: 0.6510\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0701 - acc: 0.6751 - val_loss: 1.0829 - val_acc: 0.6630\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.0405 - acc: 0.6873 - val_loss: 1.0565 - val_acc: 0.6660\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0115 - acc: 0.6955 - val_loss: 1.0290 - val_acc: 0.6750\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9832 - acc: 0.7067 - val_loss: 1.0018 - val_acc: 0.6870\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9556 - acc: 0.7171 - val_loss: 0.9760 - val_acc: 0.6900\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.9289 - acc: 0.7237 - val_loss: 0.9528 - val_acc: 0.6900\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9037 - acc: 0.7280 - val_loss: 0.9289 - val_acc: 0.7020\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8796 - acc: 0.7333 - val_loss: 0.9081 - val_acc: 0.7020\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8569 - acc: 0.7388 - val_loss: 0.8867 - val_acc: 0.7120\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8355 - acc: 0.7437 - val_loss: 0.8702 - val_acc: 0.7060\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8157 - acc: 0.7481 - val_loss: 0.8510 - val_acc: 0.7080\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7965 - acc: 0.7525 - val_loss: 0.8342 - val_acc: 0.7120\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7786 - acc: 0.7567 - val_loss: 0.8185 - val_acc: 0.7140\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7622 - acc: 0.7604 - val_loss: 0.8055 - val_acc: 0.7210\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.7465 - acc: 0.7628 - val_loss: 0.7916 - val_acc: 0.7280\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7319 - acc: 0.7660 - val_loss: 0.7813 - val_acc: 0.7260\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7182 - acc: 0.7676 - val_loss: 0.7765 - val_acc: 0.7280\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7057 - acc: 0.7693 - val_loss: 0.7587 - val_acc: 0.7250\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6932 - acc: 0.7736 - val_loss: 0.7486 - val_acc: 0.7320\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6818 - acc: 0.7741 - val_loss: 0.7401 - val_acc: 0.7280\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6710 - acc: 0.7775 - val_loss: 0.7333 - val_acc: 0.7300\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6606 - acc: 0.7795 - val_loss: 0.7257 - val_acc: 0.7330\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6512 - acc: 0.7817 - val_loss: 0.7180 - val_acc: 0.7380\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.6419 - acc: 0.7847 - val_loss: 0.7130 - val_acc: 0.7370\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6330 - acc: 0.7857 - val_loss: 0.7060 - val_acc: 0.7410\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6249 - acc: 0.7893 - val_loss: 0.7026 - val_acc: 0.7360\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6167 - acc: 0.7897 - val_loss: 0.6961 - val_acc: 0.7420\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6092 - acc: 0.7919 - val_loss: 0.6892 - val_acc: 0.7410\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.6018 - acc: 0.7933 - val_loss: 0.6862 - val_acc: 0.7420\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5949 - acc: 0.7980 - val_loss: 0.6795 - val_acc: 0.7460\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5883 - acc: 0.7983 - val_loss: 0.6751 - val_acc: 0.7460\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5816 - acc: 0.8017 - val_loss: 0.6738 - val_acc: 0.7460\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5756 - acc: 0.8027 - val_loss: 0.6691 - val_acc: 0.7460\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5693 - acc: 0.8039 - val_loss: 0.6649 - val_acc: 0.7510\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5636 - acc: 0.8064 - val_loss: 0.6603 - val_acc: 0.7510\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5576 - acc: 0.8085 - val_loss: 0.6569 - val_acc: 0.7530\n",
      "Epoch 60/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.5523 - acc: 0.8099 - val_loss: 0.6565 - val_acc: 0.7570\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 30us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5689828497727712, 0.8097333333651224]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7319343857765198, 0.7146666668256124]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 2.6081 - acc: 0.1360 - val_loss: 2.5970 - val_acc: 0.1600\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.5885 - acc: 0.1633 - val_loss: 2.5812 - val_acc: 0.1930\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5726 - acc: 0.1891 - val_loss: 2.5674 - val_acc: 0.2100\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5574 - acc: 0.2021 - val_loss: 2.5525 - val_acc: 0.2220\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5416 - acc: 0.2129 - val_loss: 2.5367 - val_acc: 0.2280\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5249 - acc: 0.2236 - val_loss: 2.5189 - val_acc: 0.2400\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5066 - acc: 0.2373 - val_loss: 2.4991 - val_acc: 0.2580\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.4857 - acc: 0.2491 - val_loss: 2.4766 - val_acc: 0.2670\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.4618 - acc: 0.2640 - val_loss: 2.4511 - val_acc: 0.2820\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.4343 - acc: 0.2881 - val_loss: 2.4223 - val_acc: 0.2930\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.4034 - acc: 0.3109 - val_loss: 2.3908 - val_acc: 0.3220\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.3690 - acc: 0.3419 - val_loss: 2.3559 - val_acc: 0.3500\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.3308 - acc: 0.3736 - val_loss: 2.3171 - val_acc: 0.3750\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.2888 - acc: 0.4143 - val_loss: 2.2752 - val_acc: 0.4260\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.2434 - acc: 0.4593 - val_loss: 2.2297 - val_acc: 0.4570\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.1946 - acc: 0.4948 - val_loss: 2.1812 - val_acc: 0.4830\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.1437 - acc: 0.5312 - val_loss: 2.1304 - val_acc: 0.5270\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.0909 - acc: 0.5647 - val_loss: 2.0791 - val_acc: 0.5580\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.0370 - acc: 0.5865 - val_loss: 2.0261 - val_acc: 0.5790\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9826 - acc: 0.6085 - val_loss: 1.9744 - val_acc: 0.5970\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9283 - acc: 0.6248 - val_loss: 1.9223 - val_acc: 0.6210\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8753 - acc: 0.6441 - val_loss: 1.8727 - val_acc: 0.6320\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8243 - acc: 0.6585 - val_loss: 1.8269 - val_acc: 0.6370\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7759 - acc: 0.6709 - val_loss: 1.7816 - val_acc: 0.6510\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7300 - acc: 0.6812 - val_loss: 1.7403 - val_acc: 0.6660\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6875 - acc: 0.6901 - val_loss: 1.7004 - val_acc: 0.6840\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6471 - acc: 0.6995 - val_loss: 1.6651 - val_acc: 0.6910\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6100 - acc: 0.7072 - val_loss: 1.6309 - val_acc: 0.7030\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5757 - acc: 0.7153 - val_loss: 1.6021 - val_acc: 0.7010\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5437 - acc: 0.7201 - val_loss: 1.5725 - val_acc: 0.7170\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5143 - acc: 0.7273 - val_loss: 1.5463 - val_acc: 0.7190\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4864 - acc: 0.7351 - val_loss: 1.5220 - val_acc: 0.7220\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4616 - acc: 0.7393 - val_loss: 1.4998 - val_acc: 0.7290\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4381 - acc: 0.7424 - val_loss: 1.4807 - val_acc: 0.7280\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4162 - acc: 0.7475 - val_loss: 1.4610 - val_acc: 0.7280\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.3958 - acc: 0.7509 - val_loss: 1.4450 - val_acc: 0.7340\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3768 - acc: 0.7545 - val_loss: 1.4295 - val_acc: 0.7250\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3592 - acc: 0.7593 - val_loss: 1.4111 - val_acc: 0.7370\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3427 - acc: 0.7592 - val_loss: 1.3984 - val_acc: 0.7380\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3269 - acc: 0.7617 - val_loss: 1.3855 - val_acc: 0.7370\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3123 - acc: 0.7672 - val_loss: 1.3727 - val_acc: 0.7460\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2982 - acc: 0.7676 - val_loss: 1.3629 - val_acc: 0.7370\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2852 - acc: 0.7724 - val_loss: 1.3500 - val_acc: 0.7410\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2725 - acc: 0.7739 - val_loss: 1.3399 - val_acc: 0.7410\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2605 - acc: 0.7765 - val_loss: 1.3313 - val_acc: 0.7430\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2497 - acc: 0.7787 - val_loss: 1.3206 - val_acc: 0.7460\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2390 - acc: 0.7812 - val_loss: 1.3119 - val_acc: 0.7440\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2282 - acc: 0.7839 - val_loss: 1.3060 - val_acc: 0.7480\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2189 - acc: 0.7849 - val_loss: 1.2964 - val_acc: 0.7490\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2088 - acc: 0.7857 - val_loss: 1.2897 - val_acc: 0.7500\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1997 - acc: 0.7904 - val_loss: 1.2838 - val_acc: 0.7550\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1907 - acc: 0.7893 - val_loss: 1.2776 - val_acc: 0.7570\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1823 - acc: 0.7924 - val_loss: 1.2729 - val_acc: 0.7570\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1739 - acc: 0.7952 - val_loss: 1.2619 - val_acc: 0.7570\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1653 - acc: 0.7947 - val_loss: 1.2590 - val_acc: 0.7460\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1576 - acc: 0.7957 - val_loss: 1.2507 - val_acc: 0.7580\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1505 - acc: 0.7985 - val_loss: 1.2452 - val_acc: 0.7600\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1428 - acc: 0.8023 - val_loss: 1.2393 - val_acc: 0.7610\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1356 - acc: 0.8008 - val_loss: 1.2349 - val_acc: 0.7650\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1284 - acc: 0.8021 - val_loss: 1.2309 - val_acc: 0.7560\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1218 - acc: 0.8059 - val_loss: 1.2256 - val_acc: 0.7660\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1150 - acc: 0.8059 - val_loss: 1.2177 - val_acc: 0.7580\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1081 - acc: 0.8077 - val_loss: 1.2133 - val_acc: 0.7620\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1018 - acc: 0.8095 - val_loss: 1.2086 - val_acc: 0.7640\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0954 - acc: 0.8109 - val_loss: 1.2048 - val_acc: 0.7690\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0893 - acc: 0.8120 - val_loss: 1.2028 - val_acc: 0.7740\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0831 - acc: 0.8152 - val_loss: 1.1964 - val_acc: 0.7690\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0770 - acc: 0.8156 - val_loss: 1.1915 - val_acc: 0.7650\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0712 - acc: 0.8184 - val_loss: 1.1875 - val_acc: 0.7680\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0649 - acc: 0.8192 - val_loss: 1.1848 - val_acc: 0.7700\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0592 - acc: 0.8225 - val_loss: 1.1807 - val_acc: 0.7710\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0539 - acc: 0.8219 - val_loss: 1.1759 - val_acc: 0.7690\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0481 - acc: 0.8219 - val_loss: 1.1712 - val_acc: 0.7660\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0431 - acc: 0.8245 - val_loss: 1.1686 - val_acc: 0.7680\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0377 - acc: 0.8248 - val_loss: 1.1644 - val_acc: 0.7710\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0322 - acc: 0.8263 - val_loss: 1.1626 - val_acc: 0.7690\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0268 - acc: 0.8296 - val_loss: 1.1592 - val_acc: 0.7710\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0222 - acc: 0.8277 - val_loss: 1.1558 - val_acc: 0.7680\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0168 - acc: 0.8299 - val_loss: 1.1529 - val_acc: 0.7680\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0117 - acc: 0.8324 - val_loss: 1.1485 - val_acc: 0.7690\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0070 - acc: 0.8329 - val_loss: 1.1443 - val_acc: 0.7720\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0023 - acc: 0.8347 - val_loss: 1.1458 - val_acc: 0.7700\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9971 - acc: 0.8363 - val_loss: 1.1376 - val_acc: 0.7700\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9924 - acc: 0.8356 - val_loss: 1.1346 - val_acc: 0.7730\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9877 - acc: 0.8381 - val_loss: 1.1316 - val_acc: 0.7690\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9830 - acc: 0.8407 - val_loss: 1.1316 - val_acc: 0.7680\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9783 - acc: 0.8400 - val_loss: 1.1255 - val_acc: 0.7690\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9737 - acc: 0.8401 - val_loss: 1.1273 - val_acc: 0.7680\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9693 - acc: 0.8413 - val_loss: 1.1240 - val_acc: 0.7630\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9643 - acc: 0.8441 - val_loss: 1.1218 - val_acc: 0.7650\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9606 - acc: 0.8428 - val_loss: 1.1142 - val_acc: 0.7690\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9556 - acc: 0.8437 - val_loss: 1.1142 - val_acc: 0.7690\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9516 - acc: 0.8469 - val_loss: 1.1122 - val_acc: 0.7670\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9472 - acc: 0.8453 - val_loss: 1.1130 - val_acc: 0.7700\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9431 - acc: 0.8493 - val_loss: 1.1040 - val_acc: 0.7720\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9389 - acc: 0.8515 - val_loss: 1.1067 - val_acc: 0.7640\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9349 - acc: 0.8516 - val_loss: 1.0995 - val_acc: 0.7690\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9304 - acc: 0.8528 - val_loss: 1.0997 - val_acc: 0.7670\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9267 - acc: 0.8527 - val_loss: 1.0947 - val_acc: 0.7700\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9225 - acc: 0.8529 - val_loss: 1.0917 - val_acc: 0.7690\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9184 - acc: 0.8556 - val_loss: 1.0894 - val_acc: 0.7680\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9145 - acc: 0.8565 - val_loss: 1.0874 - val_acc: 0.7690\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9107 - acc: 0.8576 - val_loss: 1.0852 - val_acc: 0.7710\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9066 - acc: 0.8589 - val_loss: 1.0841 - val_acc: 0.7680\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9030 - acc: 0.8599 - val_loss: 1.0810 - val_acc: 0.7700\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8994 - acc: 0.8609 - val_loss: 1.0811 - val_acc: 0.7720\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8951 - acc: 0.8624 - val_loss: 1.0788 - val_acc: 0.7660\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8914 - acc: 0.8637 - val_loss: 1.0738 - val_acc: 0.7710\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8877 - acc: 0.8647 - val_loss: 1.0724 - val_acc: 0.7710\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8840 - acc: 0.8656 - val_loss: 1.0685 - val_acc: 0.7660\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8806 - acc: 0.8668 - val_loss: 1.0686 - val_acc: 0.7730\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8768 - acc: 0.8672 - val_loss: 1.0649 - val_acc: 0.7730\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8729 - acc: 0.8683 - val_loss: 1.0650 - val_acc: 0.7730\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8695 - acc: 0.8704 - val_loss: 1.0622 - val_acc: 0.7720\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8660 - acc: 0.8700 - val_loss: 1.0637 - val_acc: 0.7650\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8630 - acc: 0.8699 - val_loss: 1.0582 - val_acc: 0.7700\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8591 - acc: 0.8735 - val_loss: 1.0610 - val_acc: 0.7700\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8558 - acc: 0.8743 - val_loss: 1.0553 - val_acc: 0.7730\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8524 - acc: 0.8744 - val_loss: 1.0517 - val_acc: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8485 - acc: 0.8740 - val_loss: 1.0503 - val_acc: 0.7670\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 15.9618 - acc: 0.1975 - val_loss: 15.5537 - val_acc: 0.2160\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 15.2004 - acc: 0.2237 - val_loss: 14.8052 - val_acc: 0.2190\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 14.4614 - acc: 0.2357 - val_loss: 14.0802 - val_acc: 0.2280\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 13.7445 - acc: 0.2436 - val_loss: 13.3759 - val_acc: 0.2340\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 13.0484 - acc: 0.2528 - val_loss: 12.6915 - val_acc: 0.2410\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 12.3722 - acc: 0.2643 - val_loss: 12.0275 - val_acc: 0.2510\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 11.7164 - acc: 0.2745 - val_loss: 11.3834 - val_acc: 0.2690\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 11.0803 - acc: 0.2959 - val_loss: 10.7583 - val_acc: 0.2910\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 10.4633 - acc: 0.3147 - val_loss: 10.1522 - val_acc: 0.3100\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 9.8659 - acc: 0.3395 - val_loss: 9.5666 - val_acc: 0.3550\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 9.2888 - acc: 0.3665 - val_loss: 9.0009 - val_acc: 0.3770\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 8.7329 - acc: 0.3947 - val_loss: 8.4563 - val_acc: 0.4200\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 8.1991 - acc: 0.4291 - val_loss: 7.9349 - val_acc: 0.4420\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 7.6872 - acc: 0.4616 - val_loss: 7.4345 - val_acc: 0.4710\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 7.1975 - acc: 0.5004 - val_loss: 6.9573 - val_acc: 0.4890\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 6.7301 - acc: 0.5200 - val_loss: 6.5029 - val_acc: 0.5400\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 6.2858 - acc: 0.5552 - val_loss: 6.0711 - val_acc: 0.5810\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 5.8648 - acc: 0.5891 - val_loss: 5.6633 - val_acc: 0.5910\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 5.4673 - acc: 0.6087 - val_loss: 5.2789 - val_acc: 0.6060\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 5.0928 - acc: 0.6236 - val_loss: 4.9162 - val_acc: 0.6270\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 4.7402 - acc: 0.6416 - val_loss: 4.5760 - val_acc: 0.6490\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 4.4108 - acc: 0.6619 - val_loss: 4.2602 - val_acc: 0.6550\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 4.1048 - acc: 0.6684 - val_loss: 3.9667 - val_acc: 0.6620\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 3.8221 - acc: 0.6776 - val_loss: 3.6990 - val_acc: 0.6670\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 3.5614 - acc: 0.6824 - val_loss: 3.4475 - val_acc: 0.6750\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 3.3225 - acc: 0.6852 - val_loss: 3.2196 - val_acc: 0.6890\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.1054 - acc: 0.6923 - val_loss: 3.0158 - val_acc: 0.6900\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.9101 - acc: 0.6947 - val_loss: 2.8321 - val_acc: 0.6870\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.7357 - acc: 0.6965 - val_loss: 2.6689 - val_acc: 0.6930\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.5823 - acc: 0.6972 - val_loss: 2.5261 - val_acc: 0.7010\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.4496 - acc: 0.6975 - val_loss: 2.4048 - val_acc: 0.6980\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.3370 - acc: 0.6980 - val_loss: 2.3005 - val_acc: 0.7030\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.2428 - acc: 0.6992 - val_loss: 2.2184 - val_acc: 0.6990\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.1674 - acc: 0.6988 - val_loss: 2.1525 - val_acc: 0.7060\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.1083 - acc: 0.7000 - val_loss: 2.0986 - val_acc: 0.7060\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0630 - acc: 0.7007 - val_loss: 2.0598 - val_acc: 0.7080\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0294 - acc: 0.7024 - val_loss: 2.0322 - val_acc: 0.7100\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0022 - acc: 0.7039 - val_loss: 2.0066 - val_acc: 0.7060\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.9781 - acc: 0.7016 - val_loss: 1.9859 - val_acc: 0.7080\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.9574 - acc: 0.7060 - val_loss: 1.9659 - val_acc: 0.7040\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9376 - acc: 0.7021 - val_loss: 1.9463 - val_acc: 0.7150\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9193 - acc: 0.7049 - val_loss: 1.9295 - val_acc: 0.7080\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9019 - acc: 0.7051 - val_loss: 1.9138 - val_acc: 0.7120\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8859 - acc: 0.7064 - val_loss: 1.8994 - val_acc: 0.7090\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8706 - acc: 0.7069 - val_loss: 1.8831 - val_acc: 0.7100\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8551 - acc: 0.7061 - val_loss: 1.8713 - val_acc: 0.7100\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8409 - acc: 0.7063 - val_loss: 1.8560 - val_acc: 0.7090\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8276 - acc: 0.7075 - val_loss: 1.8425 - val_acc: 0.7080\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8138 - acc: 0.7092 - val_loss: 1.8291 - val_acc: 0.7180\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8014 - acc: 0.7072 - val_loss: 1.8168 - val_acc: 0.7090\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7887 - acc: 0.7089 - val_loss: 1.8039 - val_acc: 0.7130\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7765 - acc: 0.7091 - val_loss: 1.7937 - val_acc: 0.7140\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7644 - acc: 0.7107 - val_loss: 1.7816 - val_acc: 0.7150\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.7536 - acc: 0.7108 - val_loss: 1.7719 - val_acc: 0.7070\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7423 - acc: 0.7099 - val_loss: 1.7614 - val_acc: 0.7170\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7316 - acc: 0.7096 - val_loss: 1.7505 - val_acc: 0.7090\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7213 - acc: 0.7089 - val_loss: 1.7401 - val_acc: 0.7100\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7111 - acc: 0.7111 - val_loss: 1.7279 - val_acc: 0.7150\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7005 - acc: 0.7115 - val_loss: 1.7195 - val_acc: 0.7150\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6910 - acc: 0.7108 - val_loss: 1.7186 - val_acc: 0.7190\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6814 - acc: 0.7128 - val_loss: 1.7005 - val_acc: 0.7160\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6722 - acc: 0.7127 - val_loss: 1.6936 - val_acc: 0.7170\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6628 - acc: 0.7132 - val_loss: 1.6865 - val_acc: 0.7050\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6536 - acc: 0.7159 - val_loss: 1.6754 - val_acc: 0.7100\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6448 - acc: 0.7143 - val_loss: 1.6674 - val_acc: 0.7150\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6357 - acc: 0.7135 - val_loss: 1.6579 - val_acc: 0.7170\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6269 - acc: 0.7149 - val_loss: 1.6479 - val_acc: 0.7100\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6185 - acc: 0.7157 - val_loss: 1.6402 - val_acc: 0.7160\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6103 - acc: 0.7155 - val_loss: 1.6309 - val_acc: 0.7170\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6014 - acc: 0.7156 - val_loss: 1.6225 - val_acc: 0.7190\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5935 - acc: 0.7173 - val_loss: 1.6184 - val_acc: 0.7090\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5857 - acc: 0.7171 - val_loss: 1.6084 - val_acc: 0.7120\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5771 - acc: 0.7175 - val_loss: 1.6014 - val_acc: 0.7140\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5700 - acc: 0.7179 - val_loss: 1.5964 - val_acc: 0.7100\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5620 - acc: 0.7177 - val_loss: 1.5841 - val_acc: 0.7090\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5533 - acc: 0.7172 - val_loss: 1.5778 - val_acc: 0.7150\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5460 - acc: 0.7183 - val_loss: 1.5705 - val_acc: 0.7130\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5387 - acc: 0.7176 - val_loss: 1.5616 - val_acc: 0.7160\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5315 - acc: 0.7197 - val_loss: 1.5594 - val_acc: 0.7080\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5243 - acc: 0.7191 - val_loss: 1.5492 - val_acc: 0.7150\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5165 - acc: 0.7195 - val_loss: 1.5397 - val_acc: 0.7150\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5096 - acc: 0.7201 - val_loss: 1.5348 - val_acc: 0.7140\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5022 - acc: 0.7221 - val_loss: 1.5273 - val_acc: 0.7160\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4958 - acc: 0.7223 - val_loss: 1.5277 - val_acc: 0.7080\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4888 - acc: 0.7211 - val_loss: 1.5141 - val_acc: 0.7160\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4823 - acc: 0.7219 - val_loss: 1.5116 - val_acc: 0.7200\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4750 - acc: 0.7236 - val_loss: 1.5026 - val_acc: 0.7180\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4686 - acc: 0.7220 - val_loss: 1.4977 - val_acc: 0.7160\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4621 - acc: 0.7227 - val_loss: 1.4879 - val_acc: 0.7180\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4551 - acc: 0.7241 - val_loss: 1.4821 - val_acc: 0.7180\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4489 - acc: 0.7248 - val_loss: 1.4770 - val_acc: 0.7110\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4430 - acc: 0.7228 - val_loss: 1.4695 - val_acc: 0.7150\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4362 - acc: 0.7252 - val_loss: 1.4669 - val_acc: 0.7190\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4304 - acc: 0.7260 - val_loss: 1.4606 - val_acc: 0.7140\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4245 - acc: 0.7256 - val_loss: 1.4513 - val_acc: 0.7120\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4180 - acc: 0.7267 - val_loss: 1.4446 - val_acc: 0.7180\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4122 - acc: 0.7265 - val_loss: 1.4401 - val_acc: 0.7180\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4064 - acc: 0.7268 - val_loss: 1.4433 - val_acc: 0.7180\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4013 - acc: 0.7277 - val_loss: 1.4289 - val_acc: 0.7180\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3942 - acc: 0.7271 - val_loss: 1.4235 - val_acc: 0.7210\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3887 - acc: 0.7255 - val_loss: 1.4182 - val_acc: 0.7190\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3830 - acc: 0.7277 - val_loss: 1.4148 - val_acc: 0.7200\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3775 - acc: 0.7277 - val_loss: 1.4096 - val_acc: 0.7210\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3715 - acc: 0.7287 - val_loss: 1.4048 - val_acc: 0.7230\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3665 - acc: 0.7297 - val_loss: 1.3966 - val_acc: 0.7220\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3613 - acc: 0.7309 - val_loss: 1.3895 - val_acc: 0.7190\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3558 - acc: 0.7301 - val_loss: 1.3878 - val_acc: 0.7160\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3510 - acc: 0.7308 - val_loss: 1.3792 - val_acc: 0.7190\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3453 - acc: 0.7311 - val_loss: 1.3740 - val_acc: 0.7240\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.3406 - acc: 0.733 - 0s 33us/step - loss: 1.3395 - acc: 0.7337 - val_loss: 1.3707 - val_acc: 0.7210\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3359 - acc: 0.7291 - val_loss: 1.3646 - val_acc: 0.7190\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3300 - acc: 0.7315 - val_loss: 1.3609 - val_acc: 0.7210\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3253 - acc: 0.7304 - val_loss: 1.3576 - val_acc: 0.7250\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3202 - acc: 0.7324 - val_loss: 1.3512 - val_acc: 0.7210\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3151 - acc: 0.7352 - val_loss: 1.3457 - val_acc: 0.7200\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3104 - acc: 0.7315 - val_loss: 1.3422 - val_acc: 0.7230\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3053 - acc: 0.7339 - val_loss: 1.3403 - val_acc: 0.7170\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3009 - acc: 0.7328 - val_loss: 1.3324 - val_acc: 0.7220\n",
      "Epoch 119/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2959 - acc: 0.7337 - val_loss: 1.3293 - val_acc: 0.7220\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2911 - acc: 0.7368 - val_loss: 1.3204 - val_acc: 0.7240\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 16.0133 - acc: 0.1571 - val_loss: 15.5912 - val_acc: 0.1780\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 15.2383 - acc: 0.1877 - val_loss: 14.8422 - val_acc: 0.2030\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 14.4995 - acc: 0.2219 - val_loss: 14.1177 - val_acc: 0.2190\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 13.7837 - acc: 0.2460 - val_loss: 13.4146 - val_acc: 0.2440\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 13.0889 - acc: 0.2620 - val_loss: 12.7318 - val_acc: 0.2660\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 12.4145 - acc: 0.2817 - val_loss: 12.0687 - val_acc: 0.2770\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 11.7597 - acc: 0.3032 - val_loss: 11.4256 - val_acc: 0.3010\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 11.1247 - acc: 0.3224 - val_loss: 10.8018 - val_acc: 0.3210\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 10.5090 - acc: 0.3413 - val_loss: 10.1975 - val_acc: 0.3260\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 9.9126 - acc: 0.3572 - val_loss: 9.6118 - val_acc: 0.3570\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 9.3365 - acc: 0.3829 - val_loss: 9.0474 - val_acc: 0.3820\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 8.7816 - acc: 0.4043 - val_loss: 8.5045 - val_acc: 0.4060\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 8.2488 - acc: 0.4263 - val_loss: 7.9842 - val_acc: 0.4300\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 7.7377 - acc: 0.4512 - val_loss: 7.4867 - val_acc: 0.4370\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 7.2497 - acc: 0.4671 - val_loss: 7.0117 - val_acc: 0.4850\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 6.7853 - acc: 0.4889 - val_loss: 6.5594 - val_acc: 0.4930\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 6.3433 - acc: 0.5011 - val_loss: 6.1307 - val_acc: 0.5140\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 5.9247 - acc: 0.5179 - val_loss: 5.7242 - val_acc: 0.5250\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 5.5289 - acc: 0.5263 - val_loss: 5.3406 - val_acc: 0.5510\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 5.1554 - acc: 0.5473 - val_loss: 4.9792 - val_acc: 0.5610\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 4.8040 - acc: 0.5577 - val_loss: 4.6403 - val_acc: 0.5760\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 4.4748 - acc: 0.5719 - val_loss: 4.3212 - val_acc: 0.5790\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 4.1679 - acc: 0.5833 - val_loss: 4.0264 - val_acc: 0.6020\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.8839 - acc: 0.5992 - val_loss: 3.7537 - val_acc: 0.5970\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.6223 - acc: 0.6036 - val_loss: 3.5052 - val_acc: 0.6260\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 3.3829 - acc: 0.6180 - val_loss: 3.2760 - val_acc: 0.6260\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 3.1657 - acc: 0.6247 - val_loss: 3.0720 - val_acc: 0.6250\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.9700 - acc: 0.6316 - val_loss: 2.8854 - val_acc: 0.6320\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.7956 - acc: 0.6376 - val_loss: 2.7251 - val_acc: 0.6330\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.6421 - acc: 0.6420 - val_loss: 2.5804 - val_acc: 0.6510\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5092 - acc: 0.6481 - val_loss: 2.4565 - val_acc: 0.6550\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.3959 - acc: 0.6537 - val_loss: 2.3539 - val_acc: 0.6550\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.3019 - acc: 0.6577 - val_loss: 2.2710 - val_acc: 0.6660\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.2266 - acc: 0.6619 - val_loss: 2.2042 - val_acc: 0.6560\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.1674 - acc: 0.6639 - val_loss: 2.1529 - val_acc: 0.6790\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.1226 - acc: 0.6687 - val_loss: 2.1141 - val_acc: 0.6760\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0886 - acc: 0.6712 - val_loss: 2.0848 - val_acc: 0.6780\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0609 - acc: 0.6767 - val_loss: 2.0582 - val_acc: 0.6790\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.0365 - acc: 0.6772 - val_loss: 2.0361 - val_acc: 0.6770\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.0145 - acc: 0.6801 - val_loss: 2.0141 - val_acc: 0.6870\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9936 - acc: 0.6801 - val_loss: 1.9949 - val_acc: 0.6860\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9738 - acc: 0.6821 - val_loss: 1.9757 - val_acc: 0.6970\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9559 - acc: 0.6863 - val_loss: 1.9595 - val_acc: 0.6940\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9377 - acc: 0.6869 - val_loss: 1.9407 - val_acc: 0.6980\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9203 - acc: 0.6891 - val_loss: 1.9254 - val_acc: 0.7000\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9039 - acc: 0.6900 - val_loss: 1.9096 - val_acc: 0.6980\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8877 - acc: 0.6929 - val_loss: 1.8947 - val_acc: 0.7020\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8722 - acc: 0.6921 - val_loss: 1.8785 - val_acc: 0.6980\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8572 - acc: 0.6941 - val_loss: 1.8640 - val_acc: 0.7000\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8422 - acc: 0.6957 - val_loss: 1.8503 - val_acc: 0.7020\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8282 - acc: 0.6967 - val_loss: 1.8382 - val_acc: 0.7040\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8148 - acc: 0.6979 - val_loss: 1.8239 - val_acc: 0.7000\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8008 - acc: 0.6983 - val_loss: 1.8133 - val_acc: 0.7000\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7881 - acc: 0.6983 - val_loss: 1.7984 - val_acc: 0.6990\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7755 - acc: 0.6983 - val_loss: 1.7876 - val_acc: 0.7050\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7630 - acc: 0.6995 - val_loss: 1.7752 - val_acc: 0.7060\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7511 - acc: 0.6987 - val_loss: 1.7655 - val_acc: 0.7030\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7397 - acc: 0.7011 - val_loss: 1.7539 - val_acc: 0.7030\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7285 - acc: 0.6983 - val_loss: 1.7442 - val_acc: 0.6960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7175 - acc: 0.6993 - val_loss: 1.7329 - val_acc: 0.7040\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7067 - acc: 0.6987 - val_loss: 1.7240 - val_acc: 0.6950\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6965 - acc: 0.7004 - val_loss: 1.7142 - val_acc: 0.6980\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6860 - acc: 0.7012 - val_loss: 1.7050 - val_acc: 0.7060\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6761 - acc: 0.7028 - val_loss: 1.6921 - val_acc: 0.6970\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6669 - acc: 0.7015 - val_loss: 1.6864 - val_acc: 0.7000\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6570 - acc: 0.7036 - val_loss: 1.6756 - val_acc: 0.7060\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6481 - acc: 0.7007 - val_loss: 1.6653 - val_acc: 0.7000\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6391 - acc: 0.7029 - val_loss: 1.6590 - val_acc: 0.7060\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6303 - acc: 0.7029 - val_loss: 1.6486 - val_acc: 0.7050\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6216 - acc: 0.7031 - val_loss: 1.6394 - val_acc: 0.7020\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6130 - acc: 0.7033 - val_loss: 1.6318 - val_acc: 0.7020\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6052 - acc: 0.7051 - val_loss: 1.6252 - val_acc: 0.7020\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5968 - acc: 0.7041 - val_loss: 1.6168 - val_acc: 0.7050\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.5884 - acc: 0.7041 - val_loss: 1.6143 - val_acc: 0.7050\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5809 - acc: 0.7053 - val_loss: 1.6031 - val_acc: 0.7050\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5727 - acc: 0.7056 - val_loss: 1.5953 - val_acc: 0.7070\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5650 - acc: 0.7075 - val_loss: 1.5863 - val_acc: 0.7100\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5575 - acc: 0.7064 - val_loss: 1.5781 - val_acc: 0.7090\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5500 - acc: 0.7072 - val_loss: 1.5731 - val_acc: 0.7080\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5431 - acc: 0.7080 - val_loss: 1.5716 - val_acc: 0.7070\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5363 - acc: 0.7076 - val_loss: 1.5604 - val_acc: 0.7090\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5288 - acc: 0.7088 - val_loss: 1.5526 - val_acc: 0.7090\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5221 - acc: 0.7076 - val_loss: 1.5438 - val_acc: 0.7110\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5148 - acc: 0.7124 - val_loss: 1.5366 - val_acc: 0.7130\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5081 - acc: 0.7111 - val_loss: 1.5288 - val_acc: 0.7110\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.5011 - acc: 0.7104 - val_loss: 1.5249 - val_acc: 0.7090\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4945 - acc: 0.7116 - val_loss: 1.5205 - val_acc: 0.7080\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4884 - acc: 0.7105 - val_loss: 1.5112 - val_acc: 0.7120\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4816 - acc: 0.7119 - val_loss: 1.5076 - val_acc: 0.7150\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4755 - acc: 0.7116 - val_loss: 1.5014 - val_acc: 0.7100\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4690 - acc: 0.7125 - val_loss: 1.4930 - val_acc: 0.7140\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4629 - acc: 0.7129 - val_loss: 1.4935 - val_acc: 0.7100\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4571 - acc: 0.7123 - val_loss: 1.4853 - val_acc: 0.7110\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4511 - acc: 0.7148 - val_loss: 1.4742 - val_acc: 0.7110\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4442 - acc: 0.7143 - val_loss: 1.4680 - val_acc: 0.7130\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4382 - acc: 0.7153 - val_loss: 1.4626 - val_acc: 0.7130\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4323 - acc: 0.7159 - val_loss: 1.4577 - val_acc: 0.7090\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4259 - acc: 0.7176 - val_loss: 1.4562 - val_acc: 0.7130\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4212 - acc: 0.7163 - val_loss: 1.4488 - val_acc: 0.7130\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4149 - acc: 0.7177 - val_loss: 1.4389 - val_acc: 0.7130\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4095 - acc: 0.7161 - val_loss: 1.4371 - val_acc: 0.7120\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4036 - acc: 0.7169 - val_loss: 1.4287 - val_acc: 0.7120\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3979 - acc: 0.7193 - val_loss: 1.4266 - val_acc: 0.7130\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3925 - acc: 0.7179 - val_loss: 1.4170 - val_acc: 0.7170\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3869 - acc: 0.7188 - val_loss: 1.4118 - val_acc: 0.7140\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3816 - acc: 0.7200 - val_loss: 1.4066 - val_acc: 0.7170\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3763 - acc: 0.7204 - val_loss: 1.4027 - val_acc: 0.7130\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3708 - acc: 0.7221 - val_loss: 1.3992 - val_acc: 0.7120\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3661 - acc: 0.7211 - val_loss: 1.3930 - val_acc: 0.7170\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3608 - acc: 0.7213 - val_loss: 1.3884 - val_acc: 0.7170\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3558 - acc: 0.7215 - val_loss: 1.3808 - val_acc: 0.7180\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3500 - acc: 0.7227 - val_loss: 1.3790 - val_acc: 0.7140\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3457 - acc: 0.7235 - val_loss: 1.3731 - val_acc: 0.7180\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3400 - acc: 0.7224 - val_loss: 1.3681 - val_acc: 0.7140\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3359 - acc: 0.7239 - val_loss: 1.3650 - val_acc: 0.7180\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3303 - acc: 0.7239 - val_loss: 1.3595 - val_acc: 0.7200\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3258 - acc: 0.7249 - val_loss: 1.3519 - val_acc: 0.7170\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3214 - acc: 0.7245 - val_loss: 1.3465 - val_acc: 0.7160\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3158 - acc: 0.7256 - val_loss: 1.3449 - val_acc: 0.7200\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3111 - acc: 0.7251 - val_loss: 1.3392 - val_acc: 0.7240\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3064 - acc: 0.7259 - val_loss: 1.3322 - val_acc: 0.7200\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3016 - acc: 0.7252 - val_loss: 1.3317 - val_acc: 0.7220\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2976 - acc: 0.7256 - val_loss: 1.3232 - val_acc: 0.7250\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2926 - acc: 0.7247 - val_loss: 1.3249 - val_acc: 0.7160\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2880 - acc: 0.7260 - val_loss: 1.3145 - val_acc: 0.7210\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2833 - acc: 0.7263 - val_loss: 1.3113 - val_acc: 0.7230\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2790 - acc: 0.7248 - val_loss: 1.3079 - val_acc: 0.7180\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2747 - acc: 0.7279 - val_loss: 1.3011 - val_acc: 0.7200\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2698 - acc: 0.7269 - val_loss: 1.2977 - val_acc: 0.7200\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2652 - acc: 0.7275 - val_loss: 1.2933 - val_acc: 0.7230\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2614 - acc: 0.7293 - val_loss: 1.2906 - val_acc: 0.7220\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2568 - acc: 0.7287 - val_loss: 1.2828 - val_acc: 0.7210\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2525 - acc: 0.7307 - val_loss: 1.2797 - val_acc: 0.7210\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2483 - acc: 0.7299 - val_loss: 1.2751 - val_acc: 0.7210\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2443 - acc: 0.7296 - val_loss: 1.2705 - val_acc: 0.7250\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2398 - acc: 0.7297 - val_loss: 1.2691 - val_acc: 0.7250\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2360 - acc: 0.7305 - val_loss: 1.2683 - val_acc: 0.7210\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2317 - acc: 0.7297 - val_loss: 1.2605 - val_acc: 0.7200\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2280 - acc: 0.7307 - val_loss: 1.2555 - val_acc: 0.7260\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2238 - acc: 0.7317 - val_loss: 1.2506 - val_acc: 0.7220\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2198 - acc: 0.7309 - val_loss: 1.2485 - val_acc: 0.7230\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2160 - acc: 0.7317 - val_loss: 1.2421 - val_acc: 0.7230\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2120 - acc: 0.7320 - val_loss: 1.2394 - val_acc: 0.7250\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2087 - acc: 0.7316 - val_loss: 1.2373 - val_acc: 0.7250\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2044 - acc: 0.7317 - val_loss: 1.2342 - val_acc: 0.7260\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2013 - acc: 0.7328 - val_loss: 1.2285 - val_acc: 0.7210\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1974 - acc: 0.7325 - val_loss: 1.2304 - val_acc: 0.7250\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1938 - acc: 0.7327 - val_loss: 1.2234 - val_acc: 0.7250\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1899 - acc: 0.7331 - val_loss: 1.2196 - val_acc: 0.7250\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1865 - acc: 0.7345 - val_loss: 1.2163 - val_acc: 0.7230\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1835 - acc: 0.7328 - val_loss: 1.2119 - val_acc: 0.7270\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1793 - acc: 0.7351 - val_loss: 1.2056 - val_acc: 0.7240\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1760 - acc: 0.7341 - val_loss: 1.2044 - val_acc: 0.7250\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1721 - acc: 0.7340 - val_loss: 1.1986 - val_acc: 0.7270\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1687 - acc: 0.7351 - val_loss: 1.1999 - val_acc: 0.7250\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1654 - acc: 0.7348 - val_loss: 1.1932 - val_acc: 0.7270\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1619 - acc: 0.7339 - val_loss: 1.1899 - val_acc: 0.7270\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1586 - acc: 0.7352 - val_loss: 1.1858 - val_acc: 0.7230\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1550 - acc: 0.7344 - val_loss: 1.1845 - val_acc: 0.7260\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1522 - acc: 0.7376 - val_loss: 1.1778 - val_acc: 0.7240\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1484 - acc: 0.7351 - val_loss: 1.1793 - val_acc: 0.7240\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1454 - acc: 0.7373 - val_loss: 1.1767 - val_acc: 0.7290\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1425 - acc: 0.7369 - val_loss: 1.1702 - val_acc: 0.7260\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1389 - acc: 0.7368 - val_loss: 1.1678 - val_acc: 0.7240\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1356 - acc: 0.7368 - val_loss: 1.1652 - val_acc: 0.7270\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1332 - acc: 0.7381 - val_loss: 1.1637 - val_acc: 0.7250\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1300 - acc: 0.7381 - val_loss: 1.1577 - val_acc: 0.7250\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1270 - acc: 0.7384 - val_loss: 1.1578 - val_acc: 0.7320\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1236 - acc: 0.7399 - val_loss: 1.1540 - val_acc: 0.7250\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1213 - acc: 0.7397 - val_loss: 1.1497 - val_acc: 0.7260\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1183 - acc: 0.7392 - val_loss: 1.1483 - val_acc: 0.7240\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1153 - acc: 0.7373 - val_loss: 1.1445 - val_acc: 0.7260\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1125 - acc: 0.7396 - val_loss: 1.1429 - val_acc: 0.7250\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1100 - acc: 0.7385 - val_loss: 1.1392 - val_acc: 0.7270\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1071 - acc: 0.7395 - val_loss: 1.1353 - val_acc: 0.7260\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1043 - acc: 0.7368 - val_loss: 1.1346 - val_acc: 0.7310\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.1021 - acc: 0.7408 - val_loss: 1.1339 - val_acc: 0.7250\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0994 - acc: 0.7404 - val_loss: 1.1498 - val_acc: 0.7220\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0978 - acc: 0.7408 - val_loss: 1.1293 - val_acc: 0.7260\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0946 - acc: 0.7424 - val_loss: 1.1259 - val_acc: 0.7300\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0922 - acc: 0.7420 - val_loss: 1.1231 - val_acc: 0.7300\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0889 - acc: 0.7423 - val_loss: 1.1200 - val_acc: 0.7290\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0874 - acc: 0.7405 - val_loss: 1.1180 - val_acc: 0.7270\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0848 - acc: 0.7405 - val_loss: 1.1187 - val_acc: 0.7250\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0826 - acc: 0.7423 - val_loss: 1.1157 - val_acc: 0.7320\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0812 - acc: 0.7428 - val_loss: 1.1100 - val_acc: 0.7300\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0778 - acc: 0.7416 - val_loss: 1.1099 - val_acc: 0.7270\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0756 - acc: 0.7428 - val_loss: 1.1095 - val_acc: 0.7250\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0733 - acc: 0.7439 - val_loss: 1.1025 - val_acc: 0.7300\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0716 - acc: 0.7432 - val_loss: 1.1020 - val_acc: 0.7280\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0690 - acc: 0.7435 - val_loss: 1.0987 - val_acc: 0.7260\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0666 - acc: 0.7439 - val_loss: 1.0984 - val_acc: 0.7260\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0655 - acc: 0.7437 - val_loss: 1.0987 - val_acc: 0.7300\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0626 - acc: 0.7449 - val_loss: 1.0937 - val_acc: 0.7300\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0608 - acc: 0.7452 - val_loss: 1.0912 - val_acc: 0.7290\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0584 - acc: 0.7445 - val_loss: 1.0923 - val_acc: 0.7320\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0567 - acc: 0.7447 - val_loss: 1.0915 - val_acc: 0.7330\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0551 - acc: 0.7457 - val_loss: 1.0849 - val_acc: 0.7300\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0525 - acc: 0.7465 - val_loss: 1.0860 - val_acc: 0.7280\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0511 - acc: 0.7469 - val_loss: 1.0814 - val_acc: 0.7310\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0481 - acc: 0.7459 - val_loss: 1.0807 - val_acc: 0.7270\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0470 - acc: 0.7472 - val_loss: 1.0795 - val_acc: 0.7260\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0451 - acc: 0.7467 - val_loss: 1.0758 - val_acc: 0.7320\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0436 - acc: 0.7473 - val_loss: 1.0796 - val_acc: 0.7340\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0412 - acc: 0.7476 - val_loss: 1.0715 - val_acc: 0.7330\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0392 - acc: 0.7483 - val_loss: 1.0702 - val_acc: 0.7330\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0378 - acc: 0.7483 - val_loss: 1.0702 - val_acc: 0.7340\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0360 - acc: 0.7487 - val_loss: 1.0690 - val_acc: 0.7360\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0352 - acc: 0.7500 - val_loss: 1.0734 - val_acc: 0.7320\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0336 - acc: 0.7483 - val_loss: 1.0636 - val_acc: 0.7310\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0308 - acc: 0.7503 - val_loss: 1.0674 - val_acc: 0.7340\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0299 - acc: 0.7493 - val_loss: 1.0627 - val_acc: 0.7310\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0279 - acc: 0.7477 - val_loss: 1.0623 - val_acc: 0.7280\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0263 - acc: 0.7491 - val_loss: 1.0600 - val_acc: 0.7320\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0247 - acc: 0.7517 - val_loss: 1.0576 - val_acc: 0.7320\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0229 - acc: 0.7508 - val_loss: 1.0611 - val_acc: 0.7300\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0222 - acc: 0.7512 - val_loss: 1.0539 - val_acc: 0.7290\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0200 - acc: 0.7512 - val_loss: 1.0526 - val_acc: 0.7370\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0181 - acc: 0.7504 - val_loss: 1.0531 - val_acc: 0.7320\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0172 - acc: 0.7513 - val_loss: 1.0498 - val_acc: 0.7360\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0151 - acc: 0.7520 - val_loss: 1.0480 - val_acc: 0.7330\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0146 - acc: 0.7524 - val_loss: 1.0470 - val_acc: 0.7370\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0125 - acc: 0.7551 - val_loss: 1.0524 - val_acc: 0.7350\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0117 - acc: 0.7509 - val_loss: 1.0439 - val_acc: 0.7330\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0096 - acc: 0.7539 - val_loss: 1.0459 - val_acc: 0.7340\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0086 - acc: 0.7531 - val_loss: 1.0423 - val_acc: 0.7310\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0072 - acc: 0.7528 - val_loss: 1.0402 - val_acc: 0.7350\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0062 - acc: 0.7537 - val_loss: 1.0434 - val_acc: 0.7350\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.0046 - acc: 0.7551 - val_loss: 1.0397 - val_acc: 0.7360\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0035 - acc: 0.7541 - val_loss: 1.0354 - val_acc: 0.7350\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.0016 - acc: 0.7532 - val_loss: 1.0384 - val_acc: 0.7340\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0002 - acc: 0.7553 - val_loss: 1.0341 - val_acc: 0.7400\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9992 - acc: 0.7571 - val_loss: 1.0341 - val_acc: 0.7320\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9979 - acc: 0.7552 - val_loss: 1.0375 - val_acc: 0.7410\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9966 - acc: 0.7544 - val_loss: 1.0311 - val_acc: 0.7390\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9949 - acc: 0.7557 - val_loss: 1.0343 - val_acc: 0.7360\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9946 - acc: 0.7572 - val_loss: 1.0319 - val_acc: 0.7350\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9925 - acc: 0.7564 - val_loss: 1.0328 - val_acc: 0.7360\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9917 - acc: 0.7564 - val_loss: 1.0274 - val_acc: 0.7400\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9903 - acc: 0.7575 - val_loss: 1.0271 - val_acc: 0.7400\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9891 - acc: 0.7568 - val_loss: 1.0268 - val_acc: 0.7370\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9878 - acc: 0.7596 - val_loss: 1.0252 - val_acc: 0.7340\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9869 - acc: 0.7567 - val_loss: 1.0219 - val_acc: 0.7460\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9856 - acc: 0.7576 - val_loss: 1.0290 - val_acc: 0.7420\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9852 - acc: 0.7577 - val_loss: 1.0212 - val_acc: 0.7400\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9829 - acc: 0.7577 - val_loss: 1.0227 - val_acc: 0.7380\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9828 - acc: 0.7579 - val_loss: 1.0192 - val_acc: 0.7390\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9813 - acc: 0.7591 - val_loss: 1.0235 - val_acc: 0.7420\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9806 - acc: 0.7595 - val_loss: 1.0194 - val_acc: 0.7390\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9794 - acc: 0.7579 - val_loss: 1.0155 - val_acc: 0.7430\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9787 - acc: 0.7604 - val_loss: 1.0179 - val_acc: 0.7430\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9775 - acc: 0.7591 - val_loss: 1.0165 - val_acc: 0.7430\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9767 - acc: 0.7604 - val_loss: 1.0136 - val_acc: 0.7440\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9747 - acc: 0.7603 - val_loss: 1.0156 - val_acc: 0.7400\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9741 - acc: 0.7584 - val_loss: 1.0128 - val_acc: 0.7460\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9727 - acc: 0.7617 - val_loss: 1.0113 - val_acc: 0.7450\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9717 - acc: 0.7613 - val_loss: 1.0102 - val_acc: 0.7430\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9720 - acc: 0.7592 - val_loss: 1.0169 - val_acc: 0.7400\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9700 - acc: 0.7623 - val_loss: 1.0101 - val_acc: 0.7440\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9694 - acc: 0.7600 - val_loss: 1.0074 - val_acc: 0.7410\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9679 - acc: 0.7624 - val_loss: 1.0079 - val_acc: 0.7400\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9675 - acc: 0.7596 - val_loss: 1.0067 - val_acc: 0.7380\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9655 - acc: 0.7627 - val_loss: 1.0094 - val_acc: 0.7370\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9659 - acc: 0.7623 - val_loss: 1.0055 - val_acc: 0.7400\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9648 - acc: 0.7636 - val_loss: 1.0140 - val_acc: 0.7450\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9640 - acc: 0.7625 - val_loss: 1.0060 - val_acc: 0.7420\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9627 - acc: 0.7627 - val_loss: 1.0029 - val_acc: 0.7430\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9617 - acc: 0.7639 - val_loss: 1.0009 - val_acc: 0.7420\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9604 - acc: 0.7633 - val_loss: 1.0045 - val_acc: 0.7410\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9602 - acc: 0.7633 - val_loss: 1.0019 - val_acc: 0.7370\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9591 - acc: 0.7668 - val_loss: 0.9981 - val_acc: 0.7460\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9568 - acc: 0.7641 - val_loss: 1.0009 - val_acc: 0.7440\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9565 - acc: 0.7651 - val_loss: 0.9963 - val_acc: 0.7460\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9551 - acc: 0.7655 - val_loss: 0.9959 - val_acc: 0.7430\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9548 - acc: 0.7661 - val_loss: 1.0049 - val_acc: 0.7400\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9541 - acc: 0.7659 - val_loss: 0.9946 - val_acc: 0.7450\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9526 - acc: 0.7643 - val_loss: 0.9953 - val_acc: 0.7440\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9519 - acc: 0.7669 - val_loss: 0.9970 - val_acc: 0.7390\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9512 - acc: 0.7667 - val_loss: 0.9924 - val_acc: 0.7480\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9499 - acc: 0.7675 - val_loss: 0.9929 - val_acc: 0.7460\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9493 - acc: 0.7665 - val_loss: 1.0002 - val_acc: 0.7380\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9492 - acc: 0.7672 - val_loss: 0.9893 - val_acc: 0.7480\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9479 - acc: 0.7672 - val_loss: 0.9956 - val_acc: 0.7450\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9479 - acc: 0.7681 - val_loss: 0.9950 - val_acc: 0.7470\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9470 - acc: 0.7687 - val_loss: 0.9920 - val_acc: 0.7460\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9449 - acc: 0.7676 - val_loss: 0.9886 - val_acc: 0.7440\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9440 - acc: 0.7679 - val_loss: 0.9861 - val_acc: 0.7460\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9430 - acc: 0.7680 - val_loss: 0.9918 - val_acc: 0.7450\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9436 - acc: 0.7669 - val_loss: 0.9884 - val_acc: 0.7420\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9423 - acc: 0.7676 - val_loss: 0.9842 - val_acc: 0.7460\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9409 - acc: 0.7699 - val_loss: 0.9851 - val_acc: 0.7460\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9404 - acc: 0.7705 - val_loss: 0.9833 - val_acc: 0.7490\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9393 - acc: 0.7684 - val_loss: 0.9861 - val_acc: 0.7450\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9385 - acc: 0.7665 - val_loss: 0.9878 - val_acc: 0.7430\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9389 - acc: 0.7656 - val_loss: 0.9819 - val_acc: 0.7460\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9366 - acc: 0.7688 - val_loss: 0.9843 - val_acc: 0.7440\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9366 - acc: 0.7691 - val_loss: 0.9822 - val_acc: 0.7560\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9356 - acc: 0.7693 - val_loss: 0.9799 - val_acc: 0.7480\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9351 - acc: 0.7693 - val_loss: 0.9883 - val_acc: 0.7410\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9343 - acc: 0.7703 - val_loss: 0.9826 - val_acc: 0.7460\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9328 - acc: 0.7692 - val_loss: 0.9780 - val_acc: 0.7460\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9322 - acc: 0.7695 - val_loss: 0.9853 - val_acc: 0.7440\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9332 - acc: 0.7687 - val_loss: 0.9798 - val_acc: 0.7400\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9317 - acc: 0.7692 - val_loss: 0.9746 - val_acc: 0.7490\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9302 - acc: 0.7677 - val_loss: 0.9765 - val_acc: 0.7440\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9291 - acc: 0.7721 - val_loss: 0.9762 - val_acc: 0.7430\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9290 - acc: 0.7707 - val_loss: 0.9752 - val_acc: 0.7460\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9283 - acc: 0.7703 - val_loss: 0.9738 - val_acc: 0.7440\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9268 - acc: 0.7703 - val_loss: 0.9751 - val_acc: 0.7460\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9267 - acc: 0.7705 - val_loss: 0.9778 - val_acc: 0.7490\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9268 - acc: 0.7704 - val_loss: 0.9717 - val_acc: 0.7500\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9251 - acc: 0.7720 - val_loss: 0.9834 - val_acc: 0.7420\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9252 - acc: 0.7696 - val_loss: 0.9791 - val_acc: 0.7420\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9240 - acc: 0.7697 - val_loss: 0.9761 - val_acc: 0.7490\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9234 - acc: 0.7715 - val_loss: 0.9733 - val_acc: 0.7580\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9232 - acc: 0.7712 - val_loss: 0.9752 - val_acc: 0.7550\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9220 - acc: 0.7711 - val_loss: 0.9707 - val_acc: 0.7460\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9207 - acc: 0.7724 - val_loss: 0.9775 - val_acc: 0.7470\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9213 - acc: 0.7715 - val_loss: 0.9698 - val_acc: 0.7470\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9194 - acc: 0.7741 - val_loss: 0.9770 - val_acc: 0.7440\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9193 - acc: 0.7732 - val_loss: 0.9784 - val_acc: 0.7510\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9189 - acc: 0.7716 - val_loss: 0.9694 - val_acc: 0.7480\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9179 - acc: 0.7735 - val_loss: 0.9670 - val_acc: 0.7510\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9171 - acc: 0.7729 - val_loss: 0.9680 - val_acc: 0.7480\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9166 - acc: 0.7731 - val_loss: 0.9672 - val_acc: 0.7560\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9158 - acc: 0.7751 - val_loss: 0.9661 - val_acc: 0.7550\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9152 - acc: 0.7731 - val_loss: 0.9661 - val_acc: 0.7580\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9148 - acc: 0.7743 - val_loss: 0.9660 - val_acc: 0.7490\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9137 - acc: 0.7741 - val_loss: 0.9658 - val_acc: 0.7490\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9137 - acc: 0.7719 - val_loss: 0.9684 - val_acc: 0.7500\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9140 - acc: 0.7732 - val_loss: 0.9631 - val_acc: 0.7570\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9120 - acc: 0.7752 - val_loss: 0.9608 - val_acc: 0.7490\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9113 - acc: 0.7748 - val_loss: 0.9639 - val_acc: 0.7500\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9108 - acc: 0.7743 - val_loss: 0.9802 - val_acc: 0.7480\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.9123 - acc: 0.772 - 0s 33us/step - loss: 0.9115 - acc: 0.7727 - val_loss: 0.9610 - val_acc: 0.7480\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9100 - acc: 0.7736 - val_loss: 0.9634 - val_acc: 0.7570\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9092 - acc: 0.7747 - val_loss: 0.9587 - val_acc: 0.7580\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9086 - acc: 0.7756 - val_loss: 0.9641 - val_acc: 0.7490\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9093 - acc: 0.7743 - val_loss: 0.9626 - val_acc: 0.7490\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9078 - acc: 0.7753 - val_loss: 0.9651 - val_acc: 0.7500\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9069 - acc: 0.7771 - val_loss: 0.9579 - val_acc: 0.7550\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9059 - acc: 0.7745 - val_loss: 0.9576 - val_acc: 0.7510\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9054 - acc: 0.7780 - val_loss: 0.9702 - val_acc: 0.7470\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9060 - acc: 0.7765 - val_loss: 0.9826 - val_acc: 0.7360\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9065 - acc: 0.7727 - val_loss: 0.9655 - val_acc: 0.7480\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9044 - acc: 0.7773 - val_loss: 0.9593 - val_acc: 0.7590\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9039 - acc: 0.7756 - val_loss: 0.9604 - val_acc: 0.7500\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9033 - acc: 0.7767 - val_loss: 0.9621 - val_acc: 0.7480\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9028 - acc: 0.7773 - val_loss: 0.9749 - val_acc: 0.7440\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9029 - acc: 0.7763 - val_loss: 0.9535 - val_acc: 0.7590\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9017 - acc: 0.7779 - val_loss: 0.9621 - val_acc: 0.7500\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.9022 - acc: 0.7769 - val_loss: 0.9640 - val_acc: 0.7450\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8997 - acc: 0.7779 - val_loss: 0.9536 - val_acc: 0.7480\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.9003 - acc: 0.7757 - val_loss: 0.9578 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8992 - acc: 0.7787 - val_loss: 0.9544 - val_acc: 0.7500\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8989 - acc: 0.7773 - val_loss: 0.9501 - val_acc: 0.7530\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8985 - acc: 0.7772 - val_loss: 0.9541 - val_acc: 0.7530\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8983 - acc: 0.7773 - val_loss: 0.9528 - val_acc: 0.7570\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8971 - acc: 0.7781 - val_loss: 0.9543 - val_acc: 0.7530\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8971 - acc: 0.7772 - val_loss: 0.9520 - val_acc: 0.7620\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8961 - acc: 0.7759 - val_loss: 0.9560 - val_acc: 0.7480\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8963 - acc: 0.7773 - val_loss: 0.9509 - val_acc: 0.7500\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8947 - acc: 0.7780 - val_loss: 0.9555 - val_acc: 0.7460\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8949 - acc: 0.7757 - val_loss: 0.9505 - val_acc: 0.7490\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8948 - acc: 0.7783 - val_loss: 0.9518 - val_acc: 0.7520\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8935 - acc: 0.7765 - val_loss: 0.9465 - val_acc: 0.7530\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8933 - acc: 0.7800 - val_loss: 0.9506 - val_acc: 0.7500\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8938 - acc: 0.7775 - val_loss: 0.9472 - val_acc: 0.7530\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8926 - acc: 0.7781 - val_loss: 0.9483 - val_acc: 0.7530\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8925 - acc: 0.7797 - val_loss: 0.9498 - val_acc: 0.7500\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8921 - acc: 0.7799 - val_loss: 0.9512 - val_acc: 0.7490\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8913 - acc: 0.7764 - val_loss: 0.9466 - val_acc: 0.7520\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8905 - acc: 0.7805 - val_loss: 0.9481 - val_acc: 0.7560\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8900 - acc: 0.7796 - val_loss: 0.9510 - val_acc: 0.7560\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8905 - acc: 0.7793 - val_loss: 0.9505 - val_acc: 0.7490\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8893 - acc: 0.7803 - val_loss: 0.9470 - val_acc: 0.7590\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8889 - acc: 0.7805 - val_loss: 0.9541 - val_acc: 0.7500\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8890 - acc: 0.7801 - val_loss: 0.9512 - val_acc: 0.7450\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8881 - acc: 0.7789 - val_loss: 0.9592 - val_acc: 0.7420\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8877 - acc: 0.7792 - val_loss: 0.9502 - val_acc: 0.7470\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8880 - acc: 0.7803 - val_loss: 0.9466 - val_acc: 0.7440\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8873 - acc: 0.7824 - val_loss: 0.9555 - val_acc: 0.7400\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8874 - acc: 0.7807 - val_loss: 0.9523 - val_acc: 0.7500\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8865 - acc: 0.7809 - val_loss: 0.9429 - val_acc: 0.7540\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8859 - acc: 0.7807 - val_loss: 0.9443 - val_acc: 0.7500\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8862 - acc: 0.7815 - val_loss: 0.9495 - val_acc: 0.7380\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8844 - acc: 0.7808 - val_loss: 0.9420 - val_acc: 0.7520\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8840 - acc: 0.7832 - val_loss: 0.9479 - val_acc: 0.7570\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8836 - acc: 0.7841 - val_loss: 0.9416 - val_acc: 0.7500\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8843 - acc: 0.7825 - val_loss: 0.9425 - val_acc: 0.7490\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8821 - acc: 0.7823 - val_loss: 0.9448 - val_acc: 0.7550\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8826 - acc: 0.7817 - val_loss: 0.9442 - val_acc: 0.7470\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8819 - acc: 0.7824 - val_loss: 0.9440 - val_acc: 0.7540\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8819 - acc: 0.7840 - val_loss: 0.9441 - val_acc: 0.7470\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8813 - acc: 0.7813 - val_loss: 0.9480 - val_acc: 0.7490\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8818 - acc: 0.7804 - val_loss: 0.9447 - val_acc: 0.7490\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8809 - acc: 0.7805 - val_loss: 0.9407 - val_acc: 0.7530\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8794 - acc: 0.7833 - val_loss: 0.9428 - val_acc: 0.7500\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8797 - acc: 0.7812 - val_loss: 0.9400 - val_acc: 0.7500\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8789 - acc: 0.7809 - val_loss: 0.9374 - val_acc: 0.7500\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8799 - acc: 0.7839 - val_loss: 0.9399 - val_acc: 0.7500\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8786 - acc: 0.7839 - val_loss: 0.9390 - val_acc: 0.7550\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8788 - acc: 0.7816 - val_loss: 0.9526 - val_acc: 0.7390\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8787 - acc: 0.7804 - val_loss: 0.9412 - val_acc: 0.7450\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8784 - acc: 0.7828 - val_loss: 0.9373 - val_acc: 0.7510\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8783 - acc: 0.7828 - val_loss: 0.9397 - val_acc: 0.7500\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8763 - acc: 0.7843 - val_loss: 0.9387 - val_acc: 0.7510\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8771 - acc: 0.7839 - val_loss: 0.9384 - val_acc: 0.7530\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8759 - acc: 0.7844 - val_loss: 0.9491 - val_acc: 0.7460\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8769 - acc: 0.7816 - val_loss: 0.9379 - val_acc: 0.7530\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8748 - acc: 0.7813 - val_loss: 0.9366 - val_acc: 0.7530\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8744 - acc: 0.7823 - val_loss: 0.9506 - val_acc: 0.7570\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8753 - acc: 0.7843 - val_loss: 0.9449 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8742 - acc: 0.7839 - val_loss: 0.9350 - val_acc: 0.7500\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8736 - acc: 0.7848 - val_loss: 0.9412 - val_acc: 0.7500\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8736 - acc: 0.7843 - val_loss: 0.9373 - val_acc: 0.7560\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8733 - acc: 0.7841 - val_loss: 0.9407 - val_acc: 0.7550\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8738 - acc: 0.7821 - val_loss: 0.9420 - val_acc: 0.7560\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8731 - acc: 0.7845 - val_loss: 0.9336 - val_acc: 0.7520\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8712 - acc: 0.7840 - val_loss: 0.9387 - val_acc: 0.7550\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8727 - acc: 0.7845 - val_loss: 0.9356 - val_acc: 0.7510\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8716 - acc: 0.7851 - val_loss: 0.9582 - val_acc: 0.7550\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8717 - acc: 0.7837 - val_loss: 0.9376 - val_acc: 0.7440\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8725 - acc: 0.7857 - val_loss: 0.9338 - val_acc: 0.7450\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8717 - acc: 0.7839 - val_loss: 0.9322 - val_acc: 0.7570\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8689 - acc: 0.7868 - val_loss: 0.9444 - val_acc: 0.7370\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8720 - acc: 0.7833 - val_loss: 0.9376 - val_acc: 0.7410\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8700 - acc: 0.7868 - val_loss: 0.9315 - val_acc: 0.7530\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8695 - acc: 0.7852 - val_loss: 0.9341 - val_acc: 0.7530\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8696 - acc: 0.7836 - val_loss: 0.9405 - val_acc: 0.7450\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8691 - acc: 0.7843 - val_loss: 0.9310 - val_acc: 0.7500\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8677 - acc: 0.7853 - val_loss: 0.9332 - val_acc: 0.7510\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8681 - acc: 0.7860 - val_loss: 0.9290 - val_acc: 0.7470\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8677 - acc: 0.7861 - val_loss: 0.9301 - val_acc: 0.7510\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8674 - acc: 0.7851 - val_loss: 0.9475 - val_acc: 0.7390\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8671 - acc: 0.7864 - val_loss: 0.9329 - val_acc: 0.7450\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8665 - acc: 0.7883 - val_loss: 0.9316 - val_acc: 0.7550\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8662 - acc: 0.7869 - val_loss: 0.9662 - val_acc: 0.7300\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8667 - acc: 0.7847 - val_loss: 0.9314 - val_acc: 0.7450\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8650 - acc: 0.7859 - val_loss: 0.9326 - val_acc: 0.7460\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8655 - acc: 0.7863 - val_loss: 0.9306 - val_acc: 0.7480\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8657 - acc: 0.7872 - val_loss: 0.9417 - val_acc: 0.7370\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8657 - acc: 0.7863 - val_loss: 0.9334 - val_acc: 0.7480\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8645 - acc: 0.7867 - val_loss: 0.9361 - val_acc: 0.7400\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8667 - acc: 0.7852 - val_loss: 0.9309 - val_acc: 0.7430\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8644 - acc: 0.7848 - val_loss: 0.9563 - val_acc: 0.7350\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8644 - acc: 0.7849 - val_loss: 0.9369 - val_acc: 0.7490\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8636 - acc: 0.7864 - val_loss: 0.9407 - val_acc: 0.7570\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8639 - acc: 0.7841 - val_loss: 0.9320 - val_acc: 0.7510\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8622 - acc: 0.7873 - val_loss: 0.9284 - val_acc: 0.7500\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8620 - acc: 0.7871 - val_loss: 0.9273 - val_acc: 0.7560\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8629 - acc: 0.7856 - val_loss: 0.9305 - val_acc: 0.7490\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8627 - acc: 0.7881 - val_loss: 0.9366 - val_acc: 0.7480\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8612 - acc: 0.7876 - val_loss: 0.9393 - val_acc: 0.7480\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8627 - acc: 0.7857 - val_loss: 0.9307 - val_acc: 0.7460\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8611 - acc: 0.7864 - val_loss: 0.9272 - val_acc: 0.7510\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8615 - acc: 0.7885 - val_loss: 0.9292 - val_acc: 0.7390\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8613 - acc: 0.7879 - val_loss: 0.9345 - val_acc: 0.7490\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8593 - acc: 0.7875 - val_loss: 0.9307 - val_acc: 0.7430\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8597 - acc: 0.7895 - val_loss: 0.9311 - val_acc: 0.7520\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8598 - acc: 0.7847 - val_loss: 0.9247 - val_acc: 0.7540\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8595 - acc: 0.7880 - val_loss: 0.9359 - val_acc: 0.7480\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8596 - acc: 0.7876 - val_loss: 0.9274 - val_acc: 0.7490\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8576 - acc: 0.7897 - val_loss: 0.9334 - val_acc: 0.7530\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8586 - acc: 0.7889 - val_loss: 0.9428 - val_acc: 0.7450\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8600 - acc: 0.7868 - val_loss: 0.9296 - val_acc: 0.7530\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8580 - acc: 0.7877 - val_loss: 0.9275 - val_acc: 0.7440\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8578 - acc: 0.7877 - val_loss: 0.9361 - val_acc: 0.7550\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8591 - acc: 0.7871 - val_loss: 0.9286 - val_acc: 0.7490\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8579 - acc: 0.7883 - val_loss: 0.9253 - val_acc: 0.7530\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8568 - acc: 0.7893 - val_loss: 0.9280 - val_acc: 0.7510\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8573 - acc: 0.7871 - val_loss: 0.9229 - val_acc: 0.7470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8560 - acc: 0.7881 - val_loss: 0.9280 - val_acc: 0.7500\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8563 - acc: 0.7872 - val_loss: 0.9281 - val_acc: 0.7520\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8558 - acc: 0.7897 - val_loss: 0.9238 - val_acc: 0.7470\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8557 - acc: 0.7872 - val_loss: 0.9285 - val_acc: 0.7420\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8549 - acc: 0.7880 - val_loss: 0.9323 - val_acc: 0.7430\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8557 - acc: 0.7883 - val_loss: 0.9246 - val_acc: 0.7480\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8548 - acc: 0.7889 - val_loss: 0.9235 - val_acc: 0.7460\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8536 - acc: 0.7903 - val_loss: 0.9378 - val_acc: 0.7420\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8554 - acc: 0.7877 - val_loss: 0.9268 - val_acc: 0.7390\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8546 - acc: 0.7913 - val_loss: 0.9252 - val_acc: 0.7440\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8549 - acc: 0.7896 - val_loss: 0.9208 - val_acc: 0.7480\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8533 - acc: 0.7873 - val_loss: 0.9245 - val_acc: 0.7410\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8533 - acc: 0.7912 - val_loss: 0.9363 - val_acc: 0.7480\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8540 - acc: 0.7877 - val_loss: 0.9286 - val_acc: 0.7360\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8528 - acc: 0.7900 - val_loss: 0.9307 - val_acc: 0.7500\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8524 - acc: 0.7899 - val_loss: 0.9362 - val_acc: 0.7380\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8523 - acc: 0.7895 - val_loss: 0.9293 - val_acc: 0.7460\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8533 - acc: 0.7912 - val_loss: 0.9246 - val_acc: 0.7510\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8522 - acc: 0.7899 - val_loss: 0.9207 - val_acc: 0.7500\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8524 - acc: 0.7900 - val_loss: 0.9190 - val_acc: 0.7450\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8522 - acc: 0.7903 - val_loss: 0.9258 - val_acc: 0.7490\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8513 - acc: 0.7905 - val_loss: 0.9306 - val_acc: 0.7550\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8523 - acc: 0.7883 - val_loss: 0.9411 - val_acc: 0.7520\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8513 - acc: 0.7901 - val_loss: 0.9213 - val_acc: 0.7460\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8503 - acc: 0.7904 - val_loss: 0.9200 - val_acc: 0.7470\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8499 - acc: 0.7912 - val_loss: 0.9324 - val_acc: 0.7460\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8503 - acc: 0.7896 - val_loss: 0.9302 - val_acc: 0.7430\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8509 - acc: 0.7917 - val_loss: 0.9182 - val_acc: 0.7470\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8498 - acc: 0.7904 - val_loss: 0.9183 - val_acc: 0.7460\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8488 - acc: 0.7900 - val_loss: 0.9179 - val_acc: 0.7530\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8491 - acc: 0.7921 - val_loss: 0.9195 - val_acc: 0.7520\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8486 - acc: 0.7916 - val_loss: 0.9216 - val_acc: 0.7580\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8492 - acc: 0.7909 - val_loss: 0.9212 - val_acc: 0.7510\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8492 - acc: 0.7933 - val_loss: 0.9506 - val_acc: 0.7440\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8499 - acc: 0.7907 - val_loss: 0.9216 - val_acc: 0.7440\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8496 - acc: 0.7909 - val_loss: 0.9196 - val_acc: 0.7510\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8480 - acc: 0.7897 - val_loss: 0.9353 - val_acc: 0.7360\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8480 - acc: 0.7921 - val_loss: 0.9291 - val_acc: 0.7510\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8479 - acc: 0.7915 - val_loss: 0.9169 - val_acc: 0.7460\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8476 - acc: 0.7917 - val_loss: 0.9215 - val_acc: 0.7490\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8465 - acc: 0.7919 - val_loss: 0.9260 - val_acc: 0.7510\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8480 - acc: 0.7903 - val_loss: 0.9229 - val_acc: 0.7430\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8466 - acc: 0.7912 - val_loss: 0.9213 - val_acc: 0.7460\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8464 - acc: 0.7933 - val_loss: 0.9258 - val_acc: 0.7430\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8472 - acc: 0.7907 - val_loss: 0.9210 - val_acc: 0.7520\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8452 - acc: 0.7932 - val_loss: 0.9151 - val_acc: 0.7480\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8458 - acc: 0.7905 - val_loss: 0.9216 - val_acc: 0.7470\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8462 - acc: 0.7925 - val_loss: 0.9271 - val_acc: 0.7520\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8446 - acc: 0.7929 - val_loss: 0.9152 - val_acc: 0.7490\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8453 - acc: 0.7900 - val_loss: 0.9167 - val_acc: 0.7510\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8451 - acc: 0.7933 - val_loss: 0.9182 - val_acc: 0.7450\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8454 - acc: 0.7933 - val_loss: 0.9151 - val_acc: 0.7560\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8442 - acc: 0.7928 - val_loss: 0.9169 - val_acc: 0.7470\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8445 - acc: 0.7939 - val_loss: 0.9182 - val_acc: 0.7590\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8447 - acc: 0.7909 - val_loss: 0.9193 - val_acc: 0.7410\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8425 - acc: 0.7912 - val_loss: 0.9193 - val_acc: 0.7500\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8445 - acc: 0.7919 - val_loss: 0.9165 - val_acc: 0.7490\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8426 - acc: 0.7935 - val_loss: 0.9181 - val_acc: 0.7490\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8434 - acc: 0.7923 - val_loss: 0.9158 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8424 - acc: 0.7908 - val_loss: 0.9293 - val_acc: 0.7440\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8432 - acc: 0.7903 - val_loss: 0.9185 - val_acc: 0.7540\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8420 - acc: 0.7936 - val_loss: 0.9290 - val_acc: 0.7520\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8426 - acc: 0.7928 - val_loss: 0.9192 - val_acc: 0.7530\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8418 - acc: 0.7924 - val_loss: 0.9305 - val_acc: 0.7510\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8428 - acc: 0.7943 - val_loss: 0.9199 - val_acc: 0.7510\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8434 - acc: 0.7924 - val_loss: 0.9167 - val_acc: 0.7530\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8401 - acc: 0.7925 - val_loss: 0.9273 - val_acc: 0.7370\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8434 - acc: 0.7920 - val_loss: 0.9222 - val_acc: 0.7480\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8427 - acc: 0.7971 - val_loss: 0.9175 - val_acc: 0.7510\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8415 - acc: 0.7924 - val_loss: 0.9294 - val_acc: 0.7550\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8413 - acc: 0.7913 - val_loss: 0.9137 - val_acc: 0.7550\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8402 - acc: 0.7927 - val_loss: 0.9145 - val_acc: 0.7540\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8400 - acc: 0.7927 - val_loss: 0.9136 - val_acc: 0.7450\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8398 - acc: 0.7927 - val_loss: 0.9325 - val_acc: 0.7390\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8413 - acc: 0.7901 - val_loss: 0.9147 - val_acc: 0.7440\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8397 - acc: 0.7944 - val_loss: 0.9122 - val_acc: 0.7530\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8402 - acc: 0.7939 - val_loss: 0.9208 - val_acc: 0.7490\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8409 - acc: 0.7944 - val_loss: 0.9177 - val_acc: 0.7480\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8398 - acc: 0.7939 - val_loss: 0.9211 - val_acc: 0.7480\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8413 - acc: 0.7931 - val_loss: 0.9170 - val_acc: 0.7590\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8401 - acc: 0.7924 - val_loss: 0.9123 - val_acc: 0.7470\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8389 - acc: 0.7937 - val_loss: 0.9161 - val_acc: 0.7580\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8383 - acc: 0.7948 - val_loss: 0.9121 - val_acc: 0.7500\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8377 - acc: 0.7968 - val_loss: 0.9164 - val_acc: 0.7560\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8383 - acc: 0.7955 - val_loss: 0.9222 - val_acc: 0.7490\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8370 - acc: 0.7949 - val_loss: 0.9157 - val_acc: 0.7500\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8367 - acc: 0.7949 - val_loss: 0.9156 - val_acc: 0.7390\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8376 - acc: 0.7972 - val_loss: 0.9317 - val_acc: 0.7400\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8382 - acc: 0.7953 - val_loss: 0.9173 - val_acc: 0.7490\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8385 - acc: 0.7949 - val_loss: 0.9171 - val_acc: 0.7420\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8384 - acc: 0.7957 - val_loss: 0.9136 - val_acc: 0.7510\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8372 - acc: 0.7945 - val_loss: 0.9152 - val_acc: 0.7360\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8386 - acc: 0.7945 - val_loss: 0.9248 - val_acc: 0.7580\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8377 - acc: 0.7939 - val_loss: 0.9260 - val_acc: 0.7420\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8378 - acc: 0.7927 - val_loss: 0.9113 - val_acc: 0.7390\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8356 - acc: 0.7948 - val_loss: 0.9181 - val_acc: 0.7400\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8353 - acc: 0.7949 - val_loss: 0.9197 - val_acc: 0.7410\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8362 - acc: 0.7965 - val_loss: 0.9155 - val_acc: 0.7420\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8354 - acc: 0.7961 - val_loss: 0.9146 - val_acc: 0.7390\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8361 - acc: 0.7947 - val_loss: 0.9121 - val_acc: 0.7490\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8346 - acc: 0.7944 - val_loss: 0.9391 - val_acc: 0.7400\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8357 - acc: 0.7969 - val_loss: 0.9142 - val_acc: 0.7460\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8345 - acc: 0.7953 - val_loss: 0.9170 - val_acc: 0.7520\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8354 - acc: 0.7944 - val_loss: 0.9114 - val_acc: 0.7470\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8345 - acc: 0.7945 - val_loss: 0.9155 - val_acc: 0.7410\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8334 - acc: 0.7968 - val_loss: 0.9266 - val_acc: 0.7540\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8347 - acc: 0.7945 - val_loss: 0.9113 - val_acc: 0.7470\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8346 - acc: 0.7951 - val_loss: 0.9450 - val_acc: 0.7480\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8354 - acc: 0.7939 - val_loss: 0.9109 - val_acc: 0.7540\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8329 - acc: 0.7963 - val_loss: 0.9254 - val_acc: 0.7430\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8334 - acc: 0.7973 - val_loss: 0.9137 - val_acc: 0.7470\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8339 - acc: 0.7965 - val_loss: 0.9129 - val_acc: 0.7450\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8341 - acc: 0.7945 - val_loss: 0.9131 - val_acc: 0.7410\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8334 - acc: 0.7948 - val_loss: 0.9239 - val_acc: 0.7550\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8341 - acc: 0.7971 - val_loss: 0.9096 - val_acc: 0.7480\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8328 - acc: 0.7984 - val_loss: 0.9135 - val_acc: 0.7460\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8321 - acc: 0.7972 - val_loss: 0.9179 - val_acc: 0.7410\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8323 - acc: 0.7960 - val_loss: 0.9335 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8341 - acc: 0.7928 - val_loss: 0.9115 - val_acc: 0.7500\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8319 - acc: 0.7940 - val_loss: 0.9181 - val_acc: 0.7430\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8312 - acc: 0.7979 - val_loss: 0.9141 - val_acc: 0.7550\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8324 - acc: 0.7959 - val_loss: 0.9097 - val_acc: 0.7370\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8296 - acc: 0.7977 - val_loss: 0.9136 - val_acc: 0.7540\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8315 - acc: 0.7983 - val_loss: 0.9070 - val_acc: 0.7520\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8313 - acc: 0.7967 - val_loss: 0.9284 - val_acc: 0.7530\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8329 - acc: 0.7947 - val_loss: 0.9245 - val_acc: 0.7590\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8328 - acc: 0.7957 - val_loss: 0.9232 - val_acc: 0.7450\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8319 - acc: 0.7963 - val_loss: 0.9113 - val_acc: 0.7430\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8309 - acc: 0.7980 - val_loss: 0.9132 - val_acc: 0.7380\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8311 - acc: 0.7955 - val_loss: 0.9088 - val_acc: 0.7500\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8293 - acc: 0.7988 - val_loss: 0.9158 - val_acc: 0.7480\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8306 - acc: 0.7973 - val_loss: 0.9099 - val_acc: 0.7470\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8318 - acc: 0.7971 - val_loss: 0.9122 - val_acc: 0.7380\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8284 - acc: 0.7972 - val_loss: 0.9176 - val_acc: 0.7530\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8287 - acc: 0.8007 - val_loss: 0.9127 - val_acc: 0.7510\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8301 - acc: 0.7963 - val_loss: 0.9104 - val_acc: 0.7450\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8297 - acc: 0.7967 - val_loss: 0.9090 - val_acc: 0.7480\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8301 - acc: 0.7984 - val_loss: 0.9087 - val_acc: 0.7510\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8294 - acc: 0.7985 - val_loss: 0.9172 - val_acc: 0.7590\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8287 - acc: 0.7979 - val_loss: 0.9077 - val_acc: 0.7470\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8283 - acc: 0.8007 - val_loss: 0.9100 - val_acc: 0.7580\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8301 - acc: 0.7980 - val_loss: 0.9262 - val_acc: 0.7430\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8289 - acc: 0.7977 - val_loss: 0.9171 - val_acc: 0.7420\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8286 - acc: 0.7983 - val_loss: 0.9243 - val_acc: 0.7510\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8276 - acc: 0.7976 - val_loss: 0.9081 - val_acc: 0.7550\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8277 - acc: 0.7969 - val_loss: 0.9222 - val_acc: 0.7460\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8279 - acc: 0.7992 - val_loss: 0.9114 - val_acc: 0.7510\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8283 - acc: 0.8000 - val_loss: 0.9089 - val_acc: 0.7480\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8265 - acc: 0.7995 - val_loss: 0.9303 - val_acc: 0.7560\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8284 - acc: 0.8009 - val_loss: 0.9135 - val_acc: 0.7400\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8284 - acc: 0.8000 - val_loss: 0.9153 - val_acc: 0.7550\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8265 - acc: 0.7995 - val_loss: 0.9105 - val_acc: 0.7410\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8275 - acc: 0.7999 - val_loss: 0.9108 - val_acc: 0.7550\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8285 - acc: 0.7981 - val_loss: 0.9128 - val_acc: 0.7360\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8274 - acc: 0.7985 - val_loss: 0.9122 - val_acc: 0.7550\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8275 - acc: 0.7984 - val_loss: 0.9240 - val_acc: 0.7550\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8286 - acc: 0.7976 - val_loss: 0.9052 - val_acc: 0.7490\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8268 - acc: 0.7995 - val_loss: 0.9139 - val_acc: 0.7580\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8272 - acc: 0.7977 - val_loss: 0.9097 - val_acc: 0.7600\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8252 - acc: 0.7999 - val_loss: 0.9091 - val_acc: 0.7470\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8245 - acc: 0.7980 - val_loss: 0.9154 - val_acc: 0.7420\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8257 - acc: 0.8005 - val_loss: 0.9082 - val_acc: 0.7560\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8252 - acc: 0.7987 - val_loss: 0.9066 - val_acc: 0.7590\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8255 - acc: 0.8007 - val_loss: 0.9132 - val_acc: 0.7410\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8254 - acc: 0.8011 - val_loss: 0.9100 - val_acc: 0.7500\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8249 - acc: 0.8003 - val_loss: 0.9171 - val_acc: 0.7560\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8252 - acc: 0.8024 - val_loss: 0.9064 - val_acc: 0.7500\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8253 - acc: 0.8023 - val_loss: 0.9202 - val_acc: 0.7500\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8258 - acc: 0.7983 - val_loss: 0.9218 - val_acc: 0.7520\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8261 - acc: 0.7971 - val_loss: 0.9078 - val_acc: 0.7460\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8259 - acc: 0.8004 - val_loss: 0.9151 - val_acc: 0.7520\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8264 - acc: 0.8013 - val_loss: 0.9063 - val_acc: 0.7490\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8247 - acc: 0.8011 - val_loss: 0.9077 - val_acc: 0.7520\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8237 - acc: 0.8016 - val_loss: 0.9252 - val_acc: 0.7540\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8252 - acc: 0.8011 - val_loss: 0.9102 - val_acc: 0.7460\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8233 - acc: 0.8013 - val_loss: 0.9171 - val_acc: 0.7580\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8255 - acc: 0.7991 - val_loss: 0.9309 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8251 - acc: 0.7984 - val_loss: 0.9119 - val_acc: 0.7500\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8236 - acc: 0.7987 - val_loss: 0.9160 - val_acc: 0.7490\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8249 - acc: 0.8015 - val_loss: 0.9251 - val_acc: 0.7630\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8235 - acc: 0.7969 - val_loss: 0.9094 - val_acc: 0.7630\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8240 - acc: 0.8005 - val_loss: 0.9057 - val_acc: 0.7470\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8227 - acc: 0.7997 - val_loss: 0.9176 - val_acc: 0.7500\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8236 - acc: 0.8040 - val_loss: 0.9087 - val_acc: 0.7450\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8226 - acc: 0.7992 - val_loss: 0.9225 - val_acc: 0.7350\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8239 - acc: 0.7997 - val_loss: 0.9126 - val_acc: 0.7470\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8226 - acc: 0.8008 - val_loss: 0.9139 - val_acc: 0.7530\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8243 - acc: 0.8024 - val_loss: 0.9085 - val_acc: 0.7480\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8228 - acc: 0.8020 - val_loss: 0.9106 - val_acc: 0.7520\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8226 - acc: 0.8008 - val_loss: 0.9441 - val_acc: 0.7510\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8249 - acc: 0.8001 - val_loss: 0.9276 - val_acc: 0.7380\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8224 - acc: 0.8033 - val_loss: 0.9106 - val_acc: 0.7520\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8222 - acc: 0.8008 - val_loss: 0.9121 - val_acc: 0.7600\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8248 - acc: 0.7999 - val_loss: 0.9187 - val_acc: 0.7540\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8208 - acc: 0.8028 - val_loss: 0.9094 - val_acc: 0.7410\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8218 - acc: 0.8024 - val_loss: 0.9179 - val_acc: 0.7570\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8217 - acc: 0.8011 - val_loss: 0.9045 - val_acc: 0.7490\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8209 - acc: 0.8023 - val_loss: 0.9256 - val_acc: 0.7320\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8229 - acc: 0.8024 - val_loss: 0.9158 - val_acc: 0.7390\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8211 - acc: 0.8019 - val_loss: 0.9049 - val_acc: 0.7470\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8216 - acc: 0.8023 - val_loss: 0.9089 - val_acc: 0.7410\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8194 - acc: 0.8029 - val_loss: 0.9066 - val_acc: 0.7510\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8216 - acc: 0.8032 - val_loss: 0.9336 - val_acc: 0.7520\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8231 - acc: 0.7984 - val_loss: 0.9102 - val_acc: 0.7590\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8216 - acc: 0.8009 - val_loss: 0.9079 - val_acc: 0.7400\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8205 - acc: 0.8032 - val_loss: 0.9183 - val_acc: 0.7380\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8203 - acc: 0.8016 - val_loss: 0.9099 - val_acc: 0.7480\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8199 - acc: 0.8019 - val_loss: 0.9052 - val_acc: 0.7520\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8205 - acc: 0.8028 - val_loss: 0.9208 - val_acc: 0.7450\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8200 - acc: 0.8029 - val_loss: 0.9077 - val_acc: 0.7570\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8203 - acc: 0.8021 - val_loss: 0.9141 - val_acc: 0.7540\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8196 - acc: 0.8060 - val_loss: 0.9128 - val_acc: 0.7450\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8190 - acc: 0.8043 - val_loss: 0.9117 - val_acc: 0.7450\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8193 - acc: 0.8045 - val_loss: 0.9062 - val_acc: 0.7400\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8186 - acc: 0.8048 - val_loss: 0.9100 - val_acc: 0.7460\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8180 - acc: 0.8043 - val_loss: 0.9190 - val_acc: 0.7510\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8217 - acc: 0.8004 - val_loss: 0.9104 - val_acc: 0.7610\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8193 - acc: 0.8031 - val_loss: 0.9041 - val_acc: 0.7530\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8181 - acc: 0.8040 - val_loss: 0.9112 - val_acc: 0.7420\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8185 - acc: 0.8017 - val_loss: 0.9111 - val_acc: 0.7620\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8189 - acc: 0.8047 - val_loss: 0.9133 - val_acc: 0.7410\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8209 - acc: 0.8033 - val_loss: 0.9245 - val_acc: 0.7460\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8175 - acc: 0.8020 - val_loss: 0.9101 - val_acc: 0.7500\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8181 - acc: 0.8057 - val_loss: 0.9173 - val_acc: 0.7500\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8184 - acc: 0.8035 - val_loss: 0.9130 - val_acc: 0.7560\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8167 - acc: 0.8037 - val_loss: 0.9111 - val_acc: 0.7490\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8190 - acc: 0.8032 - val_loss: 0.9078 - val_acc: 0.7540\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8188 - acc: 0.8032 - val_loss: 0.9127 - val_acc: 0.7500\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8160 - acc: 0.8052 - val_loss: 0.9074 - val_acc: 0.7480\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8180 - acc: 0.8016 - val_loss: 0.9146 - val_acc: 0.7580\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8168 - acc: 0.8057 - val_loss: 0.9093 - val_acc: 0.7510\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8179 - acc: 0.8032 - val_loss: 0.9085 - val_acc: 0.7600\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8164 - acc: 0.8052 - val_loss: 0.9152 - val_acc: 0.7470\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8176 - acc: 0.8041 - val_loss: 0.9120 - val_acc: 0.7560\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8155 - acc: 0.8029 - val_loss: 0.9161 - val_acc: 0.7520\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8171 - acc: 0.8047 - val_loss: 0.9156 - val_acc: 0.7480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8173 - acc: 0.8071 - val_loss: 0.9140 - val_acc: 0.7440\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8170 - acc: 0.8029 - val_loss: 0.9440 - val_acc: 0.7420\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8166 - acc: 0.8044 - val_loss: 0.9089 - val_acc: 0.7520\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8161 - acc: 0.8024 - val_loss: 0.9310 - val_acc: 0.7380\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8175 - acc: 0.8023 - val_loss: 0.9104 - val_acc: 0.7550\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8160 - acc: 0.8049 - val_loss: 0.9185 - val_acc: 0.7570\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8157 - acc: 0.8055 - val_loss: 0.9184 - val_acc: 0.7580\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8175 - acc: 0.8021 - val_loss: 0.9138 - val_acc: 0.7460\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8169 - acc: 0.8064 - val_loss: 0.9159 - val_acc: 0.7450\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8165 - acc: 0.8052 - val_loss: 0.9539 - val_acc: 0.7470\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8177 - acc: 0.8036 - val_loss: 0.9130 - val_acc: 0.7470\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8163 - acc: 0.8089 - val_loss: 0.9070 - val_acc: 0.7510\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8146 - acc: 0.8049 - val_loss: 0.9096 - val_acc: 0.7410\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8157 - acc: 0.8068 - val_loss: 0.9165 - val_acc: 0.7390\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8145 - acc: 0.8061 - val_loss: 0.9104 - val_acc: 0.7540\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8134 - acc: 0.8089 - val_loss: 0.9196 - val_acc: 0.7600\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8162 - acc: 0.8059 - val_loss: 0.9041 - val_acc: 0.7490\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8159 - acc: 0.8048 - val_loss: 0.9153 - val_acc: 0.7380\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8149 - acc: 0.8052 - val_loss: 0.9194 - val_acc: 0.7560\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8153 - acc: 0.8043 - val_loss: 0.9147 - val_acc: 0.7520\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8151 - acc: 0.8061 - val_loss: 0.9089 - val_acc: 0.7580\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8156 - acc: 0.8051 - val_loss: 0.9117 - val_acc: 0.7540\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8152 - acc: 0.8081 - val_loss: 0.9143 - val_acc: 0.7560\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8142 - acc: 0.8055 - val_loss: 0.9347 - val_acc: 0.7420\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8158 - acc: 0.8069 - val_loss: 0.9133 - val_acc: 0.7450\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8142 - acc: 0.8037 - val_loss: 0.9369 - val_acc: 0.7370\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8142 - acc: 0.8056 - val_loss: 0.9280 - val_acc: 0.7470\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8133 - acc: 0.8052 - val_loss: 0.9228 - val_acc: 0.7450\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8146 - acc: 0.8032 - val_loss: 0.9106 - val_acc: 0.7490\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8130 - acc: 0.8047 - val_loss: 0.9082 - val_acc: 0.7420\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8140 - acc: 0.8067 - val_loss: 0.9181 - val_acc: 0.7450\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8138 - acc: 0.8068 - val_loss: 0.9092 - val_acc: 0.7530\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8131 - acc: 0.8065 - val_loss: 0.9043 - val_acc: 0.7450\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8119 - acc: 0.8073 - val_loss: 0.9073 - val_acc: 0.7520\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8137 - acc: 0.8056 - val_loss: 0.9042 - val_acc: 0.7540\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8154 - acc: 0.8071 - val_loss: 0.9080 - val_acc: 0.7500\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8145 - acc: 0.8060 - val_loss: 0.9247 - val_acc: 0.7540\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8170 - acc: 0.8043 - val_loss: 0.9177 - val_acc: 0.7480\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8142 - acc: 0.8028 - val_loss: 0.9143 - val_acc: 0.7500\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8129 - acc: 0.8051 - val_loss: 0.9201 - val_acc: 0.7570\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8125 - acc: 0.8067 - val_loss: 0.9038 - val_acc: 0.7500\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8123 - acc: 0.8088 - val_loss: 0.9417 - val_acc: 0.7540\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8129 - acc: 0.8047 - val_loss: 0.9049 - val_acc: 0.7450\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8127 - acc: 0.8081 - val_loss: 0.9200 - val_acc: 0.7440\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8117 - acc: 0.8059 - val_loss: 0.9058 - val_acc: 0.7560\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8123 - acc: 0.8044 - val_loss: 0.9067 - val_acc: 0.7450\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8106 - acc: 0.8068 - val_loss: 0.9119 - val_acc: 0.7470\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8113 - acc: 0.8107 - val_loss: 0.9092 - val_acc: 0.7560\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8116 - acc: 0.8084 - val_loss: 0.9263 - val_acc: 0.7420\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8117 - acc: 0.8073 - val_loss: 0.9283 - val_acc: 0.7420\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8114 - acc: 0.8093 - val_loss: 0.9094 - val_acc: 0.7500\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8126 - acc: 0.8056 - val_loss: 0.9321 - val_acc: 0.7570\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8123 - acc: 0.8073 - val_loss: 0.9541 - val_acc: 0.7400\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8126 - acc: 0.8095 - val_loss: 0.9115 - val_acc: 0.7440\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8116 - acc: 0.8059 - val_loss: 0.9126 - val_acc: 0.7500\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8111 - acc: 0.8056 - val_loss: 0.9178 - val_acc: 0.7580\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8099 - acc: 0.8077 - val_loss: 0.9067 - val_acc: 0.7500\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8132 - acc: 0.8065 - val_loss: 0.9193 - val_acc: 0.7410\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8132 - acc: 0.8069 - val_loss: 0.9718 - val_acc: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8117 - acc: 0.8069 - val_loss: 0.9134 - val_acc: 0.7490\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8115 - acc: 0.8081 - val_loss: 0.9035 - val_acc: 0.7570\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8105 - acc: 0.8069 - val_loss: 0.9151 - val_acc: 0.7500\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8099 - acc: 0.8067 - val_loss: 0.9219 - val_acc: 0.7500\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8099 - acc: 0.8077 - val_loss: 0.9093 - val_acc: 0.7570\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8125 - acc: 0.8097 - val_loss: 0.9100 - val_acc: 0.7440\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8094 - acc: 0.8088 - val_loss: 0.9078 - val_acc: 0.7530\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8119 - acc: 0.8067 - val_loss: 0.9238 - val_acc: 0.7510\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8096 - acc: 0.8085 - val_loss: 0.9107 - val_acc: 0.7470\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8087 - acc: 0.8073 - val_loss: 0.9312 - val_acc: 0.7400\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8124 - acc: 0.8077 - val_loss: 0.9081 - val_acc: 0.7490\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8093 - acc: 0.8085 - val_loss: 0.9371 - val_acc: 0.7440\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8106 - acc: 0.8099 - val_loss: 0.9578 - val_acc: 0.7390\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8130 - acc: 0.8068 - val_loss: 0.9084 - val_acc: 0.7450\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8107 - acc: 0.8081 - val_loss: 0.9206 - val_acc: 0.7590\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8099 - acc: 0.8053 - val_loss: 0.9401 - val_acc: 0.7390\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8095 - acc: 0.8095 - val_loss: 0.9323 - val_acc: 0.7450\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8108 - acc: 0.8049 - val_loss: 0.9029 - val_acc: 0.7530\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8098 - acc: 0.8068 - val_loss: 0.9046 - val_acc: 0.7530\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8090 - acc: 0.8087 - val_loss: 0.9102 - val_acc: 0.7530\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8079 - acc: 0.8091 - val_loss: 0.9008 - val_acc: 0.7470\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8097 - acc: 0.8085 - val_loss: 0.9139 - val_acc: 0.7470\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8097 - acc: 0.8120 - val_loss: 0.9084 - val_acc: 0.7500\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8080 - acc: 0.8092 - val_loss: 0.9076 - val_acc: 0.7540\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8110 - acc: 0.8104 - val_loss: 0.9177 - val_acc: 0.7500\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8125 - acc: 0.8059 - val_loss: 0.9347 - val_acc: 0.7580\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8075 - acc: 0.8092 - val_loss: 0.9063 - val_acc: 0.7520\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8069 - acc: 0.8099 - val_loss: 0.9068 - val_acc: 0.7510\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8066 - acc: 0.8092 - val_loss: 0.9127 - val_acc: 0.7530\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8067 - acc: 0.8109 - val_loss: 0.9266 - val_acc: 0.7450\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8070 - acc: 0.8116 - val_loss: 0.9037 - val_acc: 0.7500\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8102 - acc: 0.8079 - val_loss: 0.9171 - val_acc: 0.7490\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8069 - acc: 0.8111 - val_loss: 0.9010 - val_acc: 0.7560\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8053 - acc: 0.8107 - val_loss: 0.9059 - val_acc: 0.7470\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8113 - acc: 0.8076 - val_loss: 0.9035 - val_acc: 0.7540\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8065 - acc: 0.8112 - val_loss: 0.9025 - val_acc: 0.7480\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8066 - acc: 0.8160 - val_loss: 0.9111 - val_acc: 0.7570\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8057 - acc: 0.8104 - val_loss: 0.9175 - val_acc: 0.7530\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8063 - acc: 0.8099 - val_loss: 0.9117 - val_acc: 0.7540\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8081 - acc: 0.8104 - val_loss: 0.9143 - val_acc: 0.7470\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8075 - acc: 0.8113 - val_loss: 0.9069 - val_acc: 0.7510\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8063 - acc: 0.8100 - val_loss: 0.9341 - val_acc: 0.7480\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8070 - acc: 0.8059 - val_loss: 0.9075 - val_acc: 0.7550\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8063 - acc: 0.8113 - val_loss: 0.9186 - val_acc: 0.7510\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8051 - acc: 0.8099 - val_loss: 0.9068 - val_acc: 0.7510\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8038 - acc: 0.8115 - val_loss: 0.9068 - val_acc: 0.7550\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8040 - acc: 0.8112 - val_loss: 0.9121 - val_acc: 0.7470\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8091 - acc: 0.8100 - val_loss: 0.9340 - val_acc: 0.7500\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8063 - acc: 0.8097 - val_loss: 0.9466 - val_acc: 0.7420\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8099 - acc: 0.8108 - val_loss: 0.9104 - val_acc: 0.7490\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8046 - acc: 0.8139 - val_loss: 0.9059 - val_acc: 0.7580\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8058 - acc: 0.8095 - val_loss: 0.9063 - val_acc: 0.7600\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8054 - acc: 0.8105 - val_loss: 0.9205 - val_acc: 0.7430\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8049 - acc: 0.8123 - val_loss: 0.9171 - val_acc: 0.7560\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8065 - acc: 0.8108 - val_loss: 0.9060 - val_acc: 0.7530\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8042 - acc: 0.8103 - val_loss: 0.9191 - val_acc: 0.7500\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8044 - acc: 0.8121 - val_loss: 0.9033 - val_acc: 0.7550\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8046 - acc: 0.8117 - val_loss: 0.9064 - val_acc: 0.7520\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8042 - acc: 0.8117 - val_loss: 0.9002 - val_acc: 0.7530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8031 - acc: 0.8116 - val_loss: 0.9137 - val_acc: 0.7520\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8069 - acc: 0.8108 - val_loss: 0.9642 - val_acc: 0.7230\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8044 - acc: 0.8140 - val_loss: 0.9035 - val_acc: 0.7550\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8050 - acc: 0.8093 - val_loss: 0.9116 - val_acc: 0.7540\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8051 - acc: 0.8097 - val_loss: 0.9327 - val_acc: 0.7610\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8044 - acc: 0.8093 - val_loss: 0.9303 - val_acc: 0.7410\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8061 - acc: 0.8124 - val_loss: 0.9144 - val_acc: 0.7540\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8028 - acc: 0.8128 - val_loss: 0.9043 - val_acc: 0.7610\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8051 - acc: 0.8127 - val_loss: 0.9234 - val_acc: 0.7560\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8066 - acc: 0.8113 - val_loss: 0.9034 - val_acc: 0.7550\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8027 - acc: 0.8129 - val_loss: 0.9032 - val_acc: 0.7530\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8019 - acc: 0.8151 - val_loss: 0.9484 - val_acc: 0.7560\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8024 - acc: 0.8117 - val_loss: 0.9224 - val_acc: 0.7490\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8037 - acc: 0.8147 - val_loss: 0.9061 - val_acc: 0.7560\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8048 - acc: 0.8101 - val_loss: 0.9131 - val_acc: 0.7510\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8051 - acc: 0.8113 - val_loss: 0.9010 - val_acc: 0.7500\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8058 - acc: 0.8116 - val_loss: 0.8990 - val_acc: 0.7530\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8023 - acc: 0.8116 - val_loss: 0.9146 - val_acc: 0.7530\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8044 - acc: 0.8141 - val_loss: 0.9646 - val_acc: 0.7470\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8030 - acc: 0.8152 - val_loss: 0.9124 - val_acc: 0.7570\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8047 - acc: 0.8109 - val_loss: 0.9215 - val_acc: 0.7480\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8050 - acc: 0.8088 - val_loss: 0.9201 - val_acc: 0.7450\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8052 - acc: 0.8108 - val_loss: 0.9063 - val_acc: 0.7550\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8030 - acc: 0.8116 - val_loss: 0.9081 - val_acc: 0.7520\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8023 - acc: 0.8133 - val_loss: 0.9104 - val_acc: 0.7600\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8005 - acc: 0.8135 - val_loss: 0.9039 - val_acc: 0.7540\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8019 - acc: 0.8125 - val_loss: 0.9414 - val_acc: 0.7420\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8020 - acc: 0.8128 - val_loss: 0.9465 - val_acc: 0.7490\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8040 - acc: 0.8103 - val_loss: 0.9095 - val_acc: 0.7460\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8024 - acc: 0.8136 - val_loss: 0.9091 - val_acc: 0.7540\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8027 - acc: 0.8140 - val_loss: 0.9125 - val_acc: 0.7470\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8015 - acc: 0.8140 - val_loss: 0.9113 - val_acc: 0.7550\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8012 - acc: 0.8123 - val_loss: 0.9090 - val_acc: 0.7610\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8016 - acc: 0.8112 - val_loss: 0.9146 - val_acc: 0.7560\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8009 - acc: 0.8139 - val_loss: 0.9116 - val_acc: 0.7520\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8000 - acc: 0.8136 - val_loss: 0.9077 - val_acc: 0.7630\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8021 - acc: 0.8137 - val_loss: 0.9333 - val_acc: 0.7490\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8011 - acc: 0.8156 - val_loss: 0.9125 - val_acc: 0.7510\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8012 - acc: 0.8151 - val_loss: 0.9212 - val_acc: 0.7480\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8022 - acc: 0.8135 - val_loss: 0.9119 - val_acc: 0.7460\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8012 - acc: 0.8148 - val_loss: 0.9459 - val_acc: 0.7500\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8042 - acc: 0.8093 - val_loss: 0.9156 - val_acc: 0.7500\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8015 - acc: 0.8128 - val_loss: 0.9140 - val_acc: 0.7480\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8009 - acc: 0.8144 - val_loss: 0.9021 - val_acc: 0.7520\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8018 - acc: 0.8119 - val_loss: 0.9151 - val_acc: 0.7540\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8005 - acc: 0.8127 - val_loss: 0.9058 - val_acc: 0.7550\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8013 - acc: 0.8141 - val_loss: 0.9152 - val_acc: 0.7470\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7999 - acc: 0.8143 - val_loss: 0.9211 - val_acc: 0.7610\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8003 - acc: 0.8153 - val_loss: 0.9146 - val_acc: 0.7600\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8013 - acc: 0.8153 - val_loss: 0.9339 - val_acc: 0.7450\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8006 - acc: 0.8159 - val_loss: 0.9202 - val_acc: 0.7590\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8010 - acc: 0.8115 - val_loss: 0.9029 - val_acc: 0.7610\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8003 - acc: 0.8135 - val_loss: 0.9131 - val_acc: 0.7530\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7988 - acc: 0.8151 - val_loss: 0.9231 - val_acc: 0.7510\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7998 - acc: 0.8139 - val_loss: 0.9070 - val_acc: 0.7580\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8021 - acc: 0.8132 - val_loss: 0.9139 - val_acc: 0.7530\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7971 - acc: 0.8157 - val_loss: 0.9016 - val_acc: 0.7520\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7995 - acc: 0.8121 - val_loss: 0.9000 - val_acc: 0.7540\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7990 - acc: 0.8153 - val_loss: 0.9137 - val_acc: 0.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7989 - acc: 0.8132 - val_loss: 0.8994 - val_acc: 0.7520\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7970 - acc: 0.8153 - val_loss: 0.9073 - val_acc: 0.7440\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8040 - acc: 0.8135 - val_loss: 0.9164 - val_acc: 0.7490\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8013 - acc: 0.8115 - val_loss: 0.9014 - val_acc: 0.7580\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7969 - acc: 0.8165 - val_loss: 0.9418 - val_acc: 0.7500\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8008 - acc: 0.8143 - val_loss: 0.9026 - val_acc: 0.7550\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7994 - acc: 0.8127 - val_loss: 0.9309 - val_acc: 0.7590\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8000 - acc: 0.8153 - val_loss: 0.9006 - val_acc: 0.7650\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7971 - acc: 0.8125 - val_loss: 0.9139 - val_acc: 0.7530\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8020 - acc: 0.8129 - val_loss: 0.9109 - val_acc: 0.7590\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7988 - acc: 0.8167 - val_loss: 0.9294 - val_acc: 0.7500\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8002 - acc: 0.8156 - val_loss: 0.9101 - val_acc: 0.7510\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7950 - acc: 0.8177 - val_loss: 0.9038 - val_acc: 0.7600\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7991 - acc: 0.8139 - val_loss: 0.9008 - val_acc: 0.7630\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7958 - acc: 0.8176 - val_loss: 0.9190 - val_acc: 0.7510\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7972 - acc: 0.8151 - val_loss: 0.9189 - val_acc: 0.7510\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7956 - acc: 0.8161 - val_loss: 0.9003 - val_acc: 0.7610\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7983 - acc: 0.8161 - val_loss: 0.9043 - val_acc: 0.7600\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7978 - acc: 0.8141 - val_loss: 0.9098 - val_acc: 0.7540\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8003 - acc: 0.8144 - val_loss: 0.9265 - val_acc: 0.7520\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7980 - acc: 0.8180 - val_loss: 0.9086 - val_acc: 0.7590\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7985 - acc: 0.8167 - val_loss: 0.9143 - val_acc: 0.7510\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7976 - acc: 0.8177 - val_loss: 0.9139 - val_acc: 0.7590\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7991 - acc: 0.8128 - val_loss: 0.9384 - val_acc: 0.7540\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7967 - acc: 0.8169 - val_loss: 0.9298 - val_acc: 0.7440\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7969 - acc: 0.8152 - val_loss: 0.9117 - val_acc: 0.7560\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7950 - acc: 0.8164 - val_loss: 0.8983 - val_acc: 0.7550\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7979 - acc: 0.8141 - val_loss: 0.9289 - val_acc: 0.7470\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7958 - acc: 0.8179 - val_loss: 0.9148 - val_acc: 0.7500\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7948 - acc: 0.8176 - val_loss: 0.9137 - val_acc: 0.7570\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8013 - acc: 0.8149 - val_loss: 0.8984 - val_acc: 0.7550\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7987 - acc: 0.8185 - val_loss: 0.9127 - val_acc: 0.7540\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7936 - acc: 0.8173 - val_loss: 0.9079 - val_acc: 0.7550\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7978 - acc: 0.8183 - val_loss: 0.9018 - val_acc: 0.7560\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7961 - acc: 0.8173 - val_loss: 0.9138 - val_acc: 0.7570\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7943 - acc: 0.8173 - val_loss: 0.9249 - val_acc: 0.7580\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8008 - acc: 0.8143 - val_loss: 0.9196 - val_acc: 0.7510\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7949 - acc: 0.8167 - val_loss: 0.9076 - val_acc: 0.7530\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7966 - acc: 0.8145 - val_loss: 0.9305 - val_acc: 0.7500\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7998 - acc: 0.8148 - val_loss: 0.9025 - val_acc: 0.7590\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7950 - acc: 0.8204 - val_loss: 0.9296 - val_acc: 0.7410\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7935 - acc: 0.8187 - val_loss: 0.9133 - val_acc: 0.7580\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7963 - acc: 0.8152 - val_loss: 0.9581 - val_acc: 0.7510\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8031 - acc: 0.8141 - val_loss: 0.9139 - val_acc: 0.7520\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7957 - acc: 0.8175 - val_loss: 0.9342 - val_acc: 0.7470\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7962 - acc: 0.8160 - val_loss: 0.9151 - val_acc: 0.7550\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7942 - acc: 0.8175 - val_loss: 0.9038 - val_acc: 0.7600\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7931 - acc: 0.8187 - val_loss: 0.9175 - val_acc: 0.7510\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7964 - acc: 0.8168 - val_loss: 0.8995 - val_acc: 0.7620\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7933 - acc: 0.8181 - val_loss: 0.9552 - val_acc: 0.7580\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7949 - acc: 0.8195 - val_loss: 0.9044 - val_acc: 0.7580\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7958 - acc: 0.8173 - val_loss: 0.9150 - val_acc: 0.7520\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7965 - acc: 0.8188 - val_loss: 0.9055 - val_acc: 0.7530\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7964 - acc: 0.8161 - val_loss: 0.9046 - val_acc: 0.7610\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7920 - acc: 0.8164 - val_loss: 0.9162 - val_acc: 0.7630\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7928 - acc: 0.8177 - val_loss: 0.9375 - val_acc: 0.7460\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7943 - acc: 0.8132 - val_loss: 0.9123 - val_acc: 0.7500\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7922 - acc: 0.8211 - val_loss: 0.9113 - val_acc: 0.7640\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7921 - acc: 0.8175 - val_loss: 0.8997 - val_acc: 0.7580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7977 - acc: 0.8153 - val_loss: 0.9152 - val_acc: 0.7600\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7960 - acc: 0.8152 - val_loss: 0.9258 - val_acc: 0.7570\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7919 - acc: 0.8213 - val_loss: 0.9246 - val_acc: 0.7590\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8019 - acc: 0.8143 - val_loss: 0.9056 - val_acc: 0.7580\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7987 - acc: 0.8135 - val_loss: 0.9144 - val_acc: 0.7510\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7999 - acc: 0.8172 - val_loss: 0.9001 - val_acc: 0.7530\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7932 - acc: 0.8212 - val_loss: 0.9118 - val_acc: 0.7550\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7914 - acc: 0.8209 - val_loss: 0.9327 - val_acc: 0.7550\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7945 - acc: 0.8161 - val_loss: 0.9131 - val_acc: 0.7530\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7920 - acc: 0.8196 - val_loss: 0.9095 - val_acc: 0.7550\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7961 - acc: 0.8168 - val_loss: 0.9041 - val_acc: 0.7620\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7925 - acc: 0.8173 - val_loss: 0.9033 - val_acc: 0.7600\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7899 - acc: 0.8188 - val_loss: 0.9333 - val_acc: 0.7530\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7924 - acc: 0.8195 - val_loss: 0.9084 - val_acc: 0.7540\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7905 - acc: 0.8208 - val_loss: 0.9269 - val_acc: 0.7510\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7990 - acc: 0.8188 - val_loss: 0.9151 - val_acc: 0.7550\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7954 - acc: 0.8159 - val_loss: 0.9159 - val_acc: 0.7500\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7906 - acc: 0.8187 - val_loss: 0.9102 - val_acc: 0.7560\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7939 - acc: 0.8209 - val_loss: 0.9003 - val_acc: 0.7630\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7886 - acc: 0.8223 - val_loss: 0.9031 - val_acc: 0.7560\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7905 - acc: 0.8196 - val_loss: 0.8997 - val_acc: 0.7620\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7889 - acc: 0.8203 - val_loss: 0.9129 - val_acc: 0.7570\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7952 - acc: 0.8184 - val_loss: 0.9417 - val_acc: 0.7450\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7930 - acc: 0.8183 - val_loss: 0.9134 - val_acc: 0.7610\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7908 - acc: 0.8201 - val_loss: 0.9042 - val_acc: 0.7530\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7934 - acc: 0.8227 - val_loss: 0.9057 - val_acc: 0.7600\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7935 - acc: 0.8199 - val_loss: 0.9031 - val_acc: 0.7620\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7919 - acc: 0.8169 - val_loss: 0.9148 - val_acc: 0.7450\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8039 - acc: 0.8125 - val_loss: 0.8987 - val_acc: 0.7670\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7913 - acc: 0.8201 - val_loss: 0.8969 - val_acc: 0.7660\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7876 - acc: 0.8193 - val_loss: 0.9048 - val_acc: 0.7620\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7906 - acc: 0.8171 - val_loss: 0.9127 - val_acc: 0.7630\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7924 - acc: 0.8183 - val_loss: 0.9083 - val_acc: 0.7640\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7935 - acc: 0.8195 - val_loss: 0.9623 - val_acc: 0.7390\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8001 - acc: 0.8153 - val_loss: 0.8959 - val_acc: 0.7630\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7916 - acc: 0.8173 - val_loss: 0.9371 - val_acc: 0.7490\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7925 - acc: 0.8193 - val_loss: 0.9106 - val_acc: 0.7580\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7879 - acc: 0.8213 - val_loss: 0.8987 - val_acc: 0.7620\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7925 - acc: 0.8184 - val_loss: 0.9100 - val_acc: 0.7560\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7903 - acc: 0.8188 - val_loss: 0.9043 - val_acc: 0.7640\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7908 - acc: 0.8211 - val_loss: 0.9337 - val_acc: 0.7590\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7978 - acc: 0.8177 - val_loss: 0.9098 - val_acc: 0.7570\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7921 - acc: 0.8189 - val_loss: 0.9733 - val_acc: 0.7410\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7943 - acc: 0.8197 - val_loss: 0.9101 - val_acc: 0.7550\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7892 - acc: 0.8216 - val_loss: 0.9182 - val_acc: 0.7640\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7891 - acc: 0.8200 - val_loss: 0.9134 - val_acc: 0.7600\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7936 - acc: 0.8188 - val_loss: 0.9249 - val_acc: 0.7540\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7871 - acc: 0.8243 - val_loss: 0.9277 - val_acc: 0.7520\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7907 - acc: 0.8211 - val_loss: 0.9102 - val_acc: 0.7530\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7932 - acc: 0.8184 - val_loss: 0.9142 - val_acc: 0.7490\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7947 - acc: 0.8175 - val_loss: 0.9850 - val_acc: 0.7440\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7917 - acc: 0.8191 - val_loss: 0.9160 - val_acc: 0.7530\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7878 - acc: 0.8235 - val_loss: 0.9138 - val_acc: 0.7640\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7860 - acc: 0.8223 - val_loss: 0.9080 - val_acc: 0.7600\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7914 - acc: 0.8207 - val_loss: 0.9243 - val_acc: 0.7580\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7906 - acc: 0.8205 - val_loss: 0.9119 - val_acc: 0.7640\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl8FPX9+PHXmxASSLjDHY5UUY5wBxDEGymoxaNesVaKB1/9etb2V7WlSm1ttZ61+rUeeNRSPCuicqiIB1VOISqgghwSzhBCQkKuhffvj5msm2V3swnZ7Cb7fj4eebAz85nPvGdmmfd8PjM7I6qKMcYYA9As2gEYY4yJHZYUjDHGeFlSMMYY42VJwRhjjJclBWOMMV6WFIwxxnhZUogRIpIgIsUi0qs+y8Y6EfmXiMxwP58qImvDKVuH5TSZbWYa3tF89xobSwp15B5gqv4Oi0ipz/DPalufqh5S1VRV/b4+y9aFiIwUkc9F5ICIfC0i4yOxHH+q+qGqDqyPukRkiYj8wqfuiG6zeOC/TX3G9xeRuSKSJyL7RGS+iPSNQoimHlhSqCP3AJOqqqnA98BPfMbN8i8vIs0bPso6+z9gLtAGOAvYHt1wTDAi0kxEov3/uC0wBzge6AKsAd5oyABi9f9XjOyfWmlUwTYmIvInEXlZRGaLyAHgchEZIyJLRWS/iOwUkUdFJNEt31xEVET6uMP/cqfPd8/YPxORjNqWdadPEpFvRaRQRP4uIv8NdMbnwwNsVccmVV1fw7puEJGJPsMt3DPGwe5/itdEZJe73h+KSP8g9YwXkS0+wyNEZI27TrOBJJ9pHUVknnt2WiAib4lID3fafcAY4B9uy+2RANusnbvd8kRki4jcISLiTrtaRD4SkYfdmDeJyIQQ6z/dLXNARNaKyGS/6f/jtrgOiMhXIjLEHd9bROa4MewVkb+54/8kIs/7zH+siKjP8BIR+aOIfAaUAL3cmNe7y/hORK72i+ECd1sWichGEZkgItkissyv3G0i8lqwdQ1EVZeq6rOquk9VK4GHgYEi0jbAthonItt9D5QicpGIfO5+PkGcVmqRiOwWkfsDLbPquyIivxWRXcDT7vjJIpLj7rclIpLpM0+Wz/fpJRF5VX7ourxaRD70KVvt++K37KDfPXf6EfunNtsz2iwpRNb5wL9xzqRexjnY3gykAScCE4H/CTH/ZcDvgQ44rZE/1rasiHQGXgH+n7vczcCoGuJeDjxYdfAKw2wg22d4ErBDVb9wh98G+gJdga+AF2uqUESSgDeBZ3HW6U3gPJ8izXAOBL2A3kAl8DcAVb0N+Ay41m253RJgEf8HtAJ+BJwOXAVc4TN9LPAl0BHnIDczRLjf4uzPtsA9wL9FpIu7HtnAdOBnOC2vC4B94pzZvgNsBPoAPXH2U7h+Dlzp1pkL7AbOdoevAf4uIoPdGMbibMdfAe2A04CtuGf3Ur2r53LC2D81OBnIVdXCANP+i7OvTvEZdxnO/xOAvwP3q2ob4FggVIJKB1JxvgP/KyIjcb4TV+Pst2eBN92TlCSc9X0G5/v0OtW/T7UR9Lvnw3//NB6qan9H+QdsAcb7jfsT8EEN8/0aeNX93BxQoI87/C/gHz5lJwNf1aHslcAnPtME2An8IkhMlwMrcbqNcoHB7vhJwLIg8/QDCoFkd/hl4LdByqa5saf4xD7D/Twe2OJ+Ph3YBojPvMurygaoNwvI8xle4ruOvtsMSMRJ0Mf5TL8eeN/9fDXwtc+0Nu68aWF+H74CznY/LwKuD1DmJGAXkBBg2p+A532Gj3X+q1ZbtztriOHtquXiJLT7g5R7GviD+3kosBdIDFK22jYNUqYXsAO4KESZe4Gn3M/tgINAujv8KXAn0LGG5YwHyoAWfutyl1+573AS9unA937Tlvp8964GPgz0ffH/nob53Qu5f2L5z1oKkbXNd0BE+onIO25XShFwN85BMphdPp8P4pwV1bZsd9841PnWhjpzuRl4VFXn4Rwo33XPOMcC7weaQVW/xvnPd7aIpALn4J75iXPXz1/d7pUinDNjCL3eVXHnuvFW2Vr1QURSROQZEfnerfeDMOqs0hlI8K3P/dzDZ9h/e0KQ7S8iv/DpstiPkySrYumJs2389cRJgIfCjNmf/3frHBFZJk633X5gQhgxALyA04oB54TgZXW6gGrNbZW+C/xNVV8NUfTfwE/F6Tr9Kc7JRtV3ciowAPhGRJaLyFkh6tmtqhU+w72B26r2g7sduuHs1+4c+b3fRh2E+d2rU92xwJJCZPk/gvZJnLPIY9VpHt+Jc+YeSTtxmtkAiIhQ/eDnrznOWTSq+iZwG04yuBx4JMR8VV1I5wNrVHWLO/4KnFbH6TjdK8dWhVKbuF2+fbO/ATKAUe62PN2vbKjH/+4BDuEcRHzrrvUFdRH5EfAEcB3O2W074Gt+WL9twDEBZt0G9BaRhADTSnC6tqp0DVDG9xpDS5xulr8AXdwY3g0jBlR1iVvHiTj7r05dRyLSEed78pqq3heqrDrdijuBH1O96whV/UZVL8VJ3A8Cr4tIcrCq/Ia34bR62vn8tVLVVwj8ferp8zmcbV6lpu9eoNgaDUsKDas1TjdLiTgXW0NdT6gvbwPDReQnbj/2zUCnEOVfBWaIyCD3YuDXQAXQEgj2nxOcpDAJmIbPf3KcdS4H8nH+090TZtxLgGYicoN70e8iYLhfvQeBAveAdKff/LtxrhccwT0Tfg34s4ikinNR/pc4XQS1lYpzAMjDyblX47QUqjwD/EZEhomjr4j0xLnmke/G0EpEWroHZnDu3jlFRHqKSDvg9hpiSAJauDEcEpFzgDN8ps8ErhaR08S58J8uIsf7TH8RJ7GVqOrSGpaVKCLJPn+J7gXld3G6S6fXMH+V2TjbfAw+1w1E5Ocikqaqh3H+ryhwOMw6nwKuF+eWanH37U9EJAXn+5QgIte536efAiN85s0BBrvf+5bAXSGWU9N3r1GzpNCwfgVMAQ7gtBpejvQCVXU3cAnwEM5B6BhgNc6BOpD7gH/i3JK6D6d1cDXOf+J3RKRNkOXk4lyLOIHqF0yfw+lj3gGsxekzDifucpxWxzVAAc4F2jk+RR7CaXnku3XO96viESDb7UZ4KMAi/hcn2W0GPsLpRvlnOLH5xfkF8CjO9Y6dOAlhmc/02Tjb9GWgCPgP0F5VPTjdbP1xznC/By50Z1uAc0vnl269c2uIYT/OAfYNnH12Ic7JQNX0T3G246M4B9rFVD9L/ieQSXithKeAUp+/p93lDcdJPL6/3+keop5/45xhv6eqBT7jzwLWi3PH3gPAJX5dREGp6jKcFtsTON+Zb3FauL7fp2vdaRcD83D/H6jqOuDPwIfAN8DHIRZV03evUZPqXbamqXO7K3YAF6rqJ9GOx0Sfeya9B8hU1c3RjqehiMgq4BFVPdq7rZoUaynEARGZKCJt3dvyfo9zzWB5lMMyseN64L9NPSGI8xiVLm730VU4rbp3ox1XrInJXwGaejcOmIXT77wWOM9tTps4JyK5OPfZnxvtWBpAf5xuvBScu7F+6navGh/WfWSMMcbLuo+MMcZ4Nbruo7S0NO3Tp0+0wzDGmEZl1apVe1U11O3oQCNMCn369GHlypXRDsMYYxoVEdlacynrPjLGGOPDkoIxxhgvSwrGGGO8LCkYY4zxsqRgjDHGy5KCMcYYL0sKxhhjvCwpGGNMjNqyfwtvrH+DDfkbKPc0zOPKGt2P14wxJlbtOLCDfaX7yOycCYDnsIedB3byn/X/YXfJblo2b8nAzgMZ0mUIn+V+xgOfPsDAzgN559t3mDl5JmcecyY5u3J4YuUT9GnXh78s+Yu37vQ26bx7+bv079Q/outgScEYY0IorSylpLIEz2EPN82/iXG9xtG6RWt+v/j33H3a3by27jXmb5zP1KFTeWXtK5RUljCy+0hW7FgRVv05u3MAuPDVC0OWyy3KZcn3SyKeFBrdU1KzsrLUHnNhjMn8v0z6pfXjtYudt3nuKt7Ft/nfcnLvkzmsh5n95WzO63cen3z/CZNmTWLWBbOY+81c/meE8xbchGYJrNi+grV5a3luzXP0btubScdO4uPvP2Zd3roGW49hXYexetfqgNNuP/F2Xl77Mp1TOvP6xa/TOqk1bZICvvywRiKySlWzaixnScEYEy35B/N5f9P7nNfvPBQlubnzGvDdxbvpnNKZncU76diyI57DHk5+/mRO63MalYcqeWb1MxysPAjA0K5DWbNrTYPGndgskcrDld7h+8bfx/Nrnmf93vU0k2a0T27PsquXoSh9/97XW65nm548/OOHOfOYM/lwy4cM7TqUXm17cevCW3l46cPce8a9eA57+O1Jv0VE6jXmmEgKIjIR+BuQADyjqvf6Te+F827cdm6Z21V1Xqg6LSkYEx0VhypokdAi4LS1e9bSTJrRL60fL+S8wNQ3pwKwZOoSHlvxGKN7jObKYVfy8lcvs2X/Fib1ncRJz510RD3nHn8uG/ZtOOozdd+DdtukthSWF1abnpSQRPmhckb3GM3QrkNJa5XGPZ/cQ882PZn/s/lMXzyd8/udz+qdq9lzcA+CMLDTQG4dcystElogIqzLW0erxFb0adcnZCx5JXmICG2S2lDmKQt4pl/uKef19a+TnZld78mgStSTgvsu4G+BM4FcYAWQ7b4gu6rMU8BqVX1CRAYA81S1T6h6LSkYc3RU9YgDz8KNC8lon0HPNj35Nv9bMtpnkNoilXV561BVPtr6ETfOv5Ex6WNIbZHKe5vei1L0P8hol8FjZz3G2f8+mw4tO7Dw8oUUVxQzqPMgOrTswPLty9mwbwOXDbqMQ4cPUVReRKmnlNlfzubWMbdSUlniPUBXHqpkwcYFnHPcORE7KEdbLCSFMcAMVf2xO3wHgKr+xafMk8AmVb3PLf+gqo4NVa8lBWMCW7RpER1bdWR93nrG9hxLblEuL37xIhcOuJATe57I3G/m8tiKx1jy/RK6pXZjaNehJCYksqt4F8u31/8ru8/IOINFmxdVGzdt+DRW7lzJ5zs/946bc8kctuzfQnFFMUO6DmFEtxG0SmyF57AHEeGsWWcxJn0MD/34IV5f/zrnHHeOt5upar0zO2fSJbVLva9DUxILSeFCYKKqXu0O/xwYrao3+JTphvPi7PY4700dr6qrAtQ1DZgG0KtXrxFbt4b1WHBjYtpHWz7imA7HkN4mHXC6EJbmLuW7gu84v9/5tG/ZnpxdORSVF9G8WXOSmifx6bZP2Va4jW1F29iyfws7i3cy+bjJPLr80XqP76IBF/HqulcBmDJkCtmZ2dyx6A5W71rNucefy88H/5z3Nr1H/7T+jO05lqW5S9lWtI2R3UdyXr/zSExI9NblOewhrySPbq27oaqs3LGSEd1H0Ezsp1INJRaSwkXAj/2SwihVvdGnzK1uDA+6LYWZQKaqHg5Wr7UUTCzLK8ljT8keBnYeSEFpAYs2L6JH6x58lvsZ3VK70SmlEwfKD/DOhneYuXomKYkp/PG0P/LFni/YeWAnC79bWG+xJDZL5MlznmTWl7O8Z+xje45l+knTuWLOFbx+8eusz1vPte9cS1JCEm9lv8XpGaeTX5pPaotUWiW24sWcF2mb3JbJx08GnK4noMl2sTRlsZAUwuk+WovTmtjmDm8CTlDVPcHqtaRgGkKZp4ziimLySvLo36k/xRXFbC/aTkqLFC57/TJyi3IZ1GUQyc2TSWuZxvyN89m8f3O9LLt9cnsACsoKjpjWLrkdJ/Y8kRU7VrCnZA9/Ou1PrN+7npHdR/LzIT9ne9F2BnUZBDhn582bBf4pku91hcPuOZidtTdt4SaFSP54bQXQV0QygO3ApcBlfmW+B84AnheR/kAykBfBmEycW7BxAbuKd3Fp5qXct+Q+vs7/muFdh3N82vEUlBbw+8W/Z2fxTjyHPTXWVdskkN4mnfyD+ZR6SrliyBVUHKpgQ/4GVu1cxRNnP0GHlh04v9/53m6XVTtWMaDTAFomtjyirrySPCoPV9K9dfdq4zu07OD9HCwhQPUzfUsGxlekb0k9C3gE53bTZ1X1HhG5G1ipqnPdO46eBlIBBX6jqu+GqtNaCqZK5aFK9h7cyz9W/oPj047Hc9jDwcqD7ClxbiH8Ku8rkpsnsz5v/RG/Lk1tkUpxRXGtl5ncPJkyTxkAI7uP5O+T/s78jfMZ2X0kg7sMpnNKZxSlzFNG6xatSWiWQLmnvNo9+If1cLUDcbmn3HubozGREvXuo0ixpBCfluYupaC0gPc2vUdSQhKb92/m5bUv17m+Xm17cUrvU2iR0ILBXQbzWe5nvPTVS1wx5ArOOvYsBnUZxIOfPsgDEx6gXXI7wPrRTeNmScE0GocOH+KJlU/wq3d/xcBOA70/+ReELqld2FW8K+i8KYkpTDx2Iof0EHO+ngM43Saje4zmiiFX8G3+t9w65lYWb17s3K+uhzh0+BBJzZMaZN2MiRWWFExM8O0q2V60nXs+uYdtRdt4+9u3gbp14yz42QL2lOzh7OPOrtaHbowJLhYuNJs4UXUny/Nrnie9TTpdUrqwYOMCVu9azZyv59C9dXe+K/gu4LyBEkLOtTl4Dnvo3ro7ZZ4y1uetJ6N9Bsd3PJ59pfvo2KpjpFfJmLhlScHUyqHDh3jxixc5PeN0/rHyH3y89WM2FWxiZ/HOoPP4JoSxPcdy3vHncdmgy3hv03uc2udUWiS0oHvr7hSUFlBSWeL9MVcV32fLWEIwTZX8QdC7ot9zY91HpkallaVsLdzKrC9m8ULOC2wr2lZtevNmzUlrlca+0n1UHKpgZPeR3HLCLTy2/DFuHn0zFw28iK37t5LRPiNKa2BqEisHpGgJZ/2rytR2W8kfnBsUAs3jX5dvWf/5QtUTVhx2TcHUxYrtKxjQaQCf7/yc333wOz75/pOA5c7uezbNmzVn5uSZ1c7ec3bl0K11NzqndK63mOL9gFVfarMdQ5UNdLAKVbah953vMoMdSP3LBDswB6s/1HT/clVlgy3Tt1yoske7LS0pmLCoKvM2zGPVzlUs2ryIj7d+HLRsy+YtWXb1Mvp36h/6h1FBvryh/iNA9f8Mof4j1Oag5F++Nge7ugpn/cNZfrgHgZrONgNt89oc0IPtC9/xweoMtEz/OH1j9RXqoB1ouf7rHmo5geoLFFtN2y6YUNsinAQUrKwlhQAsKRw9VWXk0yNZtXPVES8LAWiT1IbjOh7HoM6DyOqexYBOAxjebTitEluFTAY1CXV2FmhcqKa0/zzBzrb8PwcqU1OS8C1fl/Wqqd6a6vefL9wDaKiDZ6A6/ecJta1rOmCHKh/uOodKLqG2YU3T/MuE850LtZ0CLce/TE3rH+pEIlCcdWF3Hxmvw3qYP370R5bvWM5/v/9vtReOVCWECcdM4M+n/5nh3YZH5EdaoZrOtTmDDPUfzbdsoLNl/3lqOvBVzR+sfKC4QsVf00EkWNyBtpP/etXH9vX9tzZn+rU5qw22DWrTagh1Fu8/f7D9VdP8gWILtC+Dfa985/UvE+g7FeyAX9OJSyRYS6EJUnUeszDjwxk8u+ZZVJX80nzv9N5tezOoyyB6tunJPaffQ7vkdtUSQbjN61BnZUfEVIuyvuVDnbGFc/Cq6Sw50FljqINMuLH7xxRoeqAYgiWkmurxrytUPeHWG+oAWJsWWqj6fOMOtU38y4WbCPy3T6g4A20D/5iC1VtT+doe2I92/oB1htlSQFUb1d+IESPUVLd1/1b9avdX+s81/9Ts17J17Myxygy8f6c9f5pe+9a1OuWNKfrOt++EXa9vHczAO853WqBygcb71hnuco+mTLBpNa1HqLLhjgu3fLjrEqxsqHUItz7/umtabjjbPFCZQN+PYLH71xVouXXZ9+FODxZLXesKR12/z7VahvPMuRqPsdZSaISKyot4atVTlFaW8s6Gd1i2fVm16R1bdmRcr3FcPPBiTs84na6pXWuss6Y+0mBnZqH6k6vK+U+rDzWdFdY0b23OBAMttz6XXdNyfIVzRuy7nJpaC3WZP1iXRk3fh1BdIaG6diJ1hh2pbpn6rLde67KWQtPhOeTR0spS7fVwL2UGmnh34hFnXhmPZOipz5+qy3OXB6wj0NlZOGf/geoIdqZXH+rrDDfcs66aWgmh6gi17rU5Gw21nWtzdl5fZ5rhtG7CWU5dWonB6qzP71isi9S6Yi2FxqvcU87Wwq2s2L6Cy9+4/IjpWd2zyM7M5uGlD5NblMuKa1aQ1f3IE4BgfeZV03wFu3AWT+J1vavE+/o3dXZLaiOzYOMCfr/49yQ3T2bJ90uOmJ7ZOZPCskK2FW0LeZE1mJq6gOL1gBCv6x0O2zZNiyWFGLe/bD9vfv0mByoOcNv7t3Gw8mC16b876Xfc88k9FN1eRJt723jH13RXjm85iHzfvjGmcbCkEIN2F+9mZ/FOTn/h9IDv361S21sg/eexg74xdgLkz368FkPKPGU8v+Z5fvfB79hXuq/G8rW9w8WSgTFHsv8PdWNJIYL2l+1n1NOj2LBvQ9Ay9XGGb19+Y0x9saRQzz7Y/AFn/POMsMr6/+zdGGOizZJCPanNHUCWBIwxsSqiSUFEJgJ/AxKAZ1T1Xr/pDwOnuYOtgM6q2i6SMR2tQPf+v5399hHl7j/zfqYOnXrEm8IsIRhjYlnE7j4SkQTgW+BMIBdYAWSr6rog5W8EhqnqlaHqjcbdR4FaASf3PpmcXTnVnjh6XMfjOL/f+dw7/t4jyhtjTDTFwt1Ho4CNqrrJDegl4FwgYFIAsoG7IhhPvfJ9Gc0ZGWfw/HnPH/FuYWOMaWwimRR6AL4v880FRgcqKCK9gQzggyDTpwHTAHr16lW/UdYg1LWCW0bfwr3j7yWpeVIDRmSMMZETyaQQ6GgarK/qUuA1VT0UaKKqPgU8BU73Uf2EVzttktpw3/j7GNBpACmJKWQ9ncXDEx+ORijGGBMxkUwKuUBPn+F0YEeQspcC10cwljo7v9/5vPH1Gzxw5gNcM+Ia73i7YGyMaYoimRRWAH1FJAPYjnPgv8y/kIgcD7QHPotgLHVS1XV0/cjrqyUEY4xpqppFqmJV9QA3AAuB9cArqrpWRO4Wkck+RbOBlzQGH8J0QvoJ9Grbi7+e+ddoh2KMMQ0ior9TUNV5wDy/cXf6Dc+IZAx1VdVKuPPkO2mV2CrK0RhjTMOIWEuhMSsqL/J+vnH0jVGMxBhjGpYlhQCqfoMw64JZpLVKi3I0xhjTcCwpBPDf7/9LM2nGT477SbRDMcaYBmVJIYB7/3sv4380ntZJraMdijHGNChLCn52HHB+SnFK71OiHIkxxjQ8Swp+/vXFvwC4eODFUY7EGGManiUFP7e9fxsAx3Y4NsqRGGNMw7OX7PjwHPaQkpjClCFToh2KMcZEhbUUfGzct5GSyhJG9RgV7VCMMSYqLCn4+HL3lwAM6jIoypEYY0x0WFLw8dWer2gmzeif1j/aoRhjTFRYUvCxaucqju94PC0TW0Y7FGOMiQpLCi5V5bPczxjbc2y0QzHGmKixpODaU7KHfaX7mLl6ZrRDMcaYqLGk4Pp679cALLx8YZQjMcaY6LGk4KpKCv3S+kU5EmOMiR5LCq6v935Nq8RWpLdJj3YoxhgTNZYUXOv3rqdfWj+aiW0SY0z8siOga1PBJnvekTEm7llScOWX5tOpVadoh2GMMVFlSQE4rIcpKC2gQ8sO0Q7FGGOiKqJJQUQmisg3IrJRRG4PUuZiEVknImtF5N+RjCeYwrJCFKVjy47RWLwxxsSMiD06W0QSgMeBM4FcYIWIzFXVdT5l+gJ3ACeqaoGIdI5UPKHsKt4FQMdWlhSMMfEtki2FUcBGVd2kqhXAS8C5fmWuAR5X1QIAVd0TwXiCWr59OQBDuw6NxuKNMSZmRDIp9AC2+QznuuN8HQccJyL/FZGlIjIxUEUiMk1EVorIyry8vHoPdGvhVgCO73h8vddtjDGNSSSTggQYp37DzYG+wKlANvCMiLQ7YibVp1Q1S1WzOnWq/zuE9pftJ7VFKokJifVetzHGNCaRTAq5QE+f4XRgR4Ayb6pqpapuBr7BSRINqqCsgHbJR+QiY4yJO5FMCiuAviKSISItgEuBuX5l5gCnAYhIGk530qYIxhTQ/rL9tE9u39CLNcaYmBOxpKCqHuAGYCGwHnhFVdeKyN0iMtktthDIF5F1wGLg/6lqfqRiCqagtID2LS0pGGNMxG5JBVDVecA8v3F3+nxW4Fb3L2r2lOxh/d710QzBGGNigv2iGedheNePvD7aYRhjTNTFfVKoPFQJQJeULlGOxBhjoi/uk0LeQed3D51TovJjamOMiSlxnxT2lDg/ou6UYk9INcYYSwpuUrCWgjHGWFIgr8TpPrJ3KRhjjCUFaykYY4yPuE8KVRea7TEXxhhjSYE9JXvomtoVkUDP7zPGmPgS90khvzTf3rhmjDGuuE8KxRXFtElqE+0wjDEmJsR9UjhQfoDUFqnRDsMYY2JC3CeF4opiWie1jnYYxhgTEywpVBTTuoUlBWOMAUsKHKiw7iNjjKkS10nhsB5mX+k+u9BsjDGuuE4Ku4t3A5DeJj3KkRhjTGyI66SwZf8WAPq06xPVOIwxJlaElRRE5BgRSXI/nyoiN4lIo38uRNVzj7qmdo1yJMYYExvCbSm8DhwSkWOBmUAG8O+IRdVA9pftB6B9cvsoR2KMMbEh3KRwWFU9wPnAI6r6S6BbTTOJyEQR+UZENorI7QGm/0JE8kRkjft3de3CPzpVSaFtctuGXKwxxsSs5mGWqxSRbGAK8BN3XGKoGUQkAXgcOBPIBVaIyFxVXedX9GVVvaEWMdebqqRgdx8ZY4wj3JbCVGAMcI+qbhaRDOBfNcwzCtioqptUtQJ4CTi37qHWv/1l+2ndojXNm4WbG40xpmkLKymo6jpVvUlVZ4tIe6C1qt5bw2w9gG0+w7nuOH8/FZEvROQ1EekZqCIRmSYiK0VkZV5eXjghhyW/NJ/2Le16gjHGVAn37qMPRaSNiHQAcoDnROShmmYLME79ht8C+qjqYOB94IVAFanqU6qapapZnTrV32sz8w7m8X3h9/VWnzFcgkQNAAAY90lEQVTGNHbhdh+1VdUi4ALgOVUdAYyvYZ5cwPfMPx3Y4VtAVfNVtdwdfBoYEWY89WJPyR7O6ntWQy7SGGNiWrhJobmIdAMuBt4Oc54VQF8RyRCRFsClwFzfAm6dVSYD68Osu17kleSR1iqtIRdpjDExLdykcDewEPhOVVeIyI+ADaFmcG9hvcGdbz3wiqquFZG7RWSyW+wmEVkrIjnATcAv6rISdXWg4gD/zPlnQy7SGGNiWli33ajqq8CrPsObgJ+GMd88YJ7fuDt9Pt8B3BFusPVJVSmuKOaOcVFZvDHGxKRwLzSni8gbIrJHRHaLyOsi0qifIldxqALPYY89NtsYY3yE2330HM71gO44t5W+5Y5rtIorigEsKRhjjI9wk0InVX1OVT3u3/NA/d0bGgWWFIwx5kjhJoW9InK5iCS4f5cD+ZEMLNIsKRhjzJHCTQpX4tyOugvYCVyI8+iLRquwvBDA3s9sjDE+wn3MxfeqOllVO6lqZ1U9D+eHbI2W97HZ9pgLY4zxOpo3r91ab1FEQUFpAWDvUjDGGF9HkxQCPduo0ahqKbRLbvQvkDPGmHpzNEnB/+F2jUpBmdNSsKRgjDE/CPmLZhE5QOCDvwAtIxJRAzlQfoAWCS1Iap4U7VCMMSZmhEwKqtpkb80pqSwhJTEl2mEYY0xMOZruo0atpLKElBaWFIwxxlf8JoWKEnKLcqMdhjHGxJT4TQqVJYzo1qDv9DHGmJgXt0mhuKKYVTtXRTsMY4yJKXGbFEoqSph07KRoh2GMMTElfpOCXWg2xpgjxG9SqLBbUo0xxl/8JoXKEntstjHG+InbpFBcUWwtBWOM8ROXScFz2EPFoQq7pmCMMX4imhREZKKIfCMiG0Xk9hDlLhQRFZGsSMZTpaSiBIC7PryrIRZnjDGNRsSSgogkAI8Dk4ABQLaIDAhQrjVwE7AsUrH4K6l0ksITZz/RUIs0xphGIZIthVHARlXdpKoVwEvAuQHK/RH4K1AWwViqqWop2IVmY4ypLpJJoQewzWc41x3nJSLDgJ6q+naoikRkmoisFJGVeXl5Rx1YcUUxgF1oNsYYP5FMCoHezOZ9N4OINAMeBn5VU0Wq+pSqZqlqVqdOnY46sKruI7vQbIwx1UUyKeQCPX2G04EdPsOtgUzgQxHZApwAzG2Ii81V3UfWUjDGmOoimRRWAH1FJENEWgCXAnOrJqpqoaqmqWofVe0DLAUmq+rKCMYEWEvBGGOCiVhSUFUPcAOwEFgPvKKqa0XkbhGZHKnlhsMuNBtjTGAhX8d5tFR1HjDPb9ydQcqeGslYfNmFZmOMCSwuf9Fs3UfGGBNYfCYFu9BsjDEBxWdSqCwhKSGJhGYJ0Q7FGGNiSnwmhQp7bLYxxgQSl0mhuLLYricYY0wAcZkU7K1rxhgTWHwmhcoS1u9dH+0wjDEm5sRlUthftp/xPxof7TCMMSbmxGVSKCgtoH1y+2iHYYwxMScuk8L+sv20S24X7TCMMSbmxF1SUFUKyqylYIwxgcRdUijzlFFxqIK2yW2jHYoxxsScuEsKByoOANAmqU2UIzHGmNgTf0mh3EkKrVu0jnIkxhgTe+IuKRSVFwHWUjDGmEDiLilUdR9d8MoFUY7EGGNiT/wlBbf7aOlVS6MciTHGxJ74SwpuS6F1kl1TMMYYf3GXFOyagjHGBBd3ScHuPjLGmOAimhREZKKIfCMiG0Xk9gDTrxWRL0VkjYgsEZEBkYwHfug+spfsGGPMkSKWFEQkAXgcmAQMALIDHPT/raqDVHUo8FfgoUjFU+VA+QFaJbayV3EaY0wAkWwpjAI2quomVa0AXgLO9S2gqkU+gymARjAewLmmcLDyYKQXY4wxjVLzCNbdA9jmM5wLjPYvJCLXA7cCLYDTIxgP4HQf9e3QN9KLMcaYRimSLQUJMO6IloCqPq6qxwC3AdMDViQyTURWisjKvLy8owrqQMUBux3VGGOCiGRSyAV6+gynAztClH8JOC/QBFV9SlWzVDWrU6dORxXUgfIDdueRMcYEEcmksALoKyIZItICuBSY61tARHz7cc4GNkQwHsC5pmC/UTDGmMAidk1BVT0icgOwEEgAnlXVtSJyN7BSVecCN4jIeKASKACmRCqeKtZ9ZIwxwUXyQjOqOg+Y5zfuTp/PN0dy+YFY95ExxgQXd79oLiov4slVT0Y7DGOMiUlxlRTKPeWUHyrnT6f9KdqhGGNMTIqrpFBYXghg72c2xpgg4iop2BNSjTEmtLhKCoVlTkthypyI3+RkjDGNUnwlBbf7aPGUxVGOxBhjYlNcJYWq7qO2SXZNwRhjAomrpFDVfWTXFIwxJrD4Sgp295ExxoQU0V80x5qqloJ1H5l4VVlZSW5uLmVlZdEOxURIcnIy6enpJCYm1mn+uEoKVdcUEhPqtrGMaexyc3Np3bo1ffr0QSTQ0+1NY6aq5Ofnk5ubS0ZGRp3qiKvuo1JPKR1adoh2GMZETVlZGR07drSE0ESJCB07djyqlmB8JYXKUlo2bxntMIyJKksITdvR7t/4SgqeUlomWlIwxphg4i8pWEvBmKjJz89n6NChDB06lK5du9KjRw/vcEVFRVh1TJ06lW+++SZkmccff5xZs2bVR8j1bvr06TzyyCNHjJ8yZQqdOnVi6NChUYjqB3F1obm00loKxkRTx44dWbNmDQAzZswgNTWVX//619XKqCqqSrNmgc9Zn3vuuRqXc/311x99sA3syiuv5Prrr2fatGlRjSO+koK1FIzxumXBLazZtaZe6xzadSiPTDzyLLgmGzdu5LzzzmPcuHEsW7aMt99+mz/84Q98/vnnlJaWcskll3Dnnc77ucaNG8djjz1GZmYmaWlpXHvttcyfP59WrVrx5ptv0rlzZ6ZPn05aWhq33HIL48aNY9y4cXzwwQcUFhby3HPPMXbsWEpKSrjiiivYuHEjAwYMYMOGDTzzzDNHnKnfddddzJs3j9LSUsaNG8cTTzyBiPDtt99y7bXXkp+fT0JCAv/5z3/o06cPf/7zn5k9ezbNmjXjnHPO4Z577glrG5xyyils3Lix1tuuvsVX91FlKcnNk6MdhjEmgHXr1nHVVVexevVqevTowb333svKlSvJycnhvffeY926dUfMU1hYyCmnnEJOTg5jxozh2WefDVi3qrJ8+XLuv/9+7r77bgD+/ve/07VrV3Jycrj99ttZvXp1wHlvvvlmVqxYwZdffklhYSELFiwAIDs7m1/+8pfk5OTw6aef0rlzZ9566y3mz5/P8uXLycnJ4Ve/+lU9bZ2GE1cthTJPGQu/WxjtMIyJCXU5o4+kY445hpEjR3qHZ8+ezcyZM/F4POzYsYN169YxYMCAavO0bNmSSZMmATBixAg++eSTgHVfcMEF3jJbtmwBYMmSJdx2220ADBkyhIEDBwacd9GiRdx///2UlZWxd+9eRowYwQknnMDevXv5yU9+Ajg/GAN4//33ufLKK2nZ0umR6NCh8d0CH1dJodRTSnZmdrTDMMYEkJKS4v28YcMG/va3v7F8+XLatWvH5ZdfHvDe+xYtWng/JyQk4PF4AtadlJR0RBlVrTGmgwcPcsMNN/D555/To0cPpk+f7o0j0K2fqtrob/mNu+4ju6ZgTOwrKiqidevWtGnThp07d7JwYf238MeNG8crr7wCwJdffhmwe6q0tJRmzZqRlpbGgQMHeP311wFo3749aWlpvPXWW4Dzo8CDBw8yYcIEZs6cSWlpKQD79u2r97gjLaJJQUQmisg3IrJRRG4PMP1WEVknIl+IyCIR6R3JeOx3CsY0DsOHD2fAgAFkZmZyzTXXcOKJJ9b7Mm688Ua2b9/O4MGDefDBB8nMzKRt2+rPRevYsSNTpkwhMzOT888/n9GjR3unzZo1iwcffJDBgwczbtw48vLyOOecc5g4cSJZWVkMHTqUhx9+OOCyZ8yYQXp6Ounp6fTp0weAiy66iJNOOol169aRnp7O888/X+/rHA4JpwlVp4pFEoBvgTOBXGAFkK2q63zKnAYsU9WDInIdcKqqXhKq3qysLF25cmWdYmp1TytKPaXoXZFZZ2Ni3fr16+nfv3+0w4gJHo8Hj8dDcnIyGzZsYMKECWzYsIHmzRt/r3qg/Swiq1Q1q6Z5I7n2o4CNqrrJDegl4FzAmxRU1fcVaEuByyMVjKpS6inl9yf/PlKLMMY0IsXFxZxxxhl4PB5UlSeffLJJJISjFckt0APY5jOcC4wOUhbgKmB+oAkiMg2YBtCrV686BVN+qBzArikYYwBo164dq1atinYYMSeS1xQCXYIP2G8jIpcDWcD9gaar6lOqmqWqWZ06dapTMGUe544Bu6ZgjDHBRbKlkAv09BlOB3b4FxKR8cDvgFNUtTxSwZRWOncD2I/XjDEmuEi2FFYAfUUkQ0RaAJcCc30LiMgw4ElgsqruiWAslHqcpGDdR8YYE1zEkoKqeoAbgIXAeuAVVV0rIneLyGS32P1AKvCqiKwRkblBqjtqVS0F6z4yxpjgIvo7BVWdp6rHqeoxqnqPO+5OVZ3rfh6vql1Udaj7Nzl0jXVnLQVjou/UU0894odojzzyCP/7v/8bcr7U1FQAduzYwYUXXhi07ppuV3/kkUc4ePCgd/iss85i//794YTeoD788EPOOeecI8Y/9thjHHvssYgIe/fujciy4+YXzdZSMCb6srOzeemll6qNe+mll8jODu/xM927d+e1116r8/L9k8K8efNo165dnetraCeeeCLvv/8+vXtH7ne+8ZMUrKVgTJ3JH+rneT4XXnghb7/9NuXlzj0lW7ZsYceOHYwbN877u4Hhw4czaNAg3nzzzSPm37JlC5mZmYDzCIpLL72UwYMHc8kll3gfLQFw3XXXkZWVxcCBA7nrrrsAePTRR9mxYwennXYap512GgB9+vTxnnE/9NBDZGZmkpmZ6X0JzpYtW+jfvz/XXHMNAwcOZMKECdWWU+Wtt95i9OjRDBs2jPHjx7N7927A+S3E1KlTGTRoEIMHD/Y+JmPBggUMHz6cIUOGcMYZZ4S9/YYNG+b9BXTEVL3QorH8jRgxQuviza/fVGagq3asqtP8xjQF69ati3YIetZZZ+mcOXNUVfUvf/mL/vrXv1ZV1crKSi0sLFRV1by8PD3mmGP08OHDqqqakpKiqqqbN2/WgQMHqqrqgw8+qFOnTlVV1ZycHE1ISNAVK1aoqmp+fr6qqno8Hj3llFM0JydHVVV79+6teXl53liqhleuXKmZmZlaXFysBw4c0AEDBujnn3+umzdv1oSEBF29erWqql500UX64osvHrFO+/bt88b69NNP66233qqqqr/5zW/05ptvrlZuz549mp6erps2baoWq6/Fixfr2WefHXQb+q+Hv0D7GVipYRxj46elUGktBWNigW8Xkm/Xkary29/+lsGDBzN+/Hi2b9/uPeMO5OOPP+byy52HIAwePJjBgwd7p73yyisMHz6cYcOGsXbt2oAPu/O1ZMkSzj//fFJSUkhNTeWCCy7wPoY7IyPD++Id30dv+8rNzeXHP/4xgwYN4v7772ft2rWA8yht37fAtW/fnqVLl3LyySeTkZEBxN7jteMnKXjsmoIxseC8885j0aJF3reqDR8+HHAeMJeXl8eqVatYs2YNXbp0Cfi4bF+BHlO9efNmHnjgARYtWsQXX3zB2WefXWM9GuIZcFWP3Ybgj+e+8cYbueGGG/jyyy958sknvcvTAI/SDjQulsRPUrCWgjExITU1lVNPPZUrr7yy2gXmwsJCOnfuTGJiIosXL2br1q0h6zn55JOZNWsWAF999RVffPEF4Dx2OyUlhbZt27J7927mz//h6TmtW7fmwIEDAeuaM2cOBw8epKSkhDfeeIOTTjop7HUqLCykR48eALzwwgve8RMmTOCxxx7zDhcUFDBmzBg++ugjNm/eDMTe47XjJyl47BfNxsSK7OxscnJyuPTSS73jfvazn7Fy5UqysrKYNWsW/fr1C1nHddddR3FxMYMHD+avf/0ro0aNApy3qA0bNoyBAwdy5ZVXVnvs9rRp05g0aZL3QnOV4cOH84tf/IJRo0YxevRorr76aoYNGxb2+syYMcP76Ou0tDTv+OnTp1NQUEBmZiZDhgxh8eLFdOrUiaeeeooLLriAIUOGcMklgR8MvWjRIu/jtdPT0/nss8949NFHSU9PJzc3l8GDB3P11VeHHWO4Ivbo7Eip66Oz3/z6TV784kVm/3Q2iQmJEYjMmNhnj86OD7H66OyYcm6/czm337nRDsMYY2Ja3HQfGWOMqZklBWPiTGPrMja1c7T715KCMXEkOTmZ/Px8SwxNlKqSn59PcnLdb6iJm2sKxhi8d67k5eVFOxQTIcnJyaSnp9d5fksKxsSRxMRE7y9pjQnEuo+MMcZ4WVIwxhjjZUnBGGOMV6P7RbOI5AGhH4oSXBoQmdcVxS5b5/hg6xwfjmade6tqp5oKNbqkcDREZGU4P/NuSmyd44Otc3xoiHW27iNjjDFelhSMMcZ4xVtSeCraAUSBrXN8sHWODxFf57i6pmCMMSa0eGspGGOMCcGSgjHGGK+4SAoiMlFEvhGRjSJye7TjqS8i0lNEFovIehFZKyI3u+M7iMh7IrLB/be9O15E5FF3O3whIsOjuwZ1JyIJIrJaRN52hzNEZJm7zi+LSAt3fJI7vNGd3ieacdeViLQTkddE5Gt3f49p6vtZRH7pfq+/EpHZIpLc1PaziDwrIntE5CufcbXeryIyxS2/QUSmHE1MTT4piEgC8DgwCRgAZIvIgOhGVW88wK9UtT9wAnC9u263A4tUtS+wyB0GZxv0df+mAU80fMj15mZgvc/wfcDD7joXAFe5468CClT1WOBht1xj9Ddggar2A4bgrHuT3c8i0gO4CchS1UwgAbiUprefnwcm+o2r1X4VkQ7AXcBoYBRwV1UiqRNVbdJ/wBhgoc/wHcAd0Y4rQuv6JnAm8A3QzR3XDfjG/fwkkO1T3luuMf0B6e5/ltOBtwHB+ZVnc/99DiwExrifm7vlJNrrUMv1bQNs9o+7Ke9noAewDejg7re3gR83xf0M9AG+qut+BbKBJ33GVytX278m31Lghy9XlVx3XJPiNpeHAcuALqq6E8D9t7NbrKlsi0eA3wCH3eGOwH5V9bjDvuvlXWd3eqFbvjH5EZAHPOd2mT0jIik04f2sqtuBB4DvgZ04+20VTXs/V6ntfq3X/R0PSUECjGtS9+GKSCrwOnCLqhaFKhpgXKPaFiJyDrBHVVf5jg5QVMOY1lg0B4YDT6jqMKCEH7oUAmn06+x2f5wLZADdgRSc7hN/TWk/1yTYOtbrusdDUsgFevoMpwM7ohRLvRORRJyEMEtV/+OO3i0i3dzp3YA97vimsC1OBCaLyBbgJZwupEeAdiJS9dIo3/XyrrM7vS2wryEDrge5QK6qLnOHX8NJEk15P48HNqtqnqpWAv8BxtK093OV2u7Xet3f8ZAUVgB93bsWWuBcrJob5ZjqhYgIMBNYr6oP+UyaC1TdgTAF51pD1fgr3LsYTgAKq5qpjYWq3qGq6araB2dffqCqPwMWAxe6xfzXuWpbXOiWb1RnkKq6C9gmIse7o84A1tGE9zNOt9EJItLK/Z5XrXOT3c8+artfFwITRKS928Ka4I6rm2hfZGmgCzlnAd8C3wG/i3Y89bhe43CaiV8Aa9y/s3D6UhcBG9x/O7jlBedOrO+AL3Hu7Ij6ehzF+p8KvO1+/hGwHNgIvAokueOT3eGN7vQfRTvuOq7rUGClu6/nAO2b+n4G/gB8DXwFvAgkNbX9DMzGuWZSiXPGf1Vd9itwpbvuG4GpRxOTPebCGGOMVzx0HxljjAmTJQVjjDFelhSMMcZ4WVIwxhjjZUnBGGOMlyUFY1wickhE1vj81dsTdUWkj++TMI2JVc1rLmJM3ChV1aHRDsKYaLKWgjE1EJEtInKfiCx3/451x/cWkUXus+0XiUgvd3wXEXlDRHLcv7FuVQki8rT7joB3RaSlW/4mEVnn1vNSlFbTGMCSgjG+Wvp1H13iM61IVUcBj+E8awn38z9VdTAwC3jUHf8o8JGqDsF5RtFad3xf4HFVHQjsB37qjr8dGObWc22kVs6YcNgvmo1xiUixqqYGGL8FOF1VN7kPINylqh1FZC/Oc+8r3fE7VTVNRPKAdFUt96mjD/CeOi9OQURuAxJV9U8isgAoxnl8xRxVLY7wqhoTlLUUjAmPBvkcrEwg5T6fD/HDNb2zcZ5pMwJY5fMUUGManCUFY8Jzic+/n7mfP8V5UivAz4Al7udFwHXgfZd0m2CVikgzoKeqLsZ5cVA74IjWijENxc5IjPlBSxFZ4zO8QFWrbktNEpFlOCdS2e64m4BnReT/4bwZbao7/mbgKRG5CqdFcB3OkzADSQD+JSJtcZ6C+bCq7q+3NTKmluyagjE1cK8pZKnq3mjHYkykWfeRMcYYL2spGGOM8bKWgjHGGC9LCsYYY7wsKRhjjPGypGCMMcbLkoIxxhiv/w91KplCZHayiwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step\n",
      "1500/1500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7929266409715017, 0.8164]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8792360235850016, 0.7640000004768371]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 125us/step - loss: 1.9754 - acc: 0.1576 - val_loss: 1.9261 - val_acc: 0.1630\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.9579 - acc: 0.1655 - val_loss: 1.9134 - val_acc: 0.2070\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9344 - acc: 0.1901 - val_loss: 1.9019 - val_acc: 0.2210\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9173 - acc: 0.2016 - val_loss: 1.8900 - val_acc: 0.2480\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.9089 - acc: 0.2108 - val_loss: 1.8779 - val_acc: 0.2680\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8940 - acc: 0.2255 - val_loss: 1.8651 - val_acc: 0.2790\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8817 - acc: 0.2323 - val_loss: 1.8502 - val_acc: 0.2950\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8710 - acc: 0.2417 - val_loss: 1.8343 - val_acc: 0.3020\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8579 - acc: 0.2475 - val_loss: 1.8168 - val_acc: 0.3070\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8485 - acc: 0.2537 - val_loss: 1.8003 - val_acc: 0.3360\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8309 - acc: 0.2672 - val_loss: 1.7808 - val_acc: 0.3460\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8170 - acc: 0.2724 - val_loss: 1.7611 - val_acc: 0.3580\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.8038 - acc: 0.2881 - val_loss: 1.7393 - val_acc: 0.3760\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7816 - acc: 0.2935 - val_loss: 1.7169 - val_acc: 0.4050\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.7704 - acc: 0.3020 - val_loss: 1.6934 - val_acc: 0.4190\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7577 - acc: 0.3103 - val_loss: 1.6735 - val_acc: 0.4510\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.7291 - acc: 0.3245 - val_loss: 1.6487 - val_acc: 0.4740\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.7187 - acc: 0.3263 - val_loss: 1.6243 - val_acc: 0.4860\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6963 - acc: 0.3505 - val_loss: 1.5976 - val_acc: 0.5080\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6769 - acc: 0.3521 - val_loss: 1.5711 - val_acc: 0.5230\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6695 - acc: 0.3531 - val_loss: 1.5482 - val_acc: 0.5350\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6504 - acc: 0.3647 - val_loss: 1.5212 - val_acc: 0.5540\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.6300 - acc: 0.3801 - val_loss: 1.4974 - val_acc: 0.5590\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6129 - acc: 0.3828 - val_loss: 1.4749 - val_acc: 0.5700\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5916 - acc: 0.3877 - val_loss: 1.4498 - val_acc: 0.5800\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5781 - acc: 0.3943 - val_loss: 1.4264 - val_acc: 0.5880\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5684 - acc: 0.3948 - val_loss: 1.4056 - val_acc: 0.5920\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5379 - acc: 0.4176 - val_loss: 1.3825 - val_acc: 0.6050\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.5199 - acc: 0.4292 - val_loss: 1.3586 - val_acc: 0.6080\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.5005 - acc: 0.4319 - val_loss: 1.3394 - val_acc: 0.6230\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4995 - acc: 0.4303 - val_loss: 1.3152 - val_acc: 0.6290\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4871 - acc: 0.4340 - val_loss: 1.2965 - val_acc: 0.6280\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4693 - acc: 0.4448 - val_loss: 1.2775 - val_acc: 0.6380\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4472 - acc: 0.4527 - val_loss: 1.2592 - val_acc: 0.6440\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4306 - acc: 0.4575 - val_loss: 1.2378 - val_acc: 0.6510\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.4123 - acc: 0.4707 - val_loss: 1.2182 - val_acc: 0.6580\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3987 - acc: 0.4679 - val_loss: 1.1991 - val_acc: 0.6670\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.3926 - acc: 0.4759 - val_loss: 1.1811 - val_acc: 0.6590\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3819 - acc: 0.4827 - val_loss: 1.1677 - val_acc: 0.6720\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3556 - acc: 0.4899 - val_loss: 1.1489 - val_acc: 0.6760\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3353 - acc: 0.5009 - val_loss: 1.1294 - val_acc: 0.6790\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3361 - acc: 0.4915 - val_loss: 1.1161 - val_acc: 0.6790\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3320 - acc: 0.5013 - val_loss: 1.1035 - val_acc: 0.6760\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3199 - acc: 0.5088 - val_loss: 1.0886 - val_acc: 0.6820\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.3019 - acc: 0.5147 - val_loss: 1.0735 - val_acc: 0.6850\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2882 - acc: 0.5144 - val_loss: 1.0571 - val_acc: 0.6890\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2782 - acc: 0.5224 - val_loss: 1.0437 - val_acc: 0.6910\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2643 - acc: 0.5261 - val_loss: 1.0310 - val_acc: 0.6970\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2586 - acc: 0.5156 - val_loss: 1.0199 - val_acc: 0.6970\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2427 - acc: 0.5357 - val_loss: 1.0077 - val_acc: 0.7030\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2370 - acc: 0.5357 - val_loss: 0.9952 - val_acc: 0.7060\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2298 - acc: 0.5369 - val_loss: 0.9838 - val_acc: 0.7050\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2108 - acc: 0.5489 - val_loss: 0.9716 - val_acc: 0.7060\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2079 - acc: 0.5469 - val_loss: 0.9629 - val_acc: 0.7090\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1915 - acc: 0.5505 - val_loss: 0.9517 - val_acc: 0.7110\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1867 - acc: 0.5509 - val_loss: 0.9407 - val_acc: 0.7130\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1825 - acc: 0.5589 - val_loss: 0.9317 - val_acc: 0.7130\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1673 - acc: 0.5644 - val_loss: 0.9240 - val_acc: 0.7220\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1730 - acc: 0.5644 - val_loss: 0.9135 - val_acc: 0.7160\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1575 - acc: 0.5775 - val_loss: 0.9052 - val_acc: 0.7240\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1538 - acc: 0.5640 - val_loss: 0.8974 - val_acc: 0.7230\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1414 - acc: 0.5811 - val_loss: 0.8877 - val_acc: 0.7230\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1352 - acc: 0.5659 - val_loss: 0.8771 - val_acc: 0.7260\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1236 - acc: 0.5813 - val_loss: 0.8703 - val_acc: 0.7320\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1200 - acc: 0.5787 - val_loss: 0.8639 - val_acc: 0.7330\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1167 - acc: 0.5848 - val_loss: 0.8574 - val_acc: 0.7310\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1069 - acc: 0.5892 - val_loss: 0.8518 - val_acc: 0.7340\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0908 - acc: 0.5913 - val_loss: 0.8426 - val_acc: 0.7340\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0942 - acc: 0.5948 - val_loss: 0.8373 - val_acc: 0.7320\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0873 - acc: 0.5915 - val_loss: 0.8304 - val_acc: 0.7340\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0732 - acc: 0.5944 - val_loss: 0.8240 - val_acc: 0.7350\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0704 - acc: 0.6003 - val_loss: 0.8160 - val_acc: 0.7350\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0740 - acc: 0.5972 - val_loss: 0.8136 - val_acc: 0.7350\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0585 - acc: 0.5995 - val_loss: 0.8088 - val_acc: 0.7370\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0587 - acc: 0.6052 - val_loss: 0.8024 - val_acc: 0.7390\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0560 - acc: 0.6073 - val_loss: 0.7958 - val_acc: 0.7390\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0436 - acc: 0.6116 - val_loss: 0.7889 - val_acc: 0.7350\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0417 - acc: 0.6083 - val_loss: 0.7859 - val_acc: 0.7370\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0278 - acc: 0.6104 - val_loss: 0.7785 - val_acc: 0.7390\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0151 - acc: 0.6237 - val_loss: 0.7745 - val_acc: 0.7420\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0322 - acc: 0.6212 - val_loss: 0.7703 - val_acc: 0.7420\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0169 - acc: 0.6217 - val_loss: 0.7647 - val_acc: 0.7450\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0113 - acc: 0.6273 - val_loss: 0.7604 - val_acc: 0.7430\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0113 - acc: 0.6213 - val_loss: 0.7571 - val_acc: 0.7420\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0072 - acc: 0.6272 - val_loss: 0.7541 - val_acc: 0.7500\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9939 - acc: 0.6312 - val_loss: 0.7501 - val_acc: 0.7460\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9867 - acc: 0.6379 - val_loss: 0.7441 - val_acc: 0.7490\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9897 - acc: 0.6349 - val_loss: 0.7431 - val_acc: 0.7480\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9822 - acc: 0.6339 - val_loss: 0.7335 - val_acc: 0.7450\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9763 - acc: 0.6337 - val_loss: 0.7308 - val_acc: 0.7480\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9804 - acc: 0.6324 - val_loss: 0.7278 - val_acc: 0.7500\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9732 - acc: 0.6377 - val_loss: 0.7261 - val_acc: 0.7440\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9639 - acc: 0.6441 - val_loss: 0.7198 - val_acc: 0.7500\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9731 - acc: 0.6377 - val_loss: 0.7198 - val_acc: 0.7540\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9646 - acc: 0.6425 - val_loss: 0.7140 - val_acc: 0.7530\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9518 - acc: 0.6417 - val_loss: 0.7101 - val_acc: 0.7550\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9494 - acc: 0.6511 - val_loss: 0.7077 - val_acc: 0.7560\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9378 - acc: 0.6497 - val_loss: 0.7025 - val_acc: 0.7560\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9420 - acc: 0.6509 - val_loss: 0.7018 - val_acc: 0.7560\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9388 - acc: 0.6507 - val_loss: 0.6973 - val_acc: 0.7600\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9295 - acc: 0.6505 - val_loss: 0.6967 - val_acc: 0.7580\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9237 - acc: 0.6549 - val_loss: 0.6929 - val_acc: 0.7610\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9223 - acc: 0.6583 - val_loss: 0.6899 - val_acc: 0.7600\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9192 - acc: 0.6588 - val_loss: 0.6862 - val_acc: 0.7580\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9275 - acc: 0.6548 - val_loss: 0.6838 - val_acc: 0.7580\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9154 - acc: 0.6612 - val_loss: 0.6802 - val_acc: 0.7600\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9198 - acc: 0.6552 - val_loss: 0.6788 - val_acc: 0.7580\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9052 - acc: 0.6623 - val_loss: 0.6743 - val_acc: 0.7610\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9036 - acc: 0.6657 - val_loss: 0.6709 - val_acc: 0.7600\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8976 - acc: 0.6679 - val_loss: 0.6686 - val_acc: 0.7620\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8870 - acc: 0.6661 - val_loss: 0.6671 - val_acc: 0.7640\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8976 - acc: 0.6645 - val_loss: 0.6661 - val_acc: 0.7660\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9005 - acc: 0.6697 - val_loss: 0.6643 - val_acc: 0.7640\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8856 - acc: 0.6709 - val_loss: 0.6639 - val_acc: 0.7630\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8790 - acc: 0.6752 - val_loss: 0.6606 - val_acc: 0.7650\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8813 - acc: 0.6724 - val_loss: 0.6586 - val_acc: 0.7660\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8859 - acc: 0.6736 - val_loss: 0.6571 - val_acc: 0.7630\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8749 - acc: 0.6759 - val_loss: 0.6535 - val_acc: 0.7660\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8773 - acc: 0.6779 - val_loss: 0.6518 - val_acc: 0.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8621 - acc: 0.6809 - val_loss: 0.6490 - val_acc: 0.7670\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8651 - acc: 0.6791 - val_loss: 0.6489 - val_acc: 0.7620\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8551 - acc: 0.6816 - val_loss: 0.6445 - val_acc: 0.7720\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8641 - acc: 0.6804 - val_loss: 0.6447 - val_acc: 0.7680\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8604 - acc: 0.6781 - val_loss: 0.6435 - val_acc: 0.7620\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8555 - acc: 0.6748 - val_loss: 0.6408 - val_acc: 0.7720\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8524 - acc: 0.6815 - val_loss: 0.6401 - val_acc: 0.7660\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8580 - acc: 0.6873 - val_loss: 0.6383 - val_acc: 0.7680\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8561 - acc: 0.6812 - val_loss: 0.6364 - val_acc: 0.7690\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8419 - acc: 0.6903 - val_loss: 0.6339 - val_acc: 0.7700\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8464 - acc: 0.6848 - val_loss: 0.6333 - val_acc: 0.7730\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8415 - acc: 0.6827 - val_loss: 0.6326 - val_acc: 0.7700\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8435 - acc: 0.6868 - val_loss: 0.6291 - val_acc: 0.7720\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8506 - acc: 0.6825 - val_loss: 0.6295 - val_acc: 0.7710\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8365 - acc: 0.6937 - val_loss: 0.6279 - val_acc: 0.7700\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8445 - acc: 0.6848 - val_loss: 0.6276 - val_acc: 0.7670\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8484 - acc: 0.6827 - val_loss: 0.6272 - val_acc: 0.7700\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8279 - acc: 0.6892 - val_loss: 0.6235 - val_acc: 0.7730\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8369 - acc: 0.6873 - val_loss: 0.6249 - val_acc: 0.7720\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8190 - acc: 0.6923 - val_loss: 0.6219 - val_acc: 0.7750\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8212 - acc: 0.6975 - val_loss: 0.6189 - val_acc: 0.7760\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8309 - acc: 0.6936 - val_loss: 0.6189 - val_acc: 0.7730\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8210 - acc: 0.6943 - val_loss: 0.6147 - val_acc: 0.7790\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8236 - acc: 0.6889 - val_loss: 0.6149 - val_acc: 0.7720\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8070 - acc: 0.7049 - val_loss: 0.6137 - val_acc: 0.7700\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8167 - acc: 0.6939 - val_loss: 0.6125 - val_acc: 0.7700\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8081 - acc: 0.7040 - val_loss: 0.6098 - val_acc: 0.7740\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8115 - acc: 0.6965 - val_loss: 0.6093 - val_acc: 0.7720\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7997 - acc: 0.6995 - val_loss: 0.6074 - val_acc: 0.7750\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8139 - acc: 0.6983 - val_loss: 0.6083 - val_acc: 0.7760\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8060 - acc: 0.6977 - val_loss: 0.6071 - val_acc: 0.7770\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8030 - acc: 0.7011 - val_loss: 0.6069 - val_acc: 0.7730\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7973 - acc: 0.7068 - val_loss: 0.6072 - val_acc: 0.7740\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7984 - acc: 0.7011 - val_loss: 0.6051 - val_acc: 0.7780\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7961 - acc: 0.7041 - val_loss: 0.6026 - val_acc: 0.7750\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7918 - acc: 0.7037 - val_loss: 0.6033 - val_acc: 0.7710\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8061 - acc: 0.7004 - val_loss: 0.6017 - val_acc: 0.7740\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7913 - acc: 0.7065 - val_loss: 0.5999 - val_acc: 0.7740\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7883 - acc: 0.7067 - val_loss: 0.5997 - val_acc: 0.7750\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7878 - acc: 0.7043 - val_loss: 0.5982 - val_acc: 0.7760\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7851 - acc: 0.7037 - val_loss: 0.5975 - val_acc: 0.7770\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7876 - acc: 0.7076 - val_loss: 0.5977 - val_acc: 0.7760\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7771 - acc: 0.7103 - val_loss: 0.5960 - val_acc: 0.7750\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7871 - acc: 0.7119 - val_loss: 0.5957 - val_acc: 0.7780\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7642 - acc: 0.7132 - val_loss: 0.5934 - val_acc: 0.7760\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7850 - acc: 0.7108 - val_loss: 0.5942 - val_acc: 0.7760\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7859 - acc: 0.7117 - val_loss: 0.5941 - val_acc: 0.7780\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7736 - acc: 0.7111 - val_loss: 0.5921 - val_acc: 0.7800\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7620 - acc: 0.7176 - val_loss: 0.5910 - val_acc: 0.7800\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7776 - acc: 0.7120 - val_loss: 0.5893 - val_acc: 0.7820\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7681 - acc: 0.7148 - val_loss: 0.5890 - val_acc: 0.7800\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7698 - acc: 0.7243 - val_loss: 0.5877 - val_acc: 0.7810\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7665 - acc: 0.7103 - val_loss: 0.5865 - val_acc: 0.7800\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7598 - acc: 0.7183 - val_loss: 0.5867 - val_acc: 0.7860\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7544 - acc: 0.7227 - val_loss: 0.5888 - val_acc: 0.7830\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7636 - acc: 0.7149 - val_loss: 0.5872 - val_acc: 0.7870\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7671 - acc: 0.7184 - val_loss: 0.5859 - val_acc: 0.7840\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7622 - acc: 0.7207 - val_loss: 0.5835 - val_acc: 0.7810\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7413 - acc: 0.7276 - val_loss: 0.5814 - val_acc: 0.7810\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7467 - acc: 0.7227 - val_loss: 0.5805 - val_acc: 0.7780\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7618 - acc: 0.7188 - val_loss: 0.5797 - val_acc: 0.7810\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7390 - acc: 0.7247 - val_loss: 0.5787 - val_acc: 0.7840\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7432 - acc: 0.7229 - val_loss: 0.5783 - val_acc: 0.7820\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7557 - acc: 0.7137 - val_loss: 0.5795 - val_acc: 0.7800\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7495 - acc: 0.7199 - val_loss: 0.5790 - val_acc: 0.7810\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7338 - acc: 0.7253 - val_loss: 0.5775 - val_acc: 0.7800\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7300 - acc: 0.7212 - val_loss: 0.5762 - val_acc: 0.7850\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7517 - acc: 0.7201 - val_loss: 0.5791 - val_acc: 0.7860\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7553 - acc: 0.7192 - val_loss: 0.5771 - val_acc: 0.7850\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7304 - acc: 0.7263 - val_loss: 0.5744 - val_acc: 0.7830\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7179 - acc: 0.7317 - val_loss: 0.5747 - val_acc: 0.7800\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.7296 - acc: 0.7277 - val_loss: 0.5738 - val_acc: 0.7820\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7372 - acc: 0.7256 - val_loss: 0.5741 - val_acc: 0.7830\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7290 - acc: 0.7315 - val_loss: 0.5724 - val_acc: 0.7860\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7422 - acc: 0.7267 - val_loss: 0.5728 - val_acc: 0.7840\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7333 - acc: 0.7225 - val_loss: 0.5722 - val_acc: 0.7860\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7338 - acc: 0.7224 - val_loss: 0.5719 - val_acc: 0.7850\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7267 - acc: 0.7235 - val_loss: 0.5706 - val_acc: 0.7810\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7226 - acc: 0.7269 - val_loss: 0.5699 - val_acc: 0.7800\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.7166 - acc: 0.7369 - val_loss: 0.5675 - val_acc: 0.7870\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.7323 - acc: 0.7299 - val_loss: 0.5673 - val_acc: 0.7870\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step\n",
      "1500/1500 [==============================] - 0s 41us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.43646411714553834, 0.8470666666666666]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5718857808113098, 0.7806666668256124]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 1.9237 - acc: 0.1952 - val_loss: 1.8884 - val_acc: 0.2013\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.8472 - acc: 0.2323 - val_loss: 1.8029 - val_acc: 0.2493\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 1.7283 - acc: 0.3281 - val_loss: 1.6523 - val_acc: 0.3693\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.5454 - acc: 0.4571 - val_loss: 1.4564 - val_acc: 0.5043\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 1.3407 - acc: 0.5632 - val_loss: 1.2656 - val_acc: 0.5933\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 1.1617 - acc: 0.6324 - val_loss: 1.1084 - val_acc: 0.6480\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 1.0180 - acc: 0.6730 - val_loss: 0.9802 - val_acc: 0.6857\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.9102 - acc: 0.6988 - val_loss: 0.8889 - val_acc: 0.7027\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.8350 - acc: 0.7140 - val_loss: 0.8242 - val_acc: 0.7130\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.7807 - acc: 0.7267 - val_loss: 0.7782 - val_acc: 0.7297\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.7394 - acc: 0.7371 - val_loss: 0.7405 - val_acc: 0.7370\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.7077 - acc: 0.7455 - val_loss: 0.7144 - val_acc: 0.7473\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6822 - acc: 0.7530 - val_loss: 0.6918 - val_acc: 0.7460\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6616 - acc: 0.7582 - val_loss: 0.6724 - val_acc: 0.7510\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6437 - acc: 0.7637 - val_loss: 0.6581 - val_acc: 0.7550\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6283 - acc: 0.7698 - val_loss: 0.6472 - val_acc: 0.7640\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.6152 - acc: 0.7751 - val_loss: 0.6335 - val_acc: 0.7663\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.6029 - acc: 0.7790 - val_loss: 0.6242 - val_acc: 0.7657\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5924 - acc: 0.7824 - val_loss: 0.6163 - val_acc: 0.7707\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5825 - acc: 0.7854 - val_loss: 0.6095 - val_acc: 0.7710\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5734 - acc: 0.7899 - val_loss: 0.6023 - val_acc: 0.7760\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5648 - acc: 0.7929 - val_loss: 0.5949 - val_acc: 0.7763\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5570 - acc: 0.7966 - val_loss: 0.5896 - val_acc: 0.7780\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5497 - acc: 0.7994 - val_loss: 0.5850 - val_acc: 0.7837\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5427 - acc: 0.8013 - val_loss: 0.5822 - val_acc: 0.7840\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5362 - acc: 0.8049 - val_loss: 0.5760 - val_acc: 0.7847\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.5297 - acc: 0.8078 - val_loss: 0.5740 - val_acc: 0.7897\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5242 - acc: 0.8110 - val_loss: 0.5689 - val_acc: 0.7877\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5183 - acc: 0.8118 - val_loss: 0.5646 - val_acc: 0.7880\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5129 - acc: 0.8148 - val_loss: 0.5622 - val_acc: 0.7887\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5077 - acc: 0.8162 - val_loss: 0.5589 - val_acc: 0.7897\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.5028 - acc: 0.8192 - val_loss: 0.5561 - val_acc: 0.7907\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4976 - acc: 0.8213 - val_loss: 0.5540 - val_acc: 0.7910\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4931 - acc: 0.8215 - val_loss: 0.5509 - val_acc: 0.7913\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4884 - acc: 0.8245 - val_loss: 0.5489 - val_acc: 0.7923\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4840 - acc: 0.8254 - val_loss: 0.5455 - val_acc: 0.7930\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4797 - acc: 0.8280 - val_loss: 0.5443 - val_acc: 0.7967\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4756 - acc: 0.8293 - val_loss: 0.5432 - val_acc: 0.7953\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4715 - acc: 0.8310 - val_loss: 0.5411 - val_acc: 0.7967\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4678 - acc: 0.8331 - val_loss: 0.5396 - val_acc: 0.7977\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4639 - acc: 0.8339 - val_loss: 0.5396 - val_acc: 0.7967\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4603 - acc: 0.8359 - val_loss: 0.5381 - val_acc: 0.8007\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4569 - acc: 0.8366 - val_loss: 0.5373 - val_acc: 0.8010\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4533 - acc: 0.8386 - val_loss: 0.5386 - val_acc: 0.7977\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4501 - acc: 0.8399 - val_loss: 0.5331 - val_acc: 0.8037\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 33us/step - loss: 0.4468 - acc: 0.8420 - val_loss: 0.5336 - val_acc: 0.8037\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4431 - acc: 0.8428 - val_loss: 0.5323 - val_acc: 0.8020\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4404 - acc: 0.8444 - val_loss: 0.5314 - val_acc: 0.8020\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4372 - acc: 0.8449 - val_loss: 0.5297 - val_acc: 0.8067\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4340 - acc: 0.8472 - val_loss: 0.5304 - val_acc: 0.8037\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4309 - acc: 0.8478 - val_loss: 0.5267 - val_acc: 0.8050\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4283 - acc: 0.8490 - val_loss: 0.5282 - val_acc: 0.8070\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4256 - acc: 0.8499 - val_loss: 0.5273 - val_acc: 0.8067\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4225 - acc: 0.8514 - val_loss: 0.5254 - val_acc: 0.8113\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4203 - acc: 0.8520 - val_loss: 0.5260 - val_acc: 0.8090\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4173 - acc: 0.8538 - val_loss: 0.5253 - val_acc: 0.8113\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4146 - acc: 0.8548 - val_loss: 0.5283 - val_acc: 0.8087\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4126 - acc: 0.8560 - val_loss: 0.5252 - val_acc: 0.8090\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4097 - acc: 0.8570 - val_loss: 0.5242 - val_acc: 0.8123\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.4075 - acc: 0.8573 - val_loss: 0.5243 - val_acc: 0.8107\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4049 - acc: 0.8582 - val_loss: 0.5248 - val_acc: 0.8097\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4028 - acc: 0.8582 - val_loss: 0.5242 - val_acc: 0.8113\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.4007 - acc: 0.8602 - val_loss: 0.5245 - val_acc: 0.8097\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3985 - acc: 0.8602 - val_loss: 0.5229 - val_acc: 0.8127\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3957 - acc: 0.8618 - val_loss: 0.5242 - val_acc: 0.8113\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3935 - acc: 0.8620 - val_loss: 0.5234 - val_acc: 0.8117\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3919 - acc: 0.8634 - val_loss: 0.5245 - val_acc: 0.8117\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3896 - acc: 0.8633 - val_loss: 0.5234 - val_acc: 0.8110\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3876 - acc: 0.8637 - val_loss: 0.5246 - val_acc: 0.8110\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3853 - acc: 0.8655 - val_loss: 0.5249 - val_acc: 0.8103\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3836 - acc: 0.8658 - val_loss: 0.5241 - val_acc: 0.8100\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3816 - acc: 0.8670 - val_loss: 0.5247 - val_acc: 0.8127\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3797 - acc: 0.8676 - val_loss: 0.5244 - val_acc: 0.8150\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3773 - acc: 0.8684 - val_loss: 0.5263 - val_acc: 0.8110\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3759 - acc: 0.8689 - val_loss: 0.5249 - val_acc: 0.8130\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3736 - acc: 0.8698 - val_loss: 0.5256 - val_acc: 0.8127\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3721 - acc: 0.8700 - val_loss: 0.5264 - val_acc: 0.8107\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3707 - acc: 0.8712 - val_loss: 0.5278 - val_acc: 0.8087\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3685 - acc: 0.8722 - val_loss: 0.5284 - val_acc: 0.8100\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3665 - acc: 0.8731 - val_loss: 0.5297 - val_acc: 0.8100\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3646 - acc: 0.8735 - val_loss: 0.5319 - val_acc: 0.8083\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3632 - acc: 0.8744 - val_loss: 0.5286 - val_acc: 0.8097\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3614 - acc: 0.8751 - val_loss: 0.5281 - val_acc: 0.8127\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3596 - acc: 0.8751 - val_loss: 0.5288 - val_acc: 0.8130\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3580 - acc: 0.8763 - val_loss: 0.5306 - val_acc: 0.8097\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3562 - acc: 0.8774 - val_loss: 0.5306 - val_acc: 0.8117\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3547 - acc: 0.8773 - val_loss: 0.5317 - val_acc: 0.8113\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3528 - acc: 0.8782 - val_loss: 0.5331 - val_acc: 0.8097\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3517 - acc: 0.8777 - val_loss: 0.5325 - val_acc: 0.8103\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3499 - acc: 0.8792 - val_loss: 0.5321 - val_acc: 0.8113\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3484 - acc: 0.8803 - val_loss: 0.5339 - val_acc: 0.8113\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3465 - acc: 0.8799 - val_loss: 0.5325 - val_acc: 0.8127\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3456 - acc: 0.8815 - val_loss: 0.5323 - val_acc: 0.8120\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3439 - acc: 0.8812 - val_loss: 0.5355 - val_acc: 0.8090\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3426 - acc: 0.8817 - val_loss: 0.5343 - val_acc: 0.8117\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3404 - acc: 0.8830 - val_loss: 0.5383 - val_acc: 0.8110\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3393 - acc: 0.8825 - val_loss: 0.5363 - val_acc: 0.8087\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3378 - acc: 0.8844 - val_loss: 0.5383 - val_acc: 0.8097\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3364 - acc: 0.8848 - val_loss: 0.5366 - val_acc: 0.8107\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3352 - acc: 0.8849 - val_loss: 0.5384 - val_acc: 0.8090\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3334 - acc: 0.8858 - val_loss: 0.5395 - val_acc: 0.8100\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3321 - acc: 0.8865 - val_loss: 0.5389 - val_acc: 0.8087\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3312 - acc: 0.8859 - val_loss: 0.5412 - val_acc: 0.8087\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3294 - acc: 0.8867 - val_loss: 0.5435 - val_acc: 0.8093\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3279 - acc: 0.8876 - val_loss: 0.5435 - val_acc: 0.8097\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3267 - acc: 0.8873 - val_loss: 0.5435 - val_acc: 0.8113\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3252 - acc: 0.8890 - val_loss: 0.5448 - val_acc: 0.8093\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3241 - acc: 0.8881 - val_loss: 0.5452 - val_acc: 0.8090\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3229 - acc: 0.8892 - val_loss: 0.5467 - val_acc: 0.8107\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3215 - acc: 0.8899 - val_loss: 0.5457 - val_acc: 0.8097\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3201 - acc: 0.8901 - val_loss: 0.5512 - val_acc: 0.8063\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3189 - acc: 0.8907 - val_loss: 0.5472 - val_acc: 0.8087\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3176 - acc: 0.8910 - val_loss: 0.5477 - val_acc: 0.8077\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3163 - acc: 0.8920 - val_loss: 0.5497 - val_acc: 0.8073\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3149 - acc: 0.8922 - val_loss: 0.5506 - val_acc: 0.8087\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 32us/step - loss: 0.3140 - acc: 0.8923 - val_loss: 0.5496 - val_acc: 0.8117\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3125 - acc: 0.8933 - val_loss: 0.5538 - val_acc: 0.8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3114 - acc: 0.8937 - val_loss: 0.5517 - val_acc: 0.8123\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 30us/step - loss: 0.3097 - acc: 0.8945 - val_loss: 0.5539 - val_acc: 0.8077\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 31us/step - loss: 0.3089 - acc: 0.8948 - val_loss: 0.5528 - val_acc: 0.8107\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 37us/step\n",
      "4000/4000 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.303767584287759, 0.8974848484848484]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.584268543124199, 0.8055]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
